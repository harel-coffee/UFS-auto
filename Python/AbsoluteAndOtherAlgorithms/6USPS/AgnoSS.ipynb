{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=\"./Dataset/USPS.mat\"\n",
    "Data = scipy.io.loadmat(data_path)\n",
    "\n",
    "data_arr=Data['X'].astype('float32')\n",
    "label_arr=Data['Y'][:, 0]-1\n",
    "\n",
    "#label_arr=to_categorical(label_arr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of C_train_x: (7438, 256)\n",
      "Shape of C_train_y: (7438,)\n",
      "Shape of C_test_x: (1860, 256)\n",
      "Shape of C_test_y: (1860,)\n",
      "Shape of x_train: (6694, 256)\n",
      "Shape of x_test: (1860, 256)\n",
      "Shape of x_validate: (744, 256)\n",
      "Shape of y_train: (6694,)\n",
      "Shape of y_test: (1860,)\n",
      "Shape of y_validate: (744,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ny_train_onehot = to_categorical(y_train_)\\ny_validate_onehot = to_categorical(y_validate_)\\ny_test_onehot = to_categorical(C_test_y)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(data_arr,label_arr,test_size=0.2,random_state=seed)\n",
    "x_train,x_validate,y_train,y_validate= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) \n",
    "\n",
    "x_test=C_test_x\n",
    "y_test=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape))\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate.shape))\n",
    "\n",
    "'''\n",
    "y_train_onehot = to_categorical(y_train_)\n",
    "y_validate_onehot = to_categorical(y_validate_)\n",
    "y_test_onehot = to_categorical(C_test_y)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.l1_lambda=l1_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0., maxval=1.),\n",
    "                                      trainable=True,\n",
    "                                      regularizer=regularizers.l1(self.l1_lambda),\n",
    "                                      constraint=constraints.NonNeg())\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=36):\n",
    "        kernel=self.kernel        \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            print(kernel_.shape)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=50,\\\n",
    "                         p_learning_rate= 1E-3,\\\n",
    "                         p_l1_lambda=0.1):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             l1_lambda=p_l1_lambda,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1.1 Identity Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "feature_selection (Feature_S (None, 256)               256       \n",
      "_________________________________________________________________\n",
      "autoencoder_hidden_layer (De (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "autoencoder_output (Dense)   (None, 256)               13056     \n",
      "=================================================================\n",
      "Total params: 26,162\n",
      "Trainable params: 26,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ide_AE,\\\n",
    "latent_encoder_score_Ide_AE=Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                 p_encoding_dim=50,\\\n",
    "                                                 p_learning_rate= 1E-2,\\\n",
    "                                                 p_l1_lambda=l1_lambda)\n",
    "\n",
    "file_name=\"./log/AgnoSS.png\"\n",
    "plot_model(Ide_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 6694 samples, validate on 744 samples\n",
      "Epoch 1/1000\n",
      "6694/6694 [==============================] - 1s 107us/step - loss: 39.4056 - val_loss: 0.2684\n",
      "Epoch 2/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.2676 - val_loss: 0.2675\n",
      "Epoch 3/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.2673 - val_loss: 0.2681\n",
      "Epoch 4/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.2678 - val_loss: 0.2687\n",
      "Epoch 5/1000\n",
      "6694/6694 [==============================] - 0s 49us/step - loss: 0.2674 - val_loss: 0.2620\n",
      "Epoch 6/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.2449 - val_loss: 0.2373\n",
      "Epoch 7/1000\n",
      "6694/6694 [==============================] - 0s 49us/step - loss: 0.2373 - val_loss: 0.2441\n",
      "Epoch 8/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.2330 - val_loss: 0.2277\n",
      "Epoch 9/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.2264 - val_loss: 0.2191\n",
      "Epoch 10/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.2171 - val_loss: 0.2158\n",
      "Epoch 11/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.2087 - val_loss: 0.2147\n",
      "Epoch 12/1000\n",
      "6694/6694 [==============================] - 0s 49us/step - loss: 0.2030 - val_loss: 0.2005\n",
      "Epoch 13/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.1910 - val_loss: 0.1845\n",
      "Epoch 14/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1847 - val_loss: 0.1916\n",
      "Epoch 15/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1826 - val_loss: 0.1848\n",
      "Epoch 16/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1810 - val_loss: 0.1788\n",
      "Epoch 17/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1788 - val_loss: 0.1720\n",
      "Epoch 18/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1746 - val_loss: 0.1855\n",
      "Epoch 19/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1731 - val_loss: 0.1711\n",
      "Epoch 20/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1650 - val_loss: 0.1626\n",
      "Epoch 21/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1679 - val_loss: 0.1616\n",
      "Epoch 22/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1603 - val_loss: 0.1756\n",
      "Epoch 23/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1603 - val_loss: 0.1528\n",
      "Epoch 24/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1570 - val_loss: 0.1481\n",
      "Epoch 25/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1546 - val_loss: 0.1485\n",
      "Epoch 26/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1498 - val_loss: 0.1628\n",
      "Epoch 27/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1525 - val_loss: 0.1407\n",
      "Epoch 28/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1438 - val_loss: 0.1453\n",
      "Epoch 29/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1513 - val_loss: 0.1332\n",
      "Epoch 30/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.1479 - val_loss: 0.1366\n",
      "Epoch 31/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1383 - val_loss: 0.1471\n",
      "Epoch 32/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1378 - val_loss: 0.1341\n",
      "Epoch 33/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1394 - val_loss: 0.1376\n",
      "Epoch 34/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1379 - val_loss: 0.1300\n",
      "Epoch 35/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1347 - val_loss: 0.1378\n",
      "Epoch 36/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1372 - val_loss: 0.1499\n",
      "Epoch 37/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1300 - val_loss: 0.1366\n",
      "Epoch 38/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1343 - val_loss: 0.2327\n",
      "Epoch 39/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1327 - val_loss: 0.1277\n",
      "Epoch 40/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1269 - val_loss: 0.1507\n",
      "Epoch 41/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1363 - val_loss: 0.1349\n",
      "Epoch 42/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1298 - val_loss: 0.1242\n",
      "Epoch 43/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1373 - val_loss: 0.1198\n",
      "Epoch 44/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1244 - val_loss: 0.1145\n",
      "Epoch 45/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1268 - val_loss: 0.1302\n",
      "Epoch 46/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1208 - val_loss: 0.1205\n",
      "Epoch 47/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1286 - val_loss: 0.1404\n",
      "Epoch 48/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1191 - val_loss: 0.1114\n",
      "Epoch 49/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1301 - val_loss: 0.1124\n",
      "Epoch 50/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1227 - val_loss: 0.1210\n",
      "Epoch 51/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1234 - val_loss: 0.1144\n",
      "Epoch 52/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1301 - val_loss: 0.1088\n",
      "Epoch 53/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1221 - val_loss: 0.1365\n",
      "Epoch 54/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1172 - val_loss: 0.1405\n",
      "Epoch 55/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1229 - val_loss: 0.1247\n",
      "Epoch 56/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1255 - val_loss: 0.1082\n",
      "Epoch 57/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1155 - val_loss: 0.1259\n",
      "Epoch 58/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1325 - val_loss: 0.1122\n",
      "Epoch 59/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1086 - val_loss: 0.1013\n",
      "Epoch 60/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1194 - val_loss: 0.1255\n",
      "Epoch 61/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1251 - val_loss: 0.1374\n",
      "Epoch 62/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1110 - val_loss: 0.1020\n",
      "Epoch 63/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1205 - val_loss: 0.1347\n",
      "Epoch 64/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1178 - val_loss: 0.1037\n",
      "Epoch 65/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1159 - val_loss: 0.1181\n",
      "Epoch 66/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1165 - val_loss: 0.1392\n",
      "Epoch 67/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1247 - val_loss: 0.1070\n",
      "Epoch 68/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1111 - val_loss: 0.1103\n",
      "Epoch 69/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1165 - val_loss: 0.1113\n",
      "Epoch 70/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1153 - val_loss: 0.1276\n",
      "Epoch 71/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1161 - val_loss: 0.1357\n",
      "Epoch 72/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1181 - val_loss: 0.1075\n",
      "Epoch 73/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1135 - val_loss: 0.1249\n",
      "Epoch 74/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1203 - val_loss: 0.1094\n",
      "Epoch 75/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1162 - val_loss: 0.1148\n",
      "Epoch 76/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1113 - val_loss: 0.1191\n",
      "Epoch 77/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1162 - val_loss: 0.1150\n",
      "Epoch 78/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1271 - val_loss: 0.1190\n",
      "Epoch 79/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1145 - val_loss: 0.1237\n",
      "Epoch 80/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1178 - val_loss: 0.1064\n",
      "Epoch 81/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1153 - val_loss: 0.1170\n",
      "Epoch 82/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1120 - val_loss: 0.1141\n",
      "Epoch 83/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1208 - val_loss: 0.1147\n",
      "Epoch 84/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1219 - val_loss: 0.1181\n",
      "Epoch 85/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1154 - val_loss: 0.1283\n",
      "Epoch 86/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1126 - val_loss: 0.1568\n",
      "Epoch 87/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1160 - val_loss: 0.1052\n",
      "Epoch 88/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1132 - val_loss: 0.1046\n",
      "Epoch 89/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1139 - val_loss: 0.1266\n",
      "Epoch 90/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1214 - val_loss: 0.1427\n",
      "Epoch 91/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1304 - val_loss: 0.1080\n",
      "Epoch 92/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1156 - val_loss: 0.1312\n",
      "Epoch 93/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1153 - val_loss: 0.1116\n",
      "Epoch 94/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1166 - val_loss: 0.1038\n",
      "Epoch 95/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1129 - val_loss: 0.1127\n",
      "Epoch 96/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1154 - val_loss: 0.1111\n",
      "Epoch 97/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1166 - val_loss: 0.1105\n",
      "Epoch 98/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1180 - val_loss: 0.1319\n",
      "Epoch 99/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1155 - val_loss: 0.1204\n",
      "Epoch 100/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1136 - val_loss: 0.0909\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/Ide_AE_weights.0100.hdf5\n",
      "Epoch 101/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1181 - val_loss: 0.1140\n",
      "Epoch 102/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1112 - val_loss: 0.1377\n",
      "Epoch 103/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1207 - val_loss: 0.1151\n",
      "Epoch 104/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1230 - val_loss: 0.1049\n",
      "Epoch 105/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1055 - val_loss: 0.1170\n",
      "Epoch 106/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1177 - val_loss: 0.1208\n",
      "Epoch 107/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1126 - val_loss: 0.1121\n",
      "Epoch 108/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1224 - val_loss: 0.1250\n",
      "Epoch 109/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1178 - val_loss: 0.1238\n",
      "Epoch 110/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1220 - val_loss: 0.1091\n",
      "Epoch 111/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1182 - val_loss: 0.1332\n",
      "Epoch 112/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1239 - val_loss: 0.1079\n",
      "Epoch 113/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1117 - val_loss: 0.1444\n",
      "Epoch 114/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1179 - val_loss: 0.1172\n",
      "Epoch 115/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1158 - val_loss: 0.1256\n",
      "Epoch 116/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1226 - val_loss: 0.1197\n",
      "Epoch 117/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1196 - val_loss: 0.1451\n",
      "Epoch 118/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1136 - val_loss: 0.0938\n",
      "Epoch 119/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1151 - val_loss: 0.1021\n",
      "Epoch 120/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1211 - val_loss: 0.1179\n",
      "Epoch 121/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1178 - val_loss: 0.1136\n",
      "Epoch 122/1000\n",
      "6694/6694 [==============================] - 0s 50us/step - loss: 0.1244 - val_loss: 0.0923\n",
      "Epoch 123/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1134 - val_loss: 0.1744\n",
      "Epoch 124/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1231 - val_loss: 0.1353\n",
      "Epoch 125/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1201 - val_loss: 0.0922\n",
      "Epoch 126/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1156 - val_loss: 0.1234\n",
      "Epoch 127/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1362 - val_loss: 0.1114\n",
      "Epoch 128/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1227 - val_loss: 0.1287\n",
      "Epoch 129/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1119 - val_loss: 0.1216\n",
      "Epoch 130/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1202 - val_loss: 0.1244\n",
      "Epoch 131/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1206 - val_loss: 0.1012\n",
      "Epoch 132/1000\n",
      "6694/6694 [==============================] - ETA: 0s - loss: 0.115 - 0s 61us/step - loss: 0.1173 - val_loss: 0.1241\n",
      "Epoch 133/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1230 - val_loss: 0.1643\n",
      "Epoch 134/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1141 - val_loss: 0.1102\n",
      "Epoch 135/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1290 - val_loss: 0.1072\n",
      "Epoch 136/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1135 - val_loss: 0.1374\n",
      "Epoch 137/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1224 - val_loss: 0.1044\n",
      "Epoch 138/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1116 - val_loss: 0.0987\n",
      "Epoch 139/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1341 - val_loss: 0.1352\n",
      "Epoch 140/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1195 - val_loss: 0.1127\n",
      "Epoch 141/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1205 - val_loss: 0.1429\n",
      "Epoch 142/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1154 - val_loss: 0.1093\n",
      "Epoch 143/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1247 - val_loss: 0.1033\n",
      "Epoch 144/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1117 - val_loss: 0.1213\n",
      "Epoch 145/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1185 - val_loss: 0.1166\n",
      "Epoch 146/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1233 - val_loss: 0.1214\n",
      "Epoch 147/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1141 - val_loss: 0.1166\n",
      "Epoch 148/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1220 - val_loss: 0.1752\n",
      "Epoch 149/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1224 - val_loss: 0.1017\n",
      "Epoch 150/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1261 - val_loss: 0.1256\n",
      "Epoch 151/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1152 - val_loss: 0.1052\n",
      "Epoch 152/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1156 - val_loss: 0.1205\n",
      "Epoch 153/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1205 - val_loss: 0.1396\n",
      "Epoch 154/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1241 - val_loss: 0.1535\n",
      "Epoch 155/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1265 - val_loss: 0.1062\n",
      "Epoch 156/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1201 - val_loss: 0.1456\n",
      "Epoch 157/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1172 - val_loss: 0.1118\n",
      "Epoch 158/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1248 - val_loss: 0.1211\n",
      "Epoch 159/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1200 - val_loss: 0.1371\n",
      "Epoch 160/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1248 - val_loss: 0.1345\n",
      "Epoch 161/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1173 - val_loss: 0.1161\n",
      "Epoch 162/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1236 - val_loss: 0.0985\n",
      "Epoch 163/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1324 - val_loss: 0.1391\n",
      "Epoch 164/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1134 - val_loss: 0.1356\n",
      "Epoch 165/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1247 - val_loss: 0.1665\n",
      "Epoch 166/1000\n",
      "6694/6694 [==============================] - ETA: 0s - loss: 0.118 - 0s 61us/step - loss: 0.1207 - val_loss: 0.1290\n",
      "Epoch 167/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1212 - val_loss: 0.0974\n",
      "Epoch 168/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1177 - val_loss: 0.1471\n",
      "Epoch 169/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1248 - val_loss: 0.1336\n",
      "Epoch 170/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1215 - val_loss: 0.1088\n",
      "Epoch 171/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1244 - val_loss: 0.1175\n",
      "Epoch 172/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1215 - val_loss: 0.1144\n",
      "Epoch 173/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1206 - val_loss: 0.1024\n",
      "Epoch 174/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1160 - val_loss: 0.0976\n",
      "Epoch 175/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1228 - val_loss: 0.1414\n",
      "Epoch 176/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1271 - val_loss: 0.1055\n",
      "Epoch 177/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1146 - val_loss: 0.1256\n",
      "Epoch 178/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1188 - val_loss: 0.1476\n",
      "Epoch 179/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1265 - val_loss: 0.1184\n",
      "Epoch 180/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1211 - val_loss: 0.1971\n",
      "Epoch 181/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1257 - val_loss: 0.1020\n",
      "Epoch 182/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1233 - val_loss: 0.1267\n",
      "Epoch 183/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1253 - val_loss: 0.1301\n",
      "Epoch 184/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1379 - val_loss: 0.1272\n",
      "Epoch 185/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1146 - val_loss: 0.1157\n",
      "Epoch 186/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1226 - val_loss: 0.1152\n",
      "Epoch 187/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1179 - val_loss: 0.1088\n",
      "Epoch 188/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1248 - val_loss: 0.1230\n",
      "Epoch 189/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1188 - val_loss: 0.1194\n",
      "Epoch 190/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1233 - val_loss: 0.1575\n",
      "Epoch 191/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1107 - val_loss: 0.1346\n",
      "Epoch 192/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1235 - val_loss: 0.1237\n",
      "Epoch 193/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1195 - val_loss: 0.1450\n",
      "Epoch 194/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1312 - val_loss: 0.1189\n",
      "Epoch 195/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1228 - val_loss: 0.0998\n",
      "Epoch 196/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1270 - val_loss: 0.1389\n",
      "Epoch 197/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1293 - val_loss: 0.1201\n",
      "Epoch 198/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1222 - val_loss: 0.1061\n",
      "Epoch 199/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1145 - val_loss: 0.1181\n",
      "Epoch 200/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1171 - val_loss: 0.1618\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/Ide_AE_weights.0200.hdf5\n",
      "Epoch 201/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1247 - val_loss: 0.1349\n",
      "Epoch 202/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1245 - val_loss: 0.1585\n",
      "Epoch 203/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1224 - val_loss: 0.1340\n",
      "Epoch 204/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1248 - val_loss: 0.1065\n",
      "Epoch 205/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1222 - val_loss: 0.2270\n",
      "Epoch 206/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1225 - val_loss: 0.1188\n",
      "Epoch 207/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1253 - val_loss: 0.1085\n",
      "Epoch 208/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1141 - val_loss: 0.1117\n",
      "Epoch 209/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1237 - val_loss: 0.1161\n",
      "Epoch 210/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1224 - val_loss: 0.1016\n",
      "Epoch 211/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1208 - val_loss: 0.0988\n",
      "Epoch 212/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1209 - val_loss: 0.1298\n",
      "Epoch 213/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1254 - val_loss: 0.1243\n",
      "Epoch 214/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1320 - val_loss: 0.1248\n",
      "Epoch 215/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1340 - val_loss: 0.1108\n",
      "Epoch 216/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1234 - val_loss: 0.1081\n",
      "Epoch 217/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1225 - val_loss: 0.1005\n",
      "Epoch 218/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1154 - val_loss: 0.1249\n",
      "Epoch 219/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1219 - val_loss: 0.1402\n",
      "Epoch 220/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1333 - val_loss: 0.1451\n",
      "Epoch 221/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1337 - val_loss: 0.1206\n",
      "Epoch 222/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1094 - val_loss: 0.0964\n",
      "Epoch 223/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1234 - val_loss: 0.1107\n",
      "Epoch 224/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1250 - val_loss: 0.1272\n",
      "Epoch 225/1000\n",
      "6694/6694 [==============================] - 1s 76us/step - loss: 0.1288 - val_loss: 0.1089\n",
      "Epoch 226/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1147 - val_loss: 0.1464\n",
      "Epoch 227/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1438 - val_loss: 0.1343\n",
      "Epoch 228/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1219 - val_loss: 0.1078\n",
      "Epoch 229/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1174 - val_loss: 0.1147\n",
      "Epoch 230/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1231 - val_loss: 0.1270\n",
      "Epoch 231/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1268 - val_loss: 0.1519\n",
      "Epoch 232/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1277 - val_loss: 0.1251\n",
      "Epoch 233/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1219 - val_loss: 0.1278\n",
      "Epoch 234/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1239 - val_loss: 0.0997\n",
      "Epoch 235/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1189 - val_loss: 0.1166\n",
      "Epoch 236/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1294 - val_loss: 0.1115\n",
      "Epoch 237/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1235 - val_loss: 0.1038\n",
      "Epoch 238/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1167 - val_loss: 0.1234\n",
      "Epoch 239/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1311 - val_loss: 0.1219\n",
      "Epoch 240/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1314 - val_loss: 0.0986\n",
      "Epoch 241/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1257 - val_loss: 0.1187\n",
      "Epoch 242/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1246 - val_loss: 0.1463\n",
      "Epoch 243/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1172 - val_loss: 0.1256\n",
      "Epoch 244/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1338 - val_loss: 0.1074\n",
      "Epoch 245/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1225 - val_loss: 0.1298\n",
      "Epoch 246/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1271 - val_loss: 0.1565\n",
      "Epoch 247/1000\n",
      "6694/6694 [==============================] - 0s 51us/step - loss: 0.1265 - val_loss: 0.1208\n",
      "Epoch 248/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1192 - val_loss: 0.1360\n",
      "Epoch 249/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1299 - val_loss: 0.1607\n",
      "Epoch 250/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1257 - val_loss: 0.1779\n",
      "Epoch 251/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1283 - val_loss: 0.1114\n",
      "Epoch 252/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1243 - val_loss: 0.1543\n",
      "Epoch 253/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1319 - val_loss: 0.1089\n",
      "Epoch 254/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1214 - val_loss: 0.2071\n",
      "Epoch 255/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1275 - val_loss: 0.1342\n",
      "Epoch 256/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1191 - val_loss: 0.1176\n",
      "Epoch 257/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1247 - val_loss: 0.1367\n",
      "Epoch 258/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1287 - val_loss: 0.1051\n",
      "Epoch 259/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1151 - val_loss: 0.1474\n",
      "Epoch 260/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1241 - val_loss: 0.1054\n",
      "Epoch 261/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1205 - val_loss: 0.0952\n",
      "Epoch 262/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1241 - val_loss: 0.1236\n",
      "Epoch 263/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1242 - val_loss: 0.1473\n",
      "Epoch 264/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1230 - val_loss: 0.1199\n",
      "Epoch 265/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1291 - val_loss: 0.1548\n",
      "Epoch 266/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1260 - val_loss: 0.0959\n",
      "Epoch 267/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1268 - val_loss: 0.1108\n",
      "Epoch 268/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1272 - val_loss: 0.1055\n",
      "Epoch 269/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1202 - val_loss: 0.1264\n",
      "Epoch 270/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1229 - val_loss: 0.1233\n",
      "Epoch 271/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1205 - val_loss: 0.1128\n",
      "Epoch 272/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1248 - val_loss: 0.1161\n",
      "Epoch 273/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1254 - val_loss: 0.1507\n",
      "Epoch 274/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1278 - val_loss: 0.1274\n",
      "Epoch 275/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1203 - val_loss: 0.1597\n",
      "Epoch 276/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1377 - val_loss: 0.1386\n",
      "Epoch 277/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1236 - val_loss: 0.1061\n",
      "Epoch 278/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1172 - val_loss: 0.1232\n",
      "Epoch 279/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1296 - val_loss: 0.1170\n",
      "Epoch 280/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1292 - val_loss: 0.1185\n",
      "Epoch 281/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1208 - val_loss: 0.1005\n",
      "Epoch 282/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1259 - val_loss: 0.1020\n",
      "Epoch 283/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1341 - val_loss: 0.1093\n",
      "Epoch 284/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1168 - val_loss: 0.1271\n",
      "Epoch 285/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1345 - val_loss: 0.1092\n",
      "Epoch 286/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1240 - val_loss: 0.1322\n",
      "Epoch 287/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1223 - val_loss: 0.0967\n",
      "Epoch 288/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1230 - val_loss: 0.1207\n",
      "Epoch 289/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1269 - val_loss: 0.1164\n",
      "Epoch 290/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1288 - val_loss: 0.1310\n",
      "Epoch 291/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1169 - val_loss: 0.1503\n",
      "Epoch 292/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1338 - val_loss: 0.1288\n",
      "Epoch 293/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1295 - val_loss: 0.1129\n",
      "Epoch 294/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1181 - val_loss: 0.1709\n",
      "Epoch 295/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1217 - val_loss: 0.2652\n",
      "Epoch 296/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1331 - val_loss: 0.1155\n",
      "Epoch 297/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1177 - val_loss: 0.1099\n",
      "Epoch 298/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1298 - val_loss: 0.1335\n",
      "Epoch 299/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1311 - val_loss: 0.1302\n",
      "Epoch 300/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1152 - val_loss: 0.1219\n",
      "\n",
      "Epoch 00300: saving model to ./log_weights/Ide_AE_weights.0300.hdf5\n",
      "Epoch 301/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1240 - val_loss: 0.1230\n",
      "Epoch 302/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1280 - val_loss: 0.1091\n",
      "Epoch 303/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1220 - val_loss: 0.1148\n",
      "Epoch 304/1000\n",
      "6694/6694 [==============================] - 0s 74us/step - loss: 0.1271 - val_loss: 0.1197\n",
      "Epoch 305/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1210 - val_loss: 0.0991\n",
      "Epoch 306/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1263 - val_loss: 0.1123\n",
      "Epoch 307/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1200 - val_loss: 0.1146\n",
      "Epoch 308/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1265 - val_loss: 0.1305\n",
      "Epoch 309/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1332 - val_loss: 0.1388\n",
      "Epoch 310/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1224 - val_loss: 0.1136\n",
      "Epoch 311/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1295 - val_loss: 0.1224\n",
      "Epoch 312/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1321 - val_loss: 0.1446\n",
      "Epoch 313/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1175 - val_loss: 0.1155\n",
      "Epoch 314/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1172 - val_loss: 0.1138\n",
      "Epoch 315/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1344 - val_loss: 0.1225\n",
      "Epoch 316/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1241 - val_loss: 0.1027\n",
      "Epoch 317/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1225 - val_loss: 0.1167\n",
      "Epoch 318/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1248 - val_loss: 0.1151\n",
      "Epoch 319/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1386 - val_loss: 0.1235\n",
      "Epoch 320/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1119 - val_loss: 0.1396\n",
      "Epoch 321/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1267 - val_loss: 0.1198\n",
      "Epoch 322/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1261 - val_loss: 0.1375\n",
      "Epoch 323/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1222 - val_loss: 0.1173\n",
      "Epoch 324/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1247 - val_loss: 0.1328\n",
      "Epoch 325/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1228 - val_loss: 0.1377\n",
      "Epoch 326/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1313 - val_loss: 0.1152\n",
      "Epoch 327/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1301 - val_loss: 0.1097\n",
      "Epoch 328/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1209 - val_loss: 0.1429\n",
      "Epoch 329/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1213 - val_loss: 0.1263\n",
      "Epoch 330/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1223 - val_loss: 0.1264\n",
      "Epoch 331/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1204 - val_loss: 0.0978\n",
      "Epoch 332/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1289 - val_loss: 0.1210\n",
      "Epoch 333/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1293 - val_loss: 0.1139\n",
      "Epoch 334/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1256 - val_loss: 0.1999\n",
      "Epoch 335/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1275 - val_loss: 0.1422\n",
      "Epoch 336/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1362 - val_loss: 0.1346\n",
      "Epoch 337/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1199 - val_loss: 0.1307\n",
      "Epoch 338/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1320 - val_loss: 0.1015\n",
      "Epoch 339/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1279 - val_loss: 0.1581\n",
      "Epoch 340/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1300 - val_loss: 0.1383\n",
      "Epoch 341/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1195 - val_loss: 0.1159\n",
      "Epoch 342/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1318 - val_loss: 0.1119\n",
      "Epoch 343/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1241 - val_loss: 0.1119\n",
      "Epoch 344/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1269 - val_loss: 0.1894\n",
      "Epoch 345/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1310 - val_loss: 0.1073\n",
      "Epoch 346/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1308 - val_loss: 0.1033\n",
      "Epoch 347/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1246 - val_loss: 0.1354\n",
      "Epoch 348/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1277 - val_loss: 0.1760\n",
      "Epoch 349/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1257 - val_loss: 0.1194\n",
      "Epoch 350/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1360 - val_loss: 0.1020\n",
      "Epoch 351/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1201 - val_loss: 0.1529\n",
      "Epoch 352/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1251 - val_loss: 0.1643\n",
      "Epoch 353/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1332 - val_loss: 0.1180\n",
      "Epoch 354/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1198 - val_loss: 0.1323\n",
      "Epoch 355/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1286 - val_loss: 0.1269\n",
      "Epoch 356/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1318 - val_loss: 0.1281\n",
      "Epoch 357/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1282 - val_loss: 0.1156\n",
      "Epoch 358/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1284 - val_loss: 0.1344\n",
      "Epoch 359/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1302 - val_loss: 0.1300\n",
      "Epoch 360/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1220 - val_loss: 0.2104\n",
      "Epoch 361/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1225 - val_loss: 0.1501\n",
      "Epoch 362/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1186 - val_loss: 0.1487\n",
      "Epoch 363/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1373 - val_loss: 0.1623\n",
      "Epoch 364/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1220 - val_loss: 0.1157\n",
      "Epoch 365/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1256 - val_loss: 0.1325\n",
      "Epoch 366/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1391 - val_loss: 0.1015\n",
      "Epoch 367/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1306 - val_loss: 0.1501\n",
      "Epoch 368/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1230 - val_loss: 0.1470\n",
      "Epoch 369/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1233 - val_loss: 0.1001\n",
      "Epoch 370/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1216 - val_loss: 0.1037\n",
      "Epoch 371/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1244 - val_loss: 0.1101\n",
      "Epoch 372/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1214 - val_loss: 0.1026\n",
      "Epoch 373/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1247 - val_loss: 0.1263\n",
      "Epoch 374/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1286 - val_loss: 0.1228\n",
      "Epoch 375/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1318 - val_loss: 0.1267\n",
      "Epoch 376/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1264 - val_loss: 0.1481\n",
      "Epoch 377/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1256 - val_loss: 0.0999\n",
      "Epoch 378/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1242 - val_loss: 0.1464\n",
      "Epoch 379/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1250 - val_loss: 0.1473\n",
      "Epoch 380/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1369 - val_loss: 0.1280\n",
      "Epoch 381/1000\n",
      "6694/6694 [==============================] - 0s 72us/step - loss: 0.1233 - val_loss: 0.1216\n",
      "Epoch 382/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1161 - val_loss: 0.1226\n",
      "Epoch 383/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1357 - val_loss: 0.1167\n",
      "Epoch 384/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1248 - val_loss: 0.1047\n",
      "Epoch 385/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1330 - val_loss: 0.1215\n",
      "Epoch 386/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1233 - val_loss: 0.1537\n",
      "Epoch 387/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1228 - val_loss: 0.0911\n",
      "Epoch 388/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1250 - val_loss: 0.1060\n",
      "Epoch 389/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1315 - val_loss: 0.1136\n",
      "Epoch 390/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1276 - val_loss: 0.1212\n",
      "Epoch 391/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1376 - val_loss: 0.1753\n",
      "Epoch 392/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1237 - val_loss: 0.1516\n",
      "Epoch 393/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1219 - val_loss: 0.1129\n",
      "Epoch 394/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1199 - val_loss: 0.1224\n",
      "Epoch 395/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1288 - val_loss: 0.1227\n",
      "Epoch 396/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1229 - val_loss: 0.1489\n",
      "Epoch 397/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1292 - val_loss: 0.1308\n",
      "Epoch 398/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1310 - val_loss: 0.1262\n",
      "Epoch 399/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1274 - val_loss: 0.1148\n",
      "Epoch 400/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1214 - val_loss: 0.1484\n",
      "\n",
      "Epoch 00400: saving model to ./log_weights/Ide_AE_weights.0400.hdf5\n",
      "Epoch 401/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1296 - val_loss: 0.1421\n",
      "Epoch 402/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1269 - val_loss: 0.1148\n",
      "Epoch 403/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1178 - val_loss: 0.1125\n",
      "Epoch 404/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1285 - val_loss: 0.1298\n",
      "Epoch 405/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1223 - val_loss: 0.1182\n",
      "Epoch 406/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1272 - val_loss: 0.1582\n",
      "Epoch 407/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1259 - val_loss: 0.1097\n",
      "Epoch 408/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1193 - val_loss: 0.1082\n",
      "Epoch 409/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1277 - val_loss: 0.1327\n",
      "Epoch 410/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1278 - val_loss: 0.0990\n",
      "Epoch 411/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1276 - val_loss: 0.1170\n",
      "Epoch 412/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1278 - val_loss: 0.1225\n",
      "Epoch 413/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1310 - val_loss: 0.1154\n",
      "Epoch 414/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1256 - val_loss: 0.1487\n",
      "Epoch 415/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1278 - val_loss: 0.1830\n",
      "Epoch 416/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1303 - val_loss: 0.1321\n",
      "Epoch 417/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1220 - val_loss: 0.1066\n",
      "Epoch 418/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1248 - val_loss: 0.1183\n",
      "Epoch 419/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1249 - val_loss: 0.1285\n",
      "Epoch 420/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1229 - val_loss: 0.1580\n",
      "Epoch 421/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1303 - val_loss: 0.1448\n",
      "Epoch 422/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1274 - val_loss: 0.1425\n",
      "Epoch 423/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1294 - val_loss: 0.1052\n",
      "Epoch 424/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1231 - val_loss: 0.1034\n",
      "Epoch 425/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1299 - val_loss: 0.1214\n",
      "Epoch 426/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1298 - val_loss: 0.1048\n",
      "Epoch 427/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1215 - val_loss: 0.1018\n",
      "Epoch 428/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1411 - val_loss: 0.1610\n",
      "Epoch 429/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1349 - val_loss: 0.1004\n",
      "Epoch 430/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1245 - val_loss: 0.1171\n",
      "Epoch 431/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1260 - val_loss: 0.1023\n",
      "Epoch 432/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1251 - val_loss: 0.1062\n",
      "Epoch 433/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1276 - val_loss: 0.2299\n",
      "Epoch 434/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1356 - val_loss: 0.1293\n",
      "Epoch 435/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1230 - val_loss: 0.1347\n",
      "Epoch 436/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1266 - val_loss: 0.1120\n",
      "Epoch 437/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1263 - val_loss: 0.1114\n",
      "Epoch 438/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1163 - val_loss: 0.1169\n",
      "Epoch 439/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1345 - val_loss: 0.1113\n",
      "Epoch 440/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1332 - val_loss: 0.1314\n",
      "Epoch 441/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1151 - val_loss: 0.1070\n",
      "Epoch 442/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1344 - val_loss: 0.1181\n",
      "Epoch 443/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1287 - val_loss: 0.1258\n",
      "Epoch 444/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1236 - val_loss: 0.1257\n",
      "Epoch 445/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1251 - val_loss: 0.1741\n",
      "Epoch 446/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1215 - val_loss: 0.1016\n",
      "Epoch 447/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1278 - val_loss: 0.1270\n",
      "Epoch 448/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1353 - val_loss: 0.1062\n",
      "Epoch 449/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1277 - val_loss: 0.1211\n",
      "Epoch 450/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1236 - val_loss: 0.1758\n",
      "Epoch 451/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1244 - val_loss: 0.1686\n",
      "Epoch 452/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1302 - val_loss: 0.1267\n",
      "Epoch 453/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1206 - val_loss: 0.1169\n",
      "Epoch 454/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1314 - val_loss: 0.1412\n",
      "Epoch 455/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1177 - val_loss: 0.1024\n",
      "Epoch 456/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1291 - val_loss: 0.0937\n",
      "Epoch 457/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1210 - val_loss: 0.1303\n",
      "Epoch 458/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1303 - val_loss: 0.1109\n",
      "Epoch 459/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1300 - val_loss: 0.1167\n",
      "Epoch 460/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1267 - val_loss: 0.1107\n",
      "Epoch 461/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1231 - val_loss: 0.1466\n",
      "Epoch 462/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1303 - val_loss: 0.1290\n",
      "Epoch 463/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1252 - val_loss: 0.1034\n",
      "Epoch 464/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1249 - val_loss: 0.1169\n",
      "Epoch 465/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1300 - val_loss: 0.1048\n",
      "Epoch 466/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1182 - val_loss: 0.1388\n",
      "Epoch 467/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1317 - val_loss: 0.1425\n",
      "Epoch 468/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1302 - val_loss: 0.1585\n",
      "Epoch 469/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1269 - val_loss: 0.1512\n",
      "Epoch 470/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1255 - val_loss: 0.1048\n",
      "Epoch 471/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1245 - val_loss: 0.1470\n",
      "Epoch 472/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1304 - val_loss: 0.1106\n",
      "Epoch 473/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1291 - val_loss: 0.1362\n",
      "Epoch 474/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1271 - val_loss: 0.1108\n",
      "Epoch 475/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1274 - val_loss: 0.1149\n",
      "Epoch 476/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1229 - val_loss: 0.1248\n",
      "Epoch 477/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1280 - val_loss: 0.1325\n",
      "Epoch 478/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1221 - val_loss: 0.1192\n",
      "Epoch 479/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1292 - val_loss: 0.1390\n",
      "Epoch 480/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1214 - val_loss: 0.1034\n",
      "Epoch 481/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1249 - val_loss: 0.1157\n",
      "Epoch 482/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1306 - val_loss: 0.1392\n",
      "Epoch 483/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1292 - val_loss: 0.1309\n",
      "Epoch 484/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1230 - val_loss: 0.1435\n",
      "Epoch 485/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1324 - val_loss: 0.1020\n",
      "Epoch 486/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1241 - val_loss: 0.1307\n",
      "Epoch 487/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1271 - val_loss: 0.1058\n",
      "Epoch 488/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1195 - val_loss: 0.1179\n",
      "Epoch 489/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1231 - val_loss: 0.1239\n",
      "Epoch 490/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1348 - val_loss: 0.1406\n",
      "Epoch 491/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1195 - val_loss: 0.1464\n",
      "Epoch 492/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1295 - val_loss: 0.2193\n",
      "Epoch 493/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1196 - val_loss: 0.1276\n",
      "Epoch 494/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1313 - val_loss: 0.1357\n",
      "Epoch 495/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1224 - val_loss: 0.1007\n",
      "Epoch 496/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1243 - val_loss: 0.1151\n",
      "Epoch 497/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1280 - val_loss: 0.1700\n",
      "Epoch 498/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1223 - val_loss: 0.1395\n",
      "Epoch 499/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1242 - val_loss: 0.1152\n",
      "Epoch 500/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1219 - val_loss: 0.1175\n",
      "\n",
      "Epoch 00500: saving model to ./log_weights/Ide_AE_weights.0500.hdf5\n",
      "Epoch 501/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1281 - val_loss: 0.1258\n",
      "Epoch 502/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1333 - val_loss: 0.1324\n",
      "Epoch 503/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1287 - val_loss: 0.1283\n",
      "Epoch 504/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1218 - val_loss: 0.1349\n",
      "Epoch 505/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1249 - val_loss: 0.1384\n",
      "Epoch 506/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1301 - val_loss: 0.1149\n",
      "Epoch 507/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1231 - val_loss: 0.1539\n",
      "Epoch 508/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1238 - val_loss: 0.0987\n",
      "Epoch 509/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1284 - val_loss: 0.1304\n",
      "Epoch 510/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1256 - val_loss: 0.1420\n",
      "Epoch 511/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1227 - val_loss: 0.1325\n",
      "Epoch 512/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1242 - val_loss: 0.1205\n",
      "Epoch 513/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1269 - val_loss: 0.1155\n",
      "Epoch 514/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1312 - val_loss: 0.1206\n",
      "Epoch 515/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1240 - val_loss: 0.1333\n",
      "Epoch 516/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1304 - val_loss: 0.0976\n",
      "Epoch 517/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1383 - val_loss: 0.1250\n",
      "Epoch 518/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1276 - val_loss: 0.1081\n",
      "Epoch 519/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1202 - val_loss: 0.1141\n",
      "Epoch 520/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1237 - val_loss: 0.1263\n",
      "Epoch 521/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1265 - val_loss: 0.1081\n",
      "Epoch 522/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1276 - val_loss: 0.1077\n",
      "Epoch 523/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1191 - val_loss: 0.1208\n",
      "Epoch 524/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1294 - val_loss: 0.1831\n",
      "Epoch 525/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1280 - val_loss: 0.1203\n",
      "Epoch 526/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1299 - val_loss: 0.1017\n",
      "Epoch 527/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1222 - val_loss: 0.0983\n",
      "Epoch 528/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1300 - val_loss: 0.1161\n",
      "Epoch 529/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1272 - val_loss: 0.1063\n",
      "Epoch 530/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1226 - val_loss: 0.1341\n",
      "Epoch 531/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1324 - val_loss: 0.1414\n",
      "Epoch 532/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1402 - val_loss: 0.1046\n",
      "Epoch 533/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1246 - val_loss: 0.1459\n",
      "Epoch 534/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1341 - val_loss: 0.1142\n",
      "Epoch 535/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1318 - val_loss: 0.1264\n",
      "Epoch 536/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1203 - val_loss: 0.1045\n",
      "Epoch 537/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1279 - val_loss: 0.1090\n",
      "Epoch 538/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1287 - val_loss: 0.1315\n",
      "Epoch 539/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1254 - val_loss: 0.1150\n",
      "Epoch 540/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1274 - val_loss: 0.1099\n",
      "Epoch 541/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1247 - val_loss: 0.1064\n",
      "Epoch 542/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1258 - val_loss: 0.1248\n",
      "Epoch 543/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1227 - val_loss: 0.1322\n",
      "Epoch 544/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1273 - val_loss: 0.1582\n",
      "Epoch 545/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1308 - val_loss: 0.1020\n",
      "Epoch 546/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1190 - val_loss: 0.1322\n",
      "Epoch 547/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1207 - val_loss: 0.1705\n",
      "Epoch 548/1000\n",
      "6694/6694 [==============================] - 0s 52us/step - loss: 0.1418 - val_loss: 0.1117\n",
      "Epoch 549/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1239 - val_loss: 0.1308\n",
      "Epoch 550/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1325 - val_loss: 0.1708\n",
      "Epoch 551/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1283 - val_loss: 0.1004\n",
      "Epoch 552/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1255 - val_loss: 0.1203\n",
      "Epoch 553/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1230 - val_loss: 0.1045\n",
      "Epoch 554/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1328 - val_loss: 0.1223\n",
      "Epoch 555/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1327 - val_loss: 0.1749\n",
      "Epoch 556/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1242 - val_loss: 0.1090\n",
      "Epoch 557/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1232 - val_loss: 0.1374\n",
      "Epoch 558/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1335 - val_loss: 0.1040\n",
      "Epoch 559/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1237 - val_loss: 0.1140\n",
      "Epoch 560/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1306 - val_loss: 0.1008\n",
      "Epoch 561/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1254 - val_loss: 0.1610\n",
      "Epoch 562/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1266 - val_loss: 0.1513\n",
      "Epoch 563/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1233 - val_loss: 0.1332\n",
      "Epoch 564/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1268 - val_loss: 0.1720\n",
      "Epoch 565/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1237 - val_loss: 0.1474\n",
      "Epoch 566/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1335 - val_loss: 0.1652\n",
      "Epoch 567/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1230 - val_loss: 0.1159\n",
      "Epoch 568/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1204 - val_loss: 0.1058\n",
      "Epoch 569/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1271 - val_loss: 0.1334\n",
      "Epoch 570/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1275 - val_loss: 0.1280\n",
      "Epoch 571/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1272 - val_loss: 0.1215\n",
      "Epoch 572/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1211 - val_loss: 0.1126\n",
      "Epoch 573/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1290 - val_loss: 0.1123\n",
      "Epoch 574/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1295 - val_loss: 0.1494\n",
      "Epoch 575/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1217 - val_loss: 0.1351\n",
      "Epoch 576/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1280 - val_loss: 0.1287\n",
      "Epoch 577/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1305 - val_loss: 0.1381\n",
      "Epoch 578/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1283 - val_loss: 0.1619\n",
      "Epoch 579/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1303 - val_loss: 0.1472\n",
      "Epoch 580/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1301 - val_loss: 0.1455\n",
      "Epoch 581/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1231 - val_loss: 0.1203\n",
      "Epoch 582/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1261 - val_loss: 0.1168\n",
      "Epoch 583/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1314 - val_loss: 0.1672\n",
      "Epoch 584/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1313 - val_loss: 0.1357\n",
      "Epoch 585/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1252 - val_loss: 0.1060\n",
      "Epoch 586/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1223 - val_loss: 0.2135\n",
      "Epoch 587/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1298 - val_loss: 0.0983\n",
      "Epoch 588/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1342 - val_loss: 0.1203\n",
      "Epoch 589/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1377 - val_loss: 0.1359\n",
      "Epoch 590/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1175 - val_loss: 0.1075\n",
      "Epoch 591/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1276 - val_loss: 0.1139\n",
      "Epoch 592/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1284 - val_loss: 0.1589\n",
      "Epoch 593/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1235 - val_loss: 0.1440\n",
      "Epoch 594/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1198 - val_loss: 0.1287\n",
      "Epoch 595/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1209 - val_loss: 0.1478\n",
      "Epoch 596/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1317 - val_loss: 0.1208\n",
      "Epoch 597/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1340 - val_loss: 0.1804\n",
      "Epoch 598/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1284 - val_loss: 0.1443\n",
      "Epoch 599/1000\n",
      "6694/6694 [==============================] - ETA: 0s - loss: 0.130 - 0s 62us/step - loss: 0.1303 - val_loss: 0.1424\n",
      "Epoch 600/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1281 - val_loss: 0.2301\n",
      "\n",
      "Epoch 00600: saving model to ./log_weights/Ide_AE_weights.0600.hdf5\n",
      "Epoch 601/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1230 - val_loss: 0.1176\n",
      "Epoch 602/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1270 - val_loss: 0.1182\n",
      "Epoch 603/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1247 - val_loss: 0.1422\n",
      "Epoch 604/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1262 - val_loss: 0.1192\n",
      "Epoch 605/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1294 - val_loss: 0.1878\n",
      "Epoch 606/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1324 - val_loss: 0.1271\n",
      "Epoch 607/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1220 - val_loss: 0.1175\n",
      "Epoch 608/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1284 - val_loss: 0.1215\n",
      "Epoch 609/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1263 - val_loss: 0.1156\n",
      "Epoch 610/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1286 - val_loss: 0.1055\n",
      "Epoch 611/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1207 - val_loss: 0.1203\n",
      "Epoch 612/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1253 - val_loss: 0.1267\n",
      "Epoch 613/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1168 - val_loss: 0.1178\n",
      "Epoch 614/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1340 - val_loss: 0.1443\n",
      "Epoch 615/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1320 - val_loss: 0.1376\n",
      "Epoch 616/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1295 - val_loss: 0.1318\n",
      "Epoch 617/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1248 - val_loss: 0.1312\n",
      "Epoch 618/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1243 - val_loss: 0.1263\n",
      "Epoch 619/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1220 - val_loss: 0.1656\n",
      "Epoch 620/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1229 - val_loss: 0.1859\n",
      "Epoch 621/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1261 - val_loss: 0.1195\n",
      "Epoch 622/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1234 - val_loss: 0.1346\n",
      "Epoch 623/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1321 - val_loss: 0.1413\n",
      "Epoch 624/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1249 - val_loss: 0.1087\n",
      "Epoch 625/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1234 - val_loss: 0.1618\n",
      "Epoch 626/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1284 - val_loss: 0.1408\n",
      "Epoch 627/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1212 - val_loss: 0.1160\n",
      "Epoch 628/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1260 - val_loss: 0.1154\n",
      "Epoch 629/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1318 - val_loss: 0.0998\n",
      "Epoch 630/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1204 - val_loss: 0.1127\n",
      "Epoch 631/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1277 - val_loss: 0.1207\n",
      "Epoch 632/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1309 - val_loss: 0.0983\n",
      "Epoch 633/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1314 - val_loss: 0.1485\n",
      "Epoch 634/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1283 - val_loss: 0.1391\n",
      "Epoch 635/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1263 - val_loss: 0.1211\n",
      "Epoch 636/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1198 - val_loss: 0.1201\n",
      "Epoch 637/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1241 - val_loss: 0.1371\n",
      "Epoch 638/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1271 - val_loss: 0.1412\n",
      "Epoch 639/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1174 - val_loss: 0.1186\n",
      "Epoch 640/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1406 - val_loss: 0.1128\n",
      "Epoch 641/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1245 - val_loss: 0.1674\n",
      "Epoch 642/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1272 - val_loss: 0.1382\n",
      "Epoch 643/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1193 - val_loss: 0.1121\n",
      "Epoch 644/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1269 - val_loss: 0.1275\n",
      "Epoch 645/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1272 - val_loss: 0.1428\n",
      "Epoch 646/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1234 - val_loss: 0.1162\n",
      "Epoch 647/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1224 - val_loss: 0.1744\n",
      "Epoch 648/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1282 - val_loss: 0.1209\n",
      "Epoch 649/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1287 - val_loss: 0.1063\n",
      "Epoch 650/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1279 - val_loss: 0.1631\n",
      "Epoch 651/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1277 - val_loss: 0.1035\n",
      "Epoch 652/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1192 - val_loss: 0.1214\n",
      "Epoch 653/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1245 - val_loss: 0.1289\n",
      "Epoch 654/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1230 - val_loss: 0.1135\n",
      "Epoch 655/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1228 - val_loss: 0.1456\n",
      "Epoch 656/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1225 - val_loss: 0.1148\n",
      "Epoch 657/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1216 - val_loss: 0.1096\n",
      "Epoch 658/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1267 - val_loss: 0.1504\n",
      "Epoch 659/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1293 - val_loss: 0.1263\n",
      "Epoch 660/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1359 - val_loss: 0.1317\n",
      "Epoch 661/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1266 - val_loss: 0.1103\n",
      "Epoch 662/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1264 - val_loss: 0.1239\n",
      "Epoch 663/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1238 - val_loss: 0.1018\n",
      "Epoch 664/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1256 - val_loss: 0.1458\n",
      "Epoch 665/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1324 - val_loss: 0.1422\n",
      "Epoch 666/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1212 - val_loss: 0.1400\n",
      "Epoch 667/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1266 - val_loss: 0.1225\n",
      "Epoch 668/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1248 - val_loss: 0.1117\n",
      "Epoch 669/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1187 - val_loss: 0.1039\n",
      "Epoch 670/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1357 - val_loss: 0.1037\n",
      "Epoch 671/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1203 - val_loss: 0.1104\n",
      "Epoch 672/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1330 - val_loss: 0.1062\n",
      "Epoch 673/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1251 - val_loss: 0.1169\n",
      "Epoch 674/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1324 - val_loss: 0.1556\n",
      "Epoch 675/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1225 - val_loss: 0.1187\n",
      "Epoch 676/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1246 - val_loss: 0.1029\n",
      "Epoch 677/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1325 - val_loss: 0.1407\n",
      "Epoch 678/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1272 - val_loss: 0.1068\n",
      "Epoch 679/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1244 - val_loss: 0.1408\n",
      "Epoch 680/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1240 - val_loss: 0.1303\n",
      "Epoch 681/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1204 - val_loss: 0.1406\n",
      "Epoch 682/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1278 - val_loss: 0.1449\n",
      "Epoch 683/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1284 - val_loss: 0.1032\n",
      "Epoch 684/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1235 - val_loss: 0.1147\n",
      "Epoch 685/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1224 - val_loss: 0.1155\n",
      "Epoch 686/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1266 - val_loss: 0.1090\n",
      "Epoch 687/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1300 - val_loss: 0.1141\n",
      "Epoch 688/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1243 - val_loss: 0.1278\n",
      "Epoch 689/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1293 - val_loss: 0.1095\n",
      "Epoch 690/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1272 - val_loss: 0.1203\n",
      "Epoch 691/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1229 - val_loss: 0.1344\n",
      "Epoch 692/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1205 - val_loss: 0.1132\n",
      "Epoch 693/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1289 - val_loss: 0.1614\n",
      "Epoch 694/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1197 - val_loss: 0.1041\n",
      "Epoch 695/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1223 - val_loss: 0.1204\n",
      "Epoch 696/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1322 - val_loss: 0.1147\n",
      "Epoch 697/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1391 - val_loss: 0.1331\n",
      "Epoch 698/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1322 - val_loss: 0.1371\n",
      "Epoch 699/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1317 - val_loss: 0.1077\n",
      "Epoch 700/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1188 - val_loss: 0.1300\n",
      "\n",
      "Epoch 00700: saving model to ./log_weights/Ide_AE_weights.0700.hdf5\n",
      "Epoch 701/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1218 - val_loss: 0.0989\n",
      "Epoch 702/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1356 - val_loss: 0.1289\n",
      "Epoch 703/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1263 - val_loss: 0.1075\n",
      "Epoch 704/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1265 - val_loss: 0.1323\n",
      "Epoch 705/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1262 - val_loss: 0.1266\n",
      "Epoch 706/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1212 - val_loss: 0.1448\n",
      "Epoch 707/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1299 - val_loss: 0.1144\n",
      "Epoch 708/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1211 - val_loss: 0.1428\n",
      "Epoch 709/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1163 - val_loss: 0.1225\n",
      "Epoch 710/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1252 - val_loss: 0.1298\n",
      "Epoch 711/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1252 - val_loss: 0.1195\n",
      "Epoch 712/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1286 - val_loss: 0.1541\n",
      "Epoch 713/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1260 - val_loss: 0.1363\n",
      "Epoch 714/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1257 - val_loss: 0.0987\n",
      "Epoch 715/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1330 - val_loss: 0.1379\n",
      "Epoch 716/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1280 - val_loss: 0.1361\n",
      "Epoch 717/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1244 - val_loss: 0.1423\n",
      "Epoch 718/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1236 - val_loss: 0.1128\n",
      "Epoch 719/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1271 - val_loss: 0.1090\n",
      "Epoch 720/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1212 - val_loss: 0.1331\n",
      "Epoch 721/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1288 - val_loss: 0.1314\n",
      "Epoch 722/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1262 - val_loss: 0.1990\n",
      "Epoch 723/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1394 - val_loss: 0.1669\n",
      "Epoch 724/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1249 - val_loss: 0.1274\n",
      "Epoch 725/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1275 - val_loss: 0.1086\n",
      "Epoch 726/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1204 - val_loss: 0.1401\n",
      "Epoch 727/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1161 - val_loss: 0.1388\n",
      "Epoch 728/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1355 - val_loss: 0.1490\n",
      "Epoch 729/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1285 - val_loss: 0.1159\n",
      "Epoch 730/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1220 - val_loss: 0.1270\n",
      "Epoch 731/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1264 - val_loss: 0.1087\n",
      "Epoch 732/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1165 - val_loss: 0.1155\n",
      "Epoch 733/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1281 - val_loss: 0.1054\n",
      "Epoch 734/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1272 - val_loss: 0.1286\n",
      "Epoch 735/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1361 - val_loss: 0.1237\n",
      "Epoch 736/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1202 - val_loss: 0.2441\n",
      "Epoch 737/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1290 - val_loss: 0.1106\n",
      "Epoch 738/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1397 - val_loss: 0.1786\n",
      "Epoch 739/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1173 - val_loss: 0.1261\n",
      "Epoch 740/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1315 - val_loss: 0.1280\n",
      "Epoch 741/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1266 - val_loss: 0.1223\n",
      "Epoch 742/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1284 - val_loss: 0.1225\n",
      "Epoch 743/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1187 - val_loss: 0.1260\n",
      "Epoch 744/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1188 - val_loss: 0.1252\n",
      "Epoch 745/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1290 - val_loss: 0.1096\n",
      "Epoch 746/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1306 - val_loss: 0.1385\n",
      "Epoch 747/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1213 - val_loss: 0.1197\n",
      "Epoch 748/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1296 - val_loss: 0.1110\n",
      "Epoch 749/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1271 - val_loss: 0.1065\n",
      "Epoch 750/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1219 - val_loss: 0.1143\n",
      "Epoch 751/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1335 - val_loss: 0.1907\n",
      "Epoch 752/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1311 - val_loss: 0.1519\n",
      "Epoch 753/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1293 - val_loss: 0.1222\n",
      "Epoch 754/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1282 - val_loss: 0.1235\n",
      "Epoch 755/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1225 - val_loss: 0.1054\n",
      "Epoch 756/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1227 - val_loss: 0.1284\n",
      "Epoch 757/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1239 - val_loss: 0.1113\n",
      "Epoch 758/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1331 - val_loss: 0.1164\n",
      "Epoch 759/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1327 - val_loss: 0.1038\n",
      "Epoch 760/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1265 - val_loss: 0.1128\n",
      "Epoch 761/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1372 - val_loss: 0.1240\n",
      "Epoch 762/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1162 - val_loss: 0.1059\n",
      "Epoch 763/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1252 - val_loss: 0.1452\n",
      "Epoch 764/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1276 - val_loss: 0.1507\n",
      "Epoch 765/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1251 - val_loss: 0.1316\n",
      "Epoch 766/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1275 - val_loss: 0.1262\n",
      "Epoch 767/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1292 - val_loss: 0.1322\n",
      "Epoch 768/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1368 - val_loss: 0.1260\n",
      "Epoch 769/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1243 - val_loss: 0.1103\n",
      "Epoch 770/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1305 - val_loss: 0.1474\n",
      "Epoch 771/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1184 - val_loss: 0.1760\n",
      "Epoch 772/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1149 - val_loss: 0.1177\n",
      "Epoch 773/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1267 - val_loss: 0.1021\n",
      "Epoch 774/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1281 - val_loss: 0.0972\n",
      "Epoch 775/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1288 - val_loss: 0.1236\n",
      "Epoch 776/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1273 - val_loss: 0.1738\n",
      "Epoch 777/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1289 - val_loss: 0.1053\n",
      "Epoch 778/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1259 - val_loss: 0.1294\n",
      "Epoch 779/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1327 - val_loss: 0.1224\n",
      "Epoch 780/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1146 - val_loss: 0.1306\n",
      "Epoch 781/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1343 - val_loss: 0.1313\n",
      "Epoch 782/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1281 - val_loss: 0.1233\n",
      "Epoch 783/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1196 - val_loss: 0.1729\n",
      "Epoch 784/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1274 - val_loss: 0.1430\n",
      "Epoch 785/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1211 - val_loss: 0.0992\n",
      "Epoch 786/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1254 - val_loss: 0.1099\n",
      "Epoch 787/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1256 - val_loss: 0.1335\n",
      "Epoch 788/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1322 - val_loss: 0.1240\n",
      "Epoch 789/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1273 - val_loss: 0.1098\n",
      "Epoch 790/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1196 - val_loss: 0.1020\n",
      "Epoch 791/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1316 - val_loss: 0.1143\n",
      "Epoch 792/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1236 - val_loss: 0.1174\n",
      "Epoch 793/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1202 - val_loss: 0.1247\n",
      "Epoch 794/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1321 - val_loss: 0.1167\n",
      "Epoch 795/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1334 - val_loss: 0.1436\n",
      "Epoch 796/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1273 - val_loss: 0.1379\n",
      "Epoch 797/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1287 - val_loss: 0.1078\n",
      "Epoch 798/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1195 - val_loss: 0.1540\n",
      "Epoch 799/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1224 - val_loss: 0.1121\n",
      "Epoch 800/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1354 - val_loss: 0.1172\n",
      "\n",
      "Epoch 00800: saving model to ./log_weights/Ide_AE_weights.0800.hdf5\n",
      "Epoch 801/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1250 - val_loss: 0.1108\n",
      "Epoch 802/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1249 - val_loss: 0.1229\n",
      "Epoch 803/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1196 - val_loss: 0.1334\n",
      "Epoch 804/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1199 - val_loss: 0.1319\n",
      "Epoch 805/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1242 - val_loss: 0.1444\n",
      "Epoch 806/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1243 - val_loss: 0.1910\n",
      "Epoch 807/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1332 - val_loss: 0.1338\n",
      "Epoch 808/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1254 - val_loss: 0.1085\n",
      "Epoch 809/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1395 - val_loss: 0.1077\n",
      "Epoch 810/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1230 - val_loss: 0.1257\n",
      "Epoch 811/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1272 - val_loss: 0.1308\n",
      "Epoch 812/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1214 - val_loss: 0.1005\n",
      "Epoch 813/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1295 - val_loss: 0.1203\n",
      "Epoch 814/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1179 - val_loss: 0.1083\n",
      "Epoch 815/1000\n",
      "6694/6694 [==============================] - 0s 55us/step - loss: 0.1217 - val_loss: 0.1293\n",
      "Epoch 816/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1241 - val_loss: 0.0965\n",
      "Epoch 817/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1271 - val_loss: 0.1808\n",
      "Epoch 818/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1259 - val_loss: 0.1389\n",
      "Epoch 819/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1280 - val_loss: 0.1853\n",
      "Epoch 820/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1203 - val_loss: 0.1363\n",
      "Epoch 821/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1239 - val_loss: 0.1236\n",
      "Epoch 822/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1268 - val_loss: 0.1170\n",
      "Epoch 823/1000\n",
      "6694/6694 [==============================] - 0s 56us/step - loss: 0.1313 - val_loss: 0.1553\n",
      "Epoch 824/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1244 - val_loss: 0.1253\n",
      "Epoch 825/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1352 - val_loss: 0.1143\n",
      "Epoch 826/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1296 - val_loss: 0.1226\n",
      "Epoch 827/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1208 - val_loss: 0.1387\n",
      "Epoch 828/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1271 - val_loss: 0.1186\n",
      "Epoch 829/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1292 - val_loss: 0.1662\n",
      "Epoch 830/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1285 - val_loss: 0.1075\n",
      "Epoch 831/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1233 - val_loss: 0.1002\n",
      "Epoch 832/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1142 - val_loss: 0.1200\n",
      "Epoch 833/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1232 - val_loss: 0.1163\n",
      "Epoch 834/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1417 - val_loss: 0.1252\n",
      "Epoch 835/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1339 - val_loss: 0.1280\n",
      "Epoch 836/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1243 - val_loss: 0.1328\n",
      "Epoch 837/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1283 - val_loss: 0.1443\n",
      "Epoch 838/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1249 - val_loss: 0.1040\n",
      "Epoch 839/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1207 - val_loss: 0.2149\n",
      "Epoch 840/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1274 - val_loss: 0.1227\n",
      "Epoch 841/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1238 - val_loss: 0.1343\n",
      "Epoch 842/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1211 - val_loss: 0.1326\n",
      "Epoch 843/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1305 - val_loss: 0.0990\n",
      "Epoch 844/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1187 - val_loss: 0.1633\n",
      "Epoch 845/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1349 - val_loss: 0.1502\n",
      "Epoch 846/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1196 - val_loss: 0.0989\n",
      "Epoch 847/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1321 - val_loss: 0.1161\n",
      "Epoch 848/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1224 - val_loss: 0.1112\n",
      "Epoch 849/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1256 - val_loss: 0.1145\n",
      "Epoch 850/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1196 - val_loss: 0.1187\n",
      "Epoch 851/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1197 - val_loss: 0.1338\n",
      "Epoch 852/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1196 - val_loss: 0.1116\n",
      "Epoch 853/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1318 - val_loss: 0.1395\n",
      "Epoch 854/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1287 - val_loss: 0.1637\n",
      "Epoch 855/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1254 - val_loss: 0.1636\n",
      "Epoch 856/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1293 - val_loss: 0.1707\n",
      "Epoch 857/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1218 - val_loss: 0.1427\n",
      "Epoch 858/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1295 - val_loss: 0.1447\n",
      "Epoch 859/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1338 - val_loss: 0.1395\n",
      "Epoch 860/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1237 - val_loss: 0.1297\n",
      "Epoch 861/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1299 - val_loss: 0.1400\n",
      "Epoch 862/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1243 - val_loss: 0.1131\n",
      "Epoch 863/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1258 - val_loss: 0.1132\n",
      "Epoch 864/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1278 - val_loss: 0.1608\n",
      "Epoch 865/1000\n",
      "6694/6694 [==============================] - 0s 53us/step - loss: 0.1293 - val_loss: 0.1192\n",
      "Epoch 866/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1265 - val_loss: 0.1201\n",
      "Epoch 867/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1175 - val_loss: 0.1336\n",
      "Epoch 868/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1348 - val_loss: 0.1388\n",
      "Epoch 869/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1165 - val_loss: 0.1558\n",
      "Epoch 870/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1282 - val_loss: 0.1088\n",
      "Epoch 871/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1225 - val_loss: 0.1069\n",
      "Epoch 872/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1293 - val_loss: 0.1235\n",
      "Epoch 873/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1241 - val_loss: 0.1074\n",
      "Epoch 874/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1269 - val_loss: 0.1310\n",
      "Epoch 875/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1271 - val_loss: 0.1504\n",
      "Epoch 876/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1341 - val_loss: 0.1199\n",
      "Epoch 877/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1194 - val_loss: 0.1032\n",
      "Epoch 878/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1223 - val_loss: 0.1163\n",
      "Epoch 879/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1290 - val_loss: 0.1120\n",
      "Epoch 880/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1229 - val_loss: 0.1394\n",
      "Epoch 881/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1189 - val_loss: 0.1210\n",
      "Epoch 882/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1367 - val_loss: 0.1097\n",
      "Epoch 883/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1258 - val_loss: 0.1876\n",
      "Epoch 884/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1256 - val_loss: 0.1046\n",
      "Epoch 885/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1208 - val_loss: 0.1891\n",
      "Epoch 886/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1247 - val_loss: 0.1325\n",
      "Epoch 887/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1219 - val_loss: 0.1260\n",
      "Epoch 888/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1279 - val_loss: 0.1059\n",
      "Epoch 889/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1248 - val_loss: 0.1525\n",
      "Epoch 890/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1287 - val_loss: 0.1147\n",
      "Epoch 891/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1241 - val_loss: 0.1300\n",
      "Epoch 892/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1240 - val_loss: 0.1220\n",
      "Epoch 893/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1205 - val_loss: 0.1116\n",
      "Epoch 894/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1255 - val_loss: 0.1064\n",
      "Epoch 895/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1218 - val_loss: 0.1228\n",
      "Epoch 896/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1300 - val_loss: 0.1284\n",
      "Epoch 897/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1238 - val_loss: 0.1149\n",
      "Epoch 898/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1288 - val_loss: 0.1203\n",
      "Epoch 899/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1250 - val_loss: 0.1178\n",
      "Epoch 900/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1258 - val_loss: 0.1185\n",
      "\n",
      "Epoch 00900: saving model to ./log_weights/Ide_AE_weights.0900.hdf5\n",
      "Epoch 901/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1244 - val_loss: 0.1376\n",
      "Epoch 902/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1249 - val_loss: 0.1559\n",
      "Epoch 903/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1261 - val_loss: 0.1043\n",
      "Epoch 904/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1283 - val_loss: 0.1154\n",
      "Epoch 905/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1234 - val_loss: 0.1252\n",
      "Epoch 906/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1171 - val_loss: 0.1027\n",
      "Epoch 907/1000\n",
      "6694/6694 [==============================] - 1s 78us/step - loss: 0.1198 - val_loss: 0.1020\n",
      "Epoch 908/1000\n",
      "6694/6694 [==============================] - 0s 66us/step - loss: 0.1240 - val_loss: 0.1250\n",
      "Epoch 909/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1251 - val_loss: 0.1520\n",
      "Epoch 910/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1234 - val_loss: 0.1868\n",
      "Epoch 911/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1164 - val_loss: 0.1127\n",
      "Epoch 912/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1231 - val_loss: 0.1414\n",
      "Epoch 913/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1250 - val_loss: 0.1266\n",
      "Epoch 914/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1226 - val_loss: 0.1447\n",
      "Epoch 915/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1179 - val_loss: 0.0991\n",
      "Epoch 916/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1258 - val_loss: 0.1311\n",
      "Epoch 917/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1274 - val_loss: 0.1189\n",
      "Epoch 918/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1258 - val_loss: 0.1074\n",
      "Epoch 919/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1269 - val_loss: 0.1826\n",
      "Epoch 920/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1212 - val_loss: 0.1139\n",
      "Epoch 921/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1301 - val_loss: 0.1151\n",
      "Epoch 922/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1213 - val_loss: 0.0919\n",
      "Epoch 923/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1215 - val_loss: 0.1060\n",
      "Epoch 924/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1328 - val_loss: 0.1197\n",
      "Epoch 925/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1193 - val_loss: 0.1701\n",
      "Epoch 926/1000\n",
      "6694/6694 [==============================] - 0s 69us/step - loss: 0.1251 - val_loss: 0.1415\n",
      "Epoch 927/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1397 - val_loss: 0.1226\n",
      "Epoch 928/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1201 - val_loss: 0.1116\n",
      "Epoch 929/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1311 - val_loss: 0.1028\n",
      "Epoch 930/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1234 - val_loss: 0.1330\n",
      "Epoch 931/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1236 - val_loss: 0.1076\n",
      "Epoch 932/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1291 - val_loss: 0.1328\n",
      "Epoch 933/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1230 - val_loss: 0.1175\n",
      "Epoch 934/1000\n",
      "6694/6694 [==============================] - 0s 54us/step - loss: 0.1269 - val_loss: 0.1153\n",
      "Epoch 935/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1240 - val_loss: 0.1135\n",
      "Epoch 936/1000\n",
      "6694/6694 [==============================] - 0s 58us/step - loss: 0.1222 - val_loss: 0.1124\n",
      "Epoch 937/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1316 - val_loss: 0.1352\n",
      "Epoch 938/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1264 - val_loss: 0.1464\n",
      "Epoch 939/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1250 - val_loss: 0.1157\n",
      "Epoch 940/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1172 - val_loss: 0.1370\n",
      "Epoch 941/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1276 - val_loss: 0.1332\n",
      "Epoch 942/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1331 - val_loss: 0.1035\n",
      "Epoch 943/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1216 - val_loss: 0.1413\n",
      "Epoch 944/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1264 - val_loss: 0.1144\n",
      "Epoch 945/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1305 - val_loss: 0.1124\n",
      "Epoch 946/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1188 - val_loss: 0.1272\n",
      "Epoch 947/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1312 - val_loss: 0.1546\n",
      "Epoch 948/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1277 - val_loss: 0.1050\n",
      "Epoch 949/1000\n",
      "6694/6694 [==============================] - 0s 59us/step - loss: 0.1240 - val_loss: 0.1429\n",
      "Epoch 950/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1211 - val_loss: 0.1156\n",
      "Epoch 951/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1300 - val_loss: 0.1069\n",
      "Epoch 952/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1238 - val_loss: 0.1245\n",
      "Epoch 953/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1315 - val_loss: 0.1191\n",
      "Epoch 954/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1211 - val_loss: 0.1342\n",
      "Epoch 955/1000\n",
      "6694/6694 [==============================] - 0s 60us/step - loss: 0.1257 - val_loss: 0.1261\n",
      "Epoch 956/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1248 - val_loss: 0.1483\n",
      "Epoch 957/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1227 - val_loss: 0.1390\n",
      "Epoch 958/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1200 - val_loss: 0.1396\n",
      "Epoch 959/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1323 - val_loss: 0.1152\n",
      "Epoch 960/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1326 - val_loss: 0.1336\n",
      "Epoch 961/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1278 - val_loss: 0.0990\n",
      "Epoch 962/1000\n",
      "6694/6694 [==============================] - 0s 57us/step - loss: 0.1285 - val_loss: 0.1269\n",
      "Epoch 963/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1137 - val_loss: 0.1306\n",
      "Epoch 964/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1225 - val_loss: 0.1268\n",
      "Epoch 965/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1269 - val_loss: 0.1297\n",
      "Epoch 966/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1223 - val_loss: 0.1415\n",
      "Epoch 967/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1304 - val_loss: 0.1085\n",
      "Epoch 968/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1273 - val_loss: 0.1102\n",
      "Epoch 969/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1165 - val_loss: 0.1237\n",
      "Epoch 970/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1282 - val_loss: 0.1306\n",
      "Epoch 971/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1266 - val_loss: 0.1115\n",
      "Epoch 972/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1292 - val_loss: 0.1466\n",
      "Epoch 973/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1283 - val_loss: 0.1194\n",
      "Epoch 974/1000\n",
      "6694/6694 [==============================] - 0s 65us/step - loss: 0.1346 - val_loss: 0.1141\n",
      "Epoch 975/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1214 - val_loss: 0.1504\n",
      "Epoch 976/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1291 - val_loss: 0.1432\n",
      "Epoch 977/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1211 - val_loss: 0.1215\n",
      "Epoch 978/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1206 - val_loss: 0.1034\n",
      "Epoch 979/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1292 - val_loss: 0.1994\n",
      "Epoch 980/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1252 - val_loss: 0.1171\n",
      "Epoch 981/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1230 - val_loss: 0.0943\n",
      "Epoch 982/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1222 - val_loss: 0.1130\n",
      "Epoch 983/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1238 - val_loss: 0.1221\n",
      "Epoch 984/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1348 - val_loss: 0.1409\n",
      "Epoch 985/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1197 - val_loss: 0.1208\n",
      "Epoch 986/1000\n",
      "6694/6694 [==============================] - 0s 62us/step - loss: 0.1294 - val_loss: 0.1912\n",
      "Epoch 987/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1268 - val_loss: 0.1107\n",
      "Epoch 988/1000\n",
      "6694/6694 [==============================] - 0s 70us/step - loss: 0.1215 - val_loss: 0.1092\n",
      "Epoch 989/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1253 - val_loss: 0.1363\n",
      "Epoch 990/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1276 - val_loss: 0.3829\n",
      "Epoch 991/1000\n",
      "6694/6694 [==============================] - 0s 61us/step - loss: 0.1344 - val_loss: 0.1080\n",
      "Epoch 992/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1173 - val_loss: 0.1048\n",
      "Epoch 993/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1264 - val_loss: 0.1168\n",
      "Epoch 994/1000\n",
      "6694/6694 [==============================] - 0s 64us/step - loss: 0.1277 - val_loss: 0.1156\n",
      "Epoch 995/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1359 - val_loss: 0.1156\n",
      "Epoch 996/1000\n",
      "6694/6694 [==============================] - 0s 63us/step - loss: 0.1214 - val_loss: 0.0993\n",
      "Epoch 997/1000\n",
      "6694/6694 [==============================] - 0s 67us/step - loss: 0.1223 - val_loss: 0.2271\n",
      "Epoch 998/1000\n",
      "6694/6694 [==============================] - 0s 72us/step - loss: 0.1306 - val_loss: 0.1240\n",
      "Epoch 999/1000\n",
      "6694/6694 [==============================] - 0s 71us/step - loss: 0.1299 - val_loss: 0.0977\n",
      "Epoch 1000/1000\n",
      "6694/6694 [==============================] - 0s 68us/step - loss: 0.1294 - val_loss: 0.1245\n",
      "\n",
      "Epoch 01000: saving model to ./log_weights/Ide_AE_weights.1000.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/Ide_AE_weights.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(Ide_AE.layers[1].get_weights()))\n",
    "\n",
    "Ide_AE_history = Ide_AE.fit(x_train, x_train,\\\n",
    "                            epochs=epochs_number,\\\n",
    "                            batch_size=batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            validation_data=(x_validate,x_validate),\\\n",
    "                            callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = Ide_AE_history.history['loss']\n",
    "val_loss = Ide_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.0537403065671203\n"
     ]
    }
   ],
   "source": [
    "p_data=Ide_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "key_number=50\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_number=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features=F.top_k_keepWeights_1(Ide_AE.get_layer(index=1).get_weights()[0],key_number)\n",
    "\n",
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9666666666666667\n",
      "Testing accuracy： 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7438, 50)\n",
      "(1860, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9559139784946237\n",
      "Testing accuracy： 0.9559139784946237\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7438, 50)\n",
      "(1860, 50)\n",
      "0.01654134823354442\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
