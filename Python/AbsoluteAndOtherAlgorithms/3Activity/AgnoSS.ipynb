{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_lambda=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5744, 561)\n",
      "(5744, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data_arr=np.array(pd.read_csv('./Dataset/final_X_train.txt',header=None))\n",
    "test_data_arr=np.array(pd.read_csv('./Dataset/final_X_test.txt',header=None))\n",
    "train_label_arr=(np.array(pd.read_csv('./Dataset/final_y_train.txt',header=None))-1)\n",
    "test_label_arr=(np.array(pd.read_csv('./Dataset/final_y_test.txt',header=None))-1)\n",
    "\n",
    "data_arr=np.r_[train_data_arr,test_data_arr]\n",
    "label_arr=np.r_[train_label_arr,test_label_arr]\n",
    "label_arr_onehot=label_arr#to_categorical(label_arr)\n",
    "print(data_arr.shape)\n",
    "print(label_arr_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_arr=MinMaxScaler(feature_range=(0,1)).fit_transform(data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (4135, 561)\n",
      "Shape of x_validate: (460, 561)\n",
      "Shape of x_test: (1149, 561)\n",
      "Shape of y_train: (4135, 1)\n",
      "Shape of y_validate: (460, 1)\n",
      "Shape of y_test: (1149, 1)\n",
      "Shape of C_train_x: (4595, 561)\n",
      "Shape of C_train_y: (4595, 1)\n",
      "Shape of C_test_x: (1149, 561)\n",
      "Shape of C_test_y: (1149, 1)\n"
     ]
    }
   ],
   "source": [
    "C_train_x,C_test_x,C_train_y,C_test_y= train_test_split(data_arr,label_arr_onehot,test_size=0.2,random_state=seed)\n",
    "x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "x_test=C_test_x\n",
    "y_test_onehot=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train_onehot.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate_onehot.shape))\n",
    "print('Shape of y_test: ' + str(y_test_onehot.shape))\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, l1_lambda, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.l1_lambda=l1_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0., maxval=1.),\n",
    "                                      trainable=True,\n",
    "                                      regularizer=regularizers.l1(self.l1_lambda),\n",
    "                                      constraint=constraints.NonNeg())\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=36):\n",
    "        kernel=self.kernel        \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            print(kernel_.shape)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=50,\\\n",
    "                         p_learning_rate= 1E-3,\\\n",
    "                         p_l1_lambda=0.1):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             l1_lambda=p_l1_lambda,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='tanh',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=1000\n",
    "batch_size_value=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4.1.1 Identity Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "autoencoder_input (InputLaye (None, 561)               0         \n",
      "_________________________________________________________________\n",
      "feature_selection (Feature_S (None, 561)               561       \n",
      "_________________________________________________________________\n",
      "autoencoder_hidden_layer (De (None, 50)                28100     \n",
      "_________________________________________________________________\n",
      "autoencoder_output (Dense)   (None, 561)               28611     \n",
      "=================================================================\n",
      "Total params: 57,272\n",
      "Trainable params: 57,272\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoQAAAGVCAIAAADot3e7AAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzde1wTV9o48BMgCSFAoChXURAX3aU0IFq8URQseAEUKmIF2q7V+lELUrWtWLF+rMhqschWLShtcVG89tW3ULX2VdktCvsLqKFquQjWqtwCCAlglMv8/jjb2TGBZEhCJtHn+5c5OZl5ZibkcWbOnIdFEAQCAAAAAHNMmA4AAAAAeNFBMgYAAAAYBskYAAAAYBgkYwAAAIBhZkwHoKikpOSLL75gOgoAAADPrXXr1k2dOpXpKJ5hcGfG9+/fP3XqFNNRAAA08eDBA/j7pSotLS0tLWU6CvCMU6dO3b9/n+koFBncmTF28uRJpkMAAAzZiRMnYmJi4O+XFB0djeAHzcCwWCymQxiAwZ0ZAwAAAC8aSMYAAAAAwyAZAwAAAAyDZAwAAAAwDJIxAAAYinv37kVEREil0paWFtYffH195XI5tRv1XRaLNWnSJKYCVmHGjBksJUlJSQrdenp6MjIy/Pz8rKys7O3t586dW1BQoFw04ezZs56enmZmAww63rhx4/Hjx4drM/QFkjEAgHmdnZ1/+tOfwsLCmA6ESTdu3Jg0aVJISIi1tfWIESMIghCJRLhdIYfhd0tKSuzs7AiCKCsrYyhkbXV1dQUFBeXm5mZkZDQ3N5eVlVlaWkZERNy6dYvsU1tbGxERkZyc3NTUNOBCVqxYkZycnJKSoq+ohwUkYwAA8wiC6O/v7+/vZyoAS0vLGTNmMLV2hJBUKg0PD3/jjTfef/99ajuXy7Wzs8vOzj569ChTsWlMJBIRz9qzZw+1w4cfflhRUXHhwoXXXnuNx+ONHj06NzeXy+VS+6SkpEybNq28vNzKymrAtXh4eJw+fTo1NfXEiRPDuDHDzECfMwYAvFCsrKxqa2uZjoJJu3btamxs3LJli0K7ubn5kSNH5s2bt3LlSj8/P09PT0bCGw5NTU0HDhx47733HBwcyEY+n69wTf7rr7/m8XiqFyUUChctWrR+/fqoqKgBL2UbPjgzBgAAhhEEkZOT4+/v7+zsrPxuaGjo5s2bZTJZdHS0QqIyat9//31fX5/aCxJqMzEWGRn54MGDH374QRehMQCSMQCAYWfOnCEH+OBkQ2357bffYmJibGxs7OzswsLCyBPo9PR03GHUqFEikSg4ONjKysrCwmLWrFlXrlzBfbZv3477kL/458+fxy0jRoygLqerq+vKlSv4Lf2fWonF4qamJqFQOFiHTz/9NCQkpKKiIiEhQfWiWltb161b5+HhweFwbG1t586de/nyZfwWnb2KSSSSxMRENzc3DoczcuTIqKioGzduaLBdeXl5Pj4+fD5fIBAEBATk5+dT37127RpCyNbWdv369a6urhwOZ8yYMYmJiW1tbRqsy8fHByH0448/avBZg0AYGDwojukoAACa0Obvd8GCBQihx48fK7QsWLDg6tWrnZ2dP/30E4/Hmzx5MvVTQqGQz+dPnToV9xGJRK+88gqHwykqKiL78Pn86dOnUz/l5+eHhz6p6IPNmjXrpZdeKikp0WyjFi1atGjRIrXd8vLyEEI7duxQaBeJRAKBAP9bIpG4uroihA4fPoxbyAFcpIaGBnd3dwcHh4KCgo6OjqqqqqioKBaLdfDgQbKP2r1aX18/ZswYBweHH374QSaT3bx5MzAw0Nzc/OrVq0Pa9unTp8fHx5eXl3d2dlZWVsbHxyOEEhISFCJxdHSMjY2tra199OjRoUOH+Hy+p6dne3u78gJdXFxMTU0HW11HRwdCKCAgQG1gCKHjx48PaVv0wODSHiRjAIzXcCRj/KALtmjRIoSQRCIhW/DZ5PXr18mWiooKhJBQKCRbtEnGgYGBtra2Q81D1IDpJONdu3YhhPbt26fQTk3GBEGUlJSw2Ww+n//rr78SAyXjd955ByF09OhRskUulzs7O/N4vMbGRtyidq++/fbbCKEjR46QHRoaGrhcrp+fH82tHsyrr76KECotLcUvQ0NDEULu7u49PT1kn+3btyOEUlJSlD+uOhkTBMFiscaNG6c2DMNMxnCZGgBg0CZPnkz+G58a1tfXUzvw+Xx8iRLz9vZ2dnYWi8UNDQ3ar72oqKitrW24y+3hi/NsNlt1tylTpqSnp3d1dUVHRz9+/Fi5w+nTpxFC8+fPJ1u4XG5wcPDjx48Vrt+q2KtnzpwxMTGhPmbm6Ojo5eVVXl7+4MGDoW4aFc76BQUF+CWfz0cIzZ49m3pfIDw8HGl6tdnMzGzA3WIUIBkDAAyaQCAg/83hcBBCCk9A2djYKHzE3t4eIdTc3Dz80emGubk5Qqinp0dtz8TExJiYmJs3byo8AYUQevLkSUdHh7m5ucIjQHiscmNjI7VxsL2KF9Lf3y8QCKiTdeD7uzU1NZptIObk5IQox8XNzQ0hZGdnR+2Dj51EItFg+b29vTRHexkgSMYAAOPW2tpKPDthE/65xz/rCCETE5OnT59SO7S3tysshNmyejhL4bueauXk5IwfP/6bb77Bd5pJXC5XIBDI5XKZTEZtx3NlODo60lk4l8u1sbExMzOjXjomzZo1i+4mDQSffJPHBY+qU7iAgY8d9WEnmqRSKUEQeE8aI0jGAADjJpfL8UxV2C+//FJfXy8UCsnfZScnp4cPH5IdGhsbf//9d4WFWFhYkAl7/PjxBw4cGOaon/Hyyy8jhGheBLa0tPzuu+/4fP7+/fsV3oqMjEQIUR/vefLkycWLF3k8Hr5BS0dUVFRvby85Ih3buXPn6NGje3t7aS4kJyfHz8+P2kIQBJ6UA1+IRgjNmzfPxcXl/Pnz1Oe18EXshQsX0lwRCR9ivCeNESRjAIBxEwgEmzZtKikp6erqKisri4uL43A4mZmZZIeQkJD6+vq9e/d2dnbW1tauXbuWPDkjTZw4sbq6+v79+yUlJXV1dQEBAbg9KCjIzs6utLR0WDdBKBTa29uLxWKa/b28vLKzs5Xb09LS3N3dk5KSCgsLZTJZdXX10qVLGxoaMjMz6Z9rpqWleXh4LFu27Ny5cx0dHW1tbdnZ2du2bUtPTydv7sbFxbFYrLt376pYzrVr19asWXPnzh25XF5VVYVHVickJPj7++MOXC43JyentbV1yZIlNTU17e3teXl5aWlp/v7+iYmJNKMl4YevQkJChvpBQ6HnAWNqwWhqAIyXZn+/eNgRKTY2tqSkhNryySefEM9eiJ4/fz7+rFAodHFxuX37dmhoqJWVFY/HCwwMLC4upi6/vb19+fLlTk5OPB5vxowZIpGIPGn7+OOPcZ/KysqAgAA+n+/q6kod1RwQEKCH0dQEQWzatMnMzOzhw4f4pcJN0wFHMq9atUphNDVBEC0tLUlJSe7u7mw2WyAQhIaGXrx4Eb9Ff6/ih5XHjh3LZrNHjhwZEhLy008/UdcSFBRkaWnZ29s72ObI5fKTJ09GRkZ6eHjg6+czZ87Mz89X7nn16tXQ0FCBQMDhcCZMmLB169bu7m5qB3LAFxX1YS0sOjraxcXl6dOng4VEQgY5mppFKBXHYNaJEydiYmIMLSoAAB36//v18fFpaWnRcpTv8ImOjkYInTx5Um3Pjo4OLy+vsLCwrKys4Y9LK+3t7c7OzrGxsQcPHmQ6lv8Qi8W+vr75+flLlixR25nFYh0/fnzx4sV6CIw+uEwNAADMEwgEBQUFp06d2rdvH9OxqEIQRGJiorW19WeffcZ0LP9RV1cXFRWVnJxMJxMbLEjGz49jx47hhxDwYxJgMJaWltRnNtLT05mO6L8MOTYw3Hx9fcvKys6dOyeVSpmOZVBNTU11dXUXL16kOTxbD7Kzs1NTU1NTU5kORCuQjJ8fS5YsIQgiODhYP6sz3gK0nZ2d169fRwgtWLCAIIgNGzYwHdF/GXJshgbPKS0Wix8+fMhisTZv3sx0RDrg5uZWWFhobW3NdCCDcnR0LC4u9vLyYjqQ/9q5c6dRnxNjL3oyZryIqfEiXvgCtFoy9vgZt2HDBur4FzyNIgBGyijrPgJDAAVoAQBAV170M2MAAACAccaajHt7e48fP/766687OjryeDxvb+/MzEzykqn2RUxV1ATFVNT7pF80lFwLl8sdNWrU7Nmzc3NzqROdqw2jsrJy4cKFAoGAz+cHBAQUFxcr7yuaoVZVVS1evNjOzg6/bGlpUbH/n78CtMYVv4rvf3t7O3UIGL5429vbS7bgyfrR8HwxAAAa0veDzerQnDQAPwa+Y8eOtrY2iUTy97//3cTEROEeksZ109TWBKVT71Nt0VC8FkdHx4KCAqlU2tjYiB8VyMjIoBlGTU2NjY2Ni4vLhQsXZDJZRUVFSEiIm5sbl8sl10I/1MDAwMuXL3d1dZWWlpqamlKr1A3GeAvQUgdJKW8Rs/EPFhuV2u9/aGioiYnJnTt3qJ+aOnUqWRdvmL4YMGmPAvqTfgC9QQY56YfB/dnQT8YzZ86ktsTFxbHZ7I6ODrJF4x9EtTVB6dT7VFs0FK9F4TsxZ84cMhmrDQPPJ3Dq1Cmyw8OHD7lcLjUZ0w/17NmzxBAZbwFa1cmY2fhpJmPV339cgW716tVkh+LiYur8RMP0xYBkrACSsQGCZEyLxn/Mn3/+OUKI+hOs8Q8iLi6Ga4CQ4uPjEUKHDh3CHUxMTKiJnyCIiRMnIoTu37+PX+IfMrKgN0EQH3zwAUJILBarWMuQwsCF0mQyGbWDt7c3NRnTD7WlpWWwSAYzWDJWsdXEH2eWCotydnZGCNXX1+OX2iQzOlQnY2bjp5OMlSl//729vS0sLMjDumDBgr/97W/ku8P0xcB/vwAYOANMxsY6mrqjo2P37t2nT59+8OABtRpad3e3lktWWxMUd0DPFgQl1dTUjBo1inypumio8lqGFIZMJjM3N7e0tKR2sLe3r66upi6EZqi40LdOaFaAtr6+vrm52RAqoBl+/HS+/0lJSe++++7+/ftTUlKqq6svXbr07bff4reG+4sBKZmUkZGBEML/pQMGIiYmhukQBmCsyTg8PPznn3/OzMx88803R4wYwWKx9uzZ88EHHxCUSXE1K2KK5zTv6OiQyWTUREjWBMX1Pjs7Ox8/fqzxuKHB1jKkMKysrGQyWWdnJzUft7W1UReifajDARegpe584ypAy3j8dL7/sbGxmzZt2rt370cffbR79+63337b1tYWvzXcXwxDm/WXQXhWatghBsUwk7FRjqbu6+u7cuWKo6NjYmLiyJEj8Y8adRAypnERU7U1QXVS7xOv5ezZs9RGX19f8j/RasOYO3cuQuj8+fNkh5aWlqqqKuoCdRKqzhl7AVqm4jczM6usrKT5/edyuatXr25ubt69e/eRI0fWrl1LfdcwvxgAvLiYvUqujOY946CgIITQrl27JBJJd3f3pUuXRo8ejRCi1vl6//33EUJffvmlTCa7c+fO4sWLXVxcFO7bzZkzRyAQ/P7771evXjUzM7t9+zbx7DBmqVRKDmM+cOAA/lRTU5OHh8fYsWPPnj3b3t7e2tqalZVlYWFBvQ+hfD/1448/RpSBP3gtTk5OhYWFUqn0/v37q1atcnBwuHfvHrWDijDu3Lnz0ksvkaOpb926FRoaam9vT71nrFmoNA12z1jFVhMEIRQKBQJBcHCwitHIGh87QhejqZmNX8U9Y1NT019//ZWg9/0nCEIikfB4PBaLpby0YfpiwAAuBTCAywAhg7xnbHB/NjT/mCUSycqVK11dXdlstoODwzvvvLNx40b83wtyOKg2RUxV1ATFVNT7pF80lLoWJyenJUuWVFdXU9eiNoyqqqqFCxdaW1vjJ3AKCwvJuanffffdoYZK/2fUqAvQKtwE/fzzz4d01IY1frU3aHEypvP9x1asWIEQ+uc//6m8H4bjiwHJWAEkYwOEDDIZQz1joG8GXoBWLeOK/9tvv923b19ZWZl+Vgd/vwro1zMGesOCesYAAD3Lyspat24d01EAuu7duxcRESGVSltaWshJ0Hx9ffE8dyTquywWa9KkSUwFrMKMGTNYSpKSkhS69fT0ZGRk+Pn5WVlZ2dvbz507Fz/or9Dt7Nmznp6eA4433Lhx43MwgB+SMQDPm5ycnMjIyM7OzqysrEePHhnaGQAYzI0bNyZNmhQSEmJtbT1ixAiCIPA4wRs3bijkMPxuSUkJHoigtysfOtfV1RUUFJSbm5uRkdHc3FxWVmZpaRkREXHr1i2yT21tbURERHJyMn6WRNmKFSuSk5NTUlL0FfWwgGQMBqb8X1rS1q1bNVumsRegNaL4z5w5Y2tr+9VXXx07dsygnmrTreEuQ6nPMpdSqTQ8PPyNN97Ao/9IXC7Xzs4uOzv76NGj+olEh0QikcKd0T179lA7fPjhhxUVFRcuXHjttdd4PN7o0aNzc3O5XC61T0pKyrRp08rLyweblcHDw+P06dOpqaknTpwYxo0Zbvq9Ra0eDAABwHjp+e9Xm1nY9LN8+gO4PvnkEzMzs4cPH1IbRSKRQCA4f/68iYmJlZVVVVUV9V3yzNgwTZ8+XTkZUzU2Npqamq5atUr1crq7u/E/XFxcTE1NB+sWHR09atSonp4etYEhgxzABWfGAADAMIIgcnJy/P398byqCkJDQzdv3iyTyaKjoxVuHhu177//vq+vT+21Bx6PR2dpkZGRDx48oE7MYFwgGQMAGKCiPKg2ZSgNp8zlkIjF4qamJlyDZECffvppSEhIRUVFQkKC6kWp2LH0q7uqKK85JHl5eT4+Pnw+XyAQBAQE5OfnU9+9du0aQsjW1nb9+vWurq4cDmfMmDGJiYnUaQTp8/HxQQjhEilGielTc0VwmRoA40Xz71dteVBCu2IbhlCmE6N5mTovLw8htGPHDoV2fJka/1sikbi6uiKEDh8+jFuUL1PT2bFq64TSKa9Jx/Tp0+Pj48vLyzs7OysrK3GRm4SEBIVIHB0dY2Nja2trHz16dOjQIT6f7+np2d7errxA1Zep8XTrAQEBagNDBnmZ2uDSHiRjAIwXzb9fteVBCa2TMWK6TCdGMxnv2rULIUSduwajJmOCIEpKSthsNp/Px3O/KCdjOjtWbZ1QOuU1NfPqq68ihEpLS/FLPK2vu7s79Ubv9u3bEUIpKSnKH1edjAmCYLFY48aNUxuGYSZjuEwNANA3PIPb/PnzyRYulxscHPz48WNdXWbk8/n4uiXm7e3t7OwsFosbGhq0X3hRUVFbW9vUqVO1XxSG7wSz2WzV3aZMmZKent7V1RUdHa08Gzkayo6dPHky+W98wl1fX49fnjlzxsTEJCwsjOzg6Ojo5eVVXl6u5Vw3OOsXFBTgl3i+udmzZ1NvAYSHhyNNrzabmZkNuFuMAiRjAIBeqS0PqpO1DFjmEv1RX8vQmJubI4R6enrU9kxMTIyJibl586bCE1BoiDtWdXXX/v5+gUBAfaAR39+tqanRbAMxXEmFPARubm4IITs7O2offJgkEokGy+/t7aU52ssAQTIGAOgVLg8ql8tlMhm1nSwPil9qWYYSl7mkthhymU6cpfBdT7VycnLGjx//zTff4DvNJJo7VjVcXtPMzGzAZ4RmzZpFd5MGgk++yUOAB9ApXKvAhwn/B2JIpFIpQRCGUBBdM5CMAQD6prY8KNK6DKVxlel8+eWXEUI0LwJbWlp+9913fD5///79Cm/R2bFq6aS8Zk5ODlkcBSMIAk/KgS9EI4TmzZvn4uJy/vx56vNa+CL2woULaa6IhI8m3pPGCJIxAEDf0tLS3N3dk5KSCgsLZTJZdXX10qVLGxoaMjMzyVOikJCQ+vr6vXv3dnZ21tbWrl27ljyjIk2cOLG6uvr+/fslJSV1dXUBAQHkWwKBYNOmTSUlJV1dXWVlZXFxcRwOJzMzk+ygzfKDgoLs7OxKS0t1tUOEQqG9vb1YLKbZ38vLKzs7W7mdzo5VKy0tzcPDY9myZefOnevo6Ghra8vOzt62bVt6ejp5czcuLo7FYt29e1fFcq5du7ZmzZo7d+7I5fKqqio8sjohIcHf3x934HK5OTk5ra2tS5YsqampaW9vz8vLS0tL8/f3T0xMpBktCT98FRISMtQPGgo9DxhTC0ZTA2C86P/9qi0Pqk0ZTcbLdJLoz8C1adMm6gxcCjdNBxzJvGrVKuUZuFTsWPp1QlWU18SCgoIsLS17e3sH2xy5XH7y5MnIyEgPDw98/XzmzJn5+fnKPa9evRoaGioQCDgczoQJE7Zu3UpOuYWRA76oqA9rYdHR0S4uLk+fPh0sJBIyyNHUUEIRAKAzBvL3azhlLumXUOzo6PDy8goLC8vKyhr+uLTS3t7u7OwcGxt78OBBpmP5D7FY7Ovrm5+fv2TJErWdoYQiAACAgQkEgoKCglOnTu3bt4/pWFQhCCIxMdHa2vqzzz5jOpb/qKuri4qKSk5OppOJDRYkYwAAMAi+vr5lZWXnzp2TSqVMxzKopqamurq6ixcv0hyerQfZ2dmpqampqalMB6IVSMYAgOeHEZW5HJCbm1thYaG1tTXTgQzK0dGxuLjYy8uL6UD+a+fOnUZ9Tow9t4VOAQAvoA0bNmzYsIHpKAAYMjgzBgAAABgGyRgAAABgGCRjAAAAgGGQjAEAAACGGegALjyFKQDAuOA5nuDvl4QnHoEdAtQy0GQcExPDdAgAAA3B368C2CFALYObDhMAMCR4Vj849wLAqME9YwAAAIBhkIwBAAAAhkEyBgAAABgGyRgAAABgGCRjAAAAgGGQjAEAAACGQTIGAAAAGAbJGAAAAGAYJGMAAACAYZCMAQAAAIZBMgYAAAAYBskYAAAAYBgkYwAAAIBhkIwBAAAAhkEyBgAAABgGyRgAAABgGCRjAAAAgGGQjAEAAACGQTIGAAAAGAbJGAAAAGAYJGMAAACAYZCMAQAAAIZBMgYAAAAYBskYAAAAYBgkYwAAAIBhkIwBAAAAhkEyBgAAABgGyRgAAABgGCRjAAAAgGGQjAEAAACGQTIGAAAAGAbJGAAAAGAYJGMAAACAYSyCIJiOAQAwBEeOHPn666/7+/vxy7t37yKE3N3d8UsTE5N33303NjaWsfgAAEMHyRgAI1NRUSEUClV0EIvFr7zyit7iAQBoD5IxAMZnwoQJVVVVA741bty4mpoaPccDANAS3DMGwPjEx8ez2Wzldjab/de//lX/8QAAtARnxgAYn7q6unHjxg34x1tTUzNu3Dj9hwQA0AacGQNgfMaOHTtx4kQWi0VtZLFYkyZNgkwMgDGCZAyAUXrrrbdMTU2pLaampm+99RZT8QAAtAGXqQEwSs3NzU5OTuQDTgghExOT+vp6BwcHBqMCAGgGzowBMEr29vaBgYHkybGpqenMmTMhEwNgpCAZA2Cs4uPjqVe24uPjGQwGAKANuEwNgLGSSqUjR458+vQpQojNZjc3N9vY2DAdFABAE3BmDICxsra2njNnjpmZmZmZ2bx58yATA2C8IBkDYMTi4uL6+vr6+vpgMmoAjBpcpgbAiMnl8hEjRhAE0dLSwuPxmA4HAKApguL48eNMhwMAAAA8/44fP07Nv2YD9tB/WAAAzdy4cYPFYqmu4/RCKSkp2bNnD/yOkTIyMhBCH3zwAdOBgP+KiYlRaBkgGS9evFgvwQAAdCAqKgohZGY2wN/yC2vPnj3wO0Y6efIkgh92A0MrGQMAjAikYQCeAzCaGgAAAGAYJGMAAACAYZCMAQAAAIZBMgYAAPBf9+7di4iIkEqlLS0trD/4+vrK5XJqN+q7uJY2UwGrMGPGDJaSpKQkhW49PT0ZGRl+fn5WVlb29vZz584tKChQnoTj7Nmznp6eA47S2Lhxo5YD+CEZAwAAQgh1dnb+6U9/CgsLYzoQJt24cWPSpEkhISHW1tZ4PhmRSITbFXIYfrekpMTOzo4giLKyMoZC1lZXV1dQUFBubm5GRkZzc3NZWZmlpWVERMStW7fIPrW1tREREcnJyU1NTQMuZMWKFcnJySkpKRqHAckYAAAQQoggiP7+fmqJaD2ztLScMWMGU2tHCEml0vDw8DfeeOP999+ntnO5XDs7u+zs7KNHjzIVm8ZEIhHxrD179lA7fPjhhxUVFRcuXHjttdd4PN7o0aNzc3O5XC61T0pKyrRp08rLy62srAZci4eHx+nTp1NTU0+cOKFZnPBQBAAAIISQlZVVbW0t01EwadeuXY2NjVu2bFFoNzc3P3LkyLx581auXOnn5+fp6clIeMOhqanpwIED7733HrUWOJ/PV7gm//XXX6udblYoFC5atGj9+vVRUVEaPHAIZ8YAAAAQQRA5OTn+/v7Ozs7K74aGhm7evFkmk0VHRyskKqP2/fff9/X1qb0gQXPi98jIyAcPHvzwww8aRALJGAAA0JkzZ8gBPjjZUFt+++23mJgYGxsbOzu7sLAw8gQ6PT0ddxg1apRIJAoODraysrKwsJg1a9aVK1dwn+3bt+M+5C/++fPnccuIESOoy+nq6rpy5Qp+S/9zuYjF4qamJhXzqn766achISEVFRUJCQmqF9Xa2rpu3ToPDw8Oh2Nrazt37tzLly/jt+jsVUwikSQmJrq5uXE4nJEjR0ZFRd24cUOD7crLy/Px8eHz+QKBICAgID8/n/rutWvXEEK2trbr1693dXXlcDhjxoxJTExsa2vTYF0+Pj4IoR9//FGDzw5QKIIAAACjpc3v2IIFCxBCjx8/VmhZsGDB1atXOzs7f/rpJx6PN3nyZOqnhEIhn8+fOnUq7iMSiV555RUOh1NUVET24fP506dPp37Kz88PD31S0QebNWvWSy+9VFJSotlGLVq0aNGiRWq75eXlIYR27Nih0C4SiQQCAf63RCJxdXVFCB0+fBi3kAO4SA0NDe7u7g4ODgUFBR0dHVVVVVFRUSwW6+DBg2QftXu1vr5+zJgxDg4OP/zwg0wmu3nzZmBgoLm5+dWrV4e07dOnT4+Pjy8vL+/s7KysrIyPj0cIJSQkKETi6OgYGxtbW1v76NGjQ4cO8fl8T1DUnyUAACAASURBVE/P9vZ25QW6uLiYmpoOtrqOjg6EUEBAgNrAkFKhCEjGAIDnynAkY/ygC7Zo0SKEkEQiIVvw2eT169fJloqKCoSQUCgkW7RJxoGBgba2tkPNQ9SA6STjXbt2IYT27dun0E5NxgRBlJSUsNlsPp//66+/EgMl43feeQchdPToUbJFLpc7OzvzeLzGxkbconavvv322wihI0eOkB0aGhq4XK6fnx/NrR7Mq6++ihAqLS3FL0NDQxFC7u7uPT09ZJ/t27cjhFJSUpQ/rjoZEwTBYrHGjRunNgzlZAyXqQEAQI3JkyeT/8anhvX19dQOfD4fX6LEvL29nZ2dxWJxQ0OD9msvKipqa2ubOnWq9otSAV+cZ7PZqrtNmTIlPT29q6srOjr68ePHyh1Onz6NEJo/fz7ZwuVyg4ODHz9+rHD9VsVePXPmjImJCfUxM0dHRy8vr/Ly8gcPHgx106hw1i8oKMAv+Xw+Qmj27NnU+wLh4eFI06vNZmZmA+4WtSAZAwCAGgKBgPw3h8NBCCk8AWVjY6PwEXt7e4RQc3Pz8EenG+bm5gihnp4etT0TExNjYmJu3ryp8AQUQujJkycdHR3m5uYKjwDhscqNjY3UxsH2Kl5If3+/QCCgTtaB7+/W1NRotoGYk5MTohwXNzc3hJCdnR21Dz52EolEg+X39vbSHO2lAJIxAABoq7W1lXh2wib8c49/1hFCJiYmT58+pXZob29XWAiLxRrOGNXAWQrf9VQrJydn/Pjx33zzDb7TTOJyuQKBQC6Xy2QyajueK8PR0ZHOwrlcro2NjZmZGfXSMWnWrFl0N2kg+OSbPC54VJ3CBQx87KgPO9EklUoJgsB7cqggGQMAgLbkcjmeqQr75Zdf6uvrhUIh+bvs5OT08OFDskNjY+Pvv/+usBALCwsyYY8fP/7AgQPDHPUzXn75ZYQQzYvAlpaW3333HZ/P379/v8JbkZGRCCHq4z1Pnjy5ePEij8fDN2jpiIqK6u3tJUekYzt37hw9enRvby/NheTk5Pj5+VFbCILAk3LgC9EIoXnz5rm4uJw/f576vBa+iL1w4UKaKyLhQ4z35FBBMgYAAG0JBIJNmzaVlJR0dXWVlZXFxcVxOJzMzEyyQ0hISH19/d69ezs7O2tra9euXUuenJEmTpxYXV19//79kpKSurq6gIAA3B4UFGRnZ1daWjqsmyAUCu3t7cViMc3+Xl5e2dnZyu1paWnu7u5JSUmFhYUymay6unrp0qUNDQ2ZmZn0zzXT0tI8PDyWLVt27ty5jo6Otra27Ozsbdu2paenkzd34+LiWCzW3bt3VSzn2rVra9asuXPnjlwur6qqwiOrExIS/P39cQcul5uTk9Pa2rpkyZKampr29va8vLy0tDR/f//ExESa0ZLww1chISFD/SBC8GgTAOD5otnvGB52RIqNjS0pKaG2fPLJJ8SzF6Lnz5+PPysUCl1cXG7fvh0aGmplZcXj8QIDA4uLi6nLb29vX758uZOTE4/HmzFjhkgkIk/aPv74Y9ynsrIyICCAz+e7urpSRzUHBAToYTQ1QRCbNm0yMzN7+PAhfqlw03TAkcyrVq1SGE1NEERLS0tSUpK7uzubzRYIBKGhoRcvXsRv0d+r+GHlsWPHstnskSNHhoSE/PTTT9S1BAUFWVpa9vb2DrY5crn85MmTkZGRHh4e+Pr5zJkz8/PzlXtevXo1NDRUIBBwOJwJEyZs3bq1u7ub2oEc8EVFfVgLi46OdnFxefr06WAhkZDSaGoWdUecOHEiJiaGUCpVAQAAxkL/v2M+Pj4tLS1ajvIdPtHR0QihkydPqu3Z0dHh5eUVFhaWlZU1/HFppb293dnZOTY29uDBg0zH8h9isdjX1zc/P3/JkiVqO7NYrOPHjy9evJhs0fwy9fHjx318fHg8Hh7ndvPmTY0X9byizs7DdCzDy9LSUrlOGVVOTg7TMQ6jvr6+rKysadOmCQQCNpvt7Ow8b968vXv3/vbbbzSXYFBfFYWjmZ6eznREQE8EAkFBQcGpU6f27dvHdCyqEASRmJhobW392WefMR3Lf9TV1UVFRSUnJ9PJxAPSMBlfuXLlzTffDAkJkUgkd+7cMYRfEAO0YcMG4o8JAfSAwQJwnZ2d169fRwgtWLBA+YJMYGCg/kPSp/j4+DVr1ixcuPDWrVsymeznn3/29fVNTEykX+FVz18V1RSO5oYNG5iOCOiPr69vWVnZuXPnpFIp07EMqqmpqa6u7uLFizSHZ+tBdnZ2ampqamqqxkvQMBmfPHmSIIi1a9daWlp6eHjcv39fs/FjChivIGYsBtxRBNMF4PTAAL8hIpHo6NGj77777kcffTRq1Chzc3MPD4/U1NRVq1YxHRpCBrnHnif4koZYLH748CGLxdq8eTPTEemAm5tbYWGhtbU104EMytHRsbi42MvLi+lA/mvnzp0anxNjGs5Ffv/+faT0oDRglsEWgCsqKmI6hGGEK5CPHz9eoX3x4sV4JBF4jm3YsAGuHACd0PDMuK+vT7dxgOfS+++/n5SUxHQUwws/rfHTTz8ptAcGBra0tDAREQDA+Aw5GeMCWP/7v/+LEMKjt6ZMmYLfUl3xqre39/jx46+//rqjoyOPx/P29s7MzCSvqQ5WQYxO9TFqTa6qqqrFixfb2dnhl/jXUJtSXE+ePNmyZcuECRMsLCxeeuml8PBwXP+S7KDBwtV+hCxAxuVyR40aNXv27NzcXDzf6WA7SrkAnMKitKllpivP5TckICDA0dHxxx9/nDt3blFRkYrbBAbyVdEVFcervb2dOgQMT7vf29tLtuD5gVVvoNqjBsBzhTrQhv7zecq1TdRWvMLPae3YsaOtrU0ikfz97383MTHB41ZIgxUtoVPwBIcUGBh4+fLlrq6u0tJSU1NTiUSiZSmu5cuXCwSCCxcudHd3NzY24ktSly9fprnVxB/PINLfUbgAmaOjY0FBgVQqbWxsxCMGMzIy1O4oheOik1pmBL0KbnjIj7K1a9fS3HAj/YYQBPHzzz/jae4RQvb29rGxsfn5+V1dXdQ+hvZVUU3FcDyS2uMVGhpqYmJy584d6qemTp1KluKhs08GO2oqAoP5EhTQf84Y6A3SVQlF5WSstuJVQUHBzJkzqQuJi4tjs9kdHR1ki/Y/tWfPnlX4rJaluNzd3adNm0Zt8fT0JJMxnYUr/MKq/QguQKZwnObMmaNBMtZJLTOCXgW3AX++16xZQybj5/Ubgsnl8kOHDi1YsICcH9/Ozo665w3tq6IazWSs+njhojerV68mOxQXF1OnRKCzTwY7aipAMlYAydgAKf/l6uyyleqKV6NGjQoLC1N46kYoFB4+fPjWrVs6LA2Ga1UOKTDVC5wzZ85XX3313nvvLVu2bPLkyaamplVVVdosXO1H8ExAc+fOpX7q3Llz6jdeyWC1zPLy8n788ce33nqLbB+wlhl5mVcnI7Ce128IxuVy33rrrbfeequ3t/df//rXwYMHjx07FhcXN378eF9fX83Wos+vigbUHq+QkBBvb+/c3Nxt27bhwZ6ff/55QkICWaSP/j5RPmpq4SmIAfpjumnYIQZON8kYV7xCz5bEItXU1IwaNaqjo2P37t2nT59+8OABtVxJd3e3TmLAcHHKIQWmeoH79u2bOnXqoUOHgoODEUIBAQErV67EM6FrsHC1Hxk5cuSABcg0oJNaZlrau3cvNRj0PH5DFJiZmQUFBQUFBY0ZM2bnzp2nTp3y9fU18K+KZugcr6SkpHfffXf//v0pKSnV1dWXLl369ttv8VtD2icKR42OmJiYoX7k+QY7xMDpplAEnYpX4eHhn3322YoVK6qrq/v7+wmCyMjIQAgRlFnrWINUEKNTfUzjwFRjsVjx8fH/93//197efubMGYIgoqKivvjiC80WrvYjgxUgU46KzrZrX8tMV57jb8iVK1cGnP4ef/bRo0earUWfXxXN0DlesbGxDg4Oe/fuffLkye7du99++21bW1uaG6hleMrLfGHBZWoDpPyN1VnVJtUVr/r6+q5cueLo6JiYmDhy5Ej8A4HHfFINVkGMTvUxzQJT+3EbG5vKykqEEJvNfv311/EIT7I6mAYLV/sRfNp99uxZagdfX98PPviAfEmz1JpOapnpyvP6DSEIorm5WbmiTllZGUIIX6PWbC36/KrQZ2ZmVllZSfN4cbnc1atXNzc37969+8iRI2vXrh3SBgLwAqHmam0GcDU1NXl4eIwdO/bs2bPt7e2tra1ZWVkWFhbkPeqgoCCE0K5duyQSSXd396VLl0aPHo0QohbimDNnjkAg+P33369evWpmZnb79m3c/v777yOEvvzyS5lMdufOncWLF7u4uAw4PIcaEs3AVBMIBIGBgWKxWC6XNzU1bd26FSG0fft2+gtXGJWj9iN4iKyTk1NhYaFUKr1///6qVascHBzu3bundkepGE0tlUrJ0dQHDhxQsd8+/vhjhND169fJFvqjqVUM+XlevyE///wzQsjV1fXIkSMPHz6Uy+V37979/PPPORyOn5+fXC6nvxZ9flVUU3E0TU1Nf/31V4Le8SIIQiKR4GcglZdGZ58MdtRUgAFcCuDM2AAh7UdTKxQaQwiRv9GqK15JJJKVK1e6urqy2WwHB4d33nln48aNeAnk4MnBKoiprj6mUJNLeRPUluJS4caNGytXrvzzn/+MnzOeMmXKwYMH8UU5tQv//PPPqVHhYmF04qEWIHNyclqyZEl1dTW1g/KOUi4Ap7wojWuZqa3gpnBLz8HBYcBuz+U3pK+vr7i4eMOGDf7+/s7OzmZmZlZWVpMmTdqxY4fC000G8lVRS+0NWpyM6RwvbMWKFQihf/7zn8rrUrGBao/aYCAZK4BkbIAQlFAEAOjZt99+u2/fPnzdXg/gd0wB/RKKQG9YOiyhCAAAdGRlZa1bt47pKAAwaJCMAQC6l5OTExkZ2dnZmZWV9ejRI+oZADBw9+7di4iIkEqlLS0t5Iykvr6+1Hl2EULUd1ksFv2Cofo0Y8YMlhLlCfN7enoyMjL8/PysrKzs7e3nzp2L50FS6Hb27FlPT88Bp5XduHGjloVhXvRkrHycSHisFnjBPX/fEL1t0ZkzZ2xtbb/66qtjx47pdlpsMHxu3LgxadKkkJAQa2vrESNGEAQhEolwu0IOw++WlJTgkZJ6uw2hc11dXUFBQbm5uRkZGc3NzWVlZZaWlhEREbggG1ZbWxsREZGcnIwfDVW2YsWK5OTklJQUjcN40f9C4MYSUO35+4boZ4uWL1++fPlyPayIcZaWlj4+PsXFxUa6fCqpVBoeHv7GG2/gxxNIXC7X0tIyOzs7MDDwzTff1EMkOiQSiVSftX/44YcVFRXV1dV4woDRo0fn5ubiqddJKSkp06ZN+5//+R83N7euri7lhXh4eJw+fdrX19fb21uz60Av+pkxAAAAbNeuXY2NjVu2bFFoNzc3P3LkiImJycqVK6urqxmJbZg0NTUdOHAAz05DNvL5fLlc/vLLL5MtX3/99caNG1Vf4BEKhYsWLVq/fr1mT8lDMgYAAIAIgsjJycFP6Cm/GxoaunnzZplMFh0drXDz2KjhkrhkAdbB8Hg8OkuLjIx88OABdZ4l+iAZAwBeUCqqfdOpkz1YxWjczmKxRo0aJRKJgoODraysLCwsZs2aRU43ps3yh4lYLG5qahIKhYN1+PTTT0NCQioqKhISElQvSidl1LWpMk6Vl5fn4+PD5/MFAkFAQEB+fj713WvXriGEbG1t169f7+rqyuFwxowZk5iY2NbWpsG6fHx8EEK4XtmQUR86hoflAQDGjubvGJ1q33RKcw5WpFIoFPL5/KlTp+JK4SKR6JVXXuFwOEVFRTpZPp158TCak37k5eUhhHbs2KHQLhKJBAIB/rdEIsFV3Q4fPoxbyAFcJJ2UUde+yjg2ffr0+Pj48vLyzs7OysrK+Ph4hFBCQoJCJI6OjrGxsbW1tY8ePTp06BCfz/f09Gxvb1deoIuLi6mp6WCrw7VPAgIC1AaGdFXPGAAADBPN3zE61b61TMbo2WllKyoqEEJCoVDFZ+kvn06VcYxmMt61axdCSHmaNmoyJgiipKSEzWbz+Xw8EZtyMtZJGXWdVBkfEC7HWVpail/iWfrd3d2pBUu2b9+OEEpJSVH+uOpkTBAEi8UaN26c2jCUkzFcpgYAvIgGq/b9+PFjDS8zKuHz+fi6Jebt7e3s7CwWixsaGrRfeFFRUVtbmw5LfeM7wWS16cFMmTIlPT29q6srOjpauTQIGsqOHbCMOn6putb1UDeNCmd9crA0nvx19uzZ1FsA4eHhSNOrzWZmZgPuFrUgGQMAXjhDqvatMRsbG4UWe3t7hFBzc7NOlq9b5ubmCKGenh61PRMTE2NiYm7evKnwBBTSURl1vJD+/n6BQEB9Dh7f362pqdFsAzEnJydEOQRubm4IITs7O2offJgkEokGy+/t7aU52ksBJGMAwAuHZrVvOnWyWYNXjG5tbSWefaob5wD8W6/98nULZyl811OtnJyc8ePHf/PNN/hOM0knZdSHtdY1PvkmDwEeQKdwrQIfpgHrlKsmlUoJgsB7cqggGQMAXkR0qn3TqZOtomK0XC7H01dhv/zyS319vVAoJH+stVy+buHHamleBLa0tPzuu+/4fP7+/fsV3tJJGXWd1LrOyckhq7dhBEGcOHEC/XEhGiE0b948FxeX8+fPU5/XwhexFy5cSHNFJHw0qQ8o0wfJGADwIkpLS3N3d09KSiosLJTJZNXV1UuXLm1oaMjMzCRPiUJCQurr6/fu3dvZ2VlbW7t27VryjIo0ceLE6urq+/fvl5SU1NXVBQQEkG8JBIJNmzaVlJR0dXWVlZXFxcVxOJzMzEyygzbLDwoKsrOzKy0t1dUOEQqF9vb2YrGYZn8vL6/s7Gzldjo7Vq20tDQPD49ly5adO3euo6Ojra0tOzt727Zt6enp5M3duLg4Fot19+5dFcu5du3amjVr7ty5I5fLq6qq8MjqhIQEf39/3IHL5ebk5LS2ti5ZsqSmpqa9vT0vLy8tLc3f3z8xMZFmtCT88FVISMhQP4gQPNoEAHi+0P8dU1HtG1NdJxv3GaxitFAodHFxuX37dmhoqJWVFY/HCwwMLC4u1tXy1VYZJ9GvZ7xp0yYzM7OHDx/ilwo3TQccybxq1SqF0dSEjsqoqy3mHRQUZGlp2dvbO9jmyOXykydPRkZGenh44OvnM2fOzM/PV+559erV0NBQgUDA4XAmTJiwdevW7u5uageF2TEx6sNaWHR0tIuLy9OnTwcLiYSgnjEA4PlmIL9jPj4+LS0tWg791Qn69Yw7Ojq8vLzCwsKysrKGPy6ttLe3Ozs7x8bGHjx4kOlY/kMsFvv6+ubn5y9ZskRtZxbUMwYAADAggUBQUFBw6tSpffv2MR2LKgRBJCYmWltbf/bZZ0zH8h91dXVRUVHJycl0MvGAIBkDAAD4D19f37KysnPnzkmlUqZjGVRTU1NdXd3FixdpDs/Wg+zs7NTU1NTUVI2XAMkYAAB0Cc8pLRaLHz58yGKxNm/ezHREQ+Pm5lZYWGhtbc10IINydHQsLi728vJiOpD/2rlzp8bnxNiLXs8YAAB0a8OGDRs2bGA6CmBk4MwYAAAAYBgkYwAAAIBhkIwBAAAAhkEyBgAAABg2wAAu/IQ4AAAYIzzPBvyOkfCUmbBDDNwzM3CVlJR88cUXDEYDABiq69evI4R8fX2ZDgQAMATr1q2jlqNmMT5pHABAG3hGPVyLBgBgpOCeMQAAAMAwSMYAAAAAwyAZAwAAAAyDZAwAAAAwDJIxAAAAwDBIxgAAAADDIBkDAAAADINkDAAAADAMkjEAAADAMEjGAAAAAMMgGQMAAAAMg2QMAAAAMAySMQAAAMAwSMYAAAAAwyAZAwAAAAyDZAwAAAAwDJIxAAAAwDBIxgAAAADDIBkDAAAADINkDAAAADAMkjEAAADAMEjGAAAAAMMgGQMAAAAMg2QMAAAAMAySMQAAAMAwSMYAAAAAwyAZAwAAAAyDZAwAAAAwDJIxAAAAwDBIxgAAAADDIBkDAAAADINkDAAAADDMjOkAAABD093d/eTJE/Ll06dPEUKPHj0iW7hcroWFBQORAQA0xSIIgukYAABDsH///jVr1qjosG/fvtWrV+stHgCA9iAZA2BkJBKJk5NTX1/fgO+ampo2NDSMHDlSz1EBALQB94wBMDIjR44MDg42NTVVfsvU1HT27NmQiQEwOpCMATA+cXFxA17TIggiLi5O//EAALQEl6kBMD4ymWzkyJHUYVwYh8ORSCTW1taMRAUA0BicGQNgfKysrMLDw9lsNrXRzMxswYIFkIkBMEaQjAEwSrGxsb29vdSWvr6+2NhYpuIBAGgDLlMDYJSePn06YsQImUxGtlhaWra0tHC5XAajAgBoBs6MATBKHA4nOjqaw+Hgl2w2OyYmBjIxAEYKkjEAxmrp0qV4+i2EUE9Pz9KlS5mNBwCgMbhMDYCx6u/vd3R0lEgkCKERI0Y0NjYO+PAxAMDwwZkxAMbKxMRk6dKlHA6HzWbHxsZCJgbAeEEyBsCIvfnmm0+fPoVr1AAYO71WbTpx4oQ+VwfAc48gCDs7O4TQ3bt3f/vtN6bDAeC5snjxYr2tS6/3jFkslt7WBQAAAGhDn/lR3/WMjx8/rs//awDw3Lt9+zZC6C9/+QvTgQxBdHQ0QujkyZNMB2IoWCwW/DYalBMnTsTExOhzjfpOxgAA3TKuNAwAGBAM4AIAAAAYBskYAAAAYBgkYwAAAIBhkIwBAAAAhkEyBgAAY3Lv3r2IiAipVNrS0sL6g6+vr1wup3ajvstisSZNmsRUwCrMmDGDpSQpKUmhW09PT0ZGhp+fn5WVlb29/dy5cwsKCpSfOzp79qynp6eZ2QADkzdu3Hj8+PHh2gxdgGQMADAanZ2df/rTn8LCwpgOhDE3btyYNGlSSEiItbX1iBEjCIIQiUS4XSGH4XdLSkrs7OwIgigrK2MoZG11dXUFBQXl5uZmZGQ0NzeXlZVZWlpGRETcunWL7FNbWxsREZGcnNzU1DTgQlasWJGcnJySkqKvqIcMkjEAwGgQBNHf39/f389UAJaWljNmzGBq7VKpNDw8/I033nj//fep7Vwu187OLjs7++jRo0zFpjGRSEQ8a8+ePdQOH374YUVFxYULF1577TUejzd69Ojc3FyFaqEpKSnTpk0rLy+3srIacC0eHh6nT59OTU012Ikg4TljAIDRsLKyqq2tZToKxuzatauxsXHLli0K7ebm5keOHJk3b97KlSv9/Pw8PT0ZCW84NDU1HThw4L333nNwcCAb+Xy+wjX5r7/+msfjqV6UUChctGjR+vXro6KiBryUzSw4MwYAACNAEEROTo6/v7+zs7Pyu6GhoZs3b5bJZNHR0QqJyqh9//33fX19aq9GqM3EWGRk5IMHD3744QddhKZjkIwBAMbhzJkz5BgfnG+oLb/99ltMTIyNjY2dnV1YWBh5Ap2eno47jBo1SiQSBQcHW1lZWVhYzJo168qVK7jP9u3bcR/yR//8+fO4ZcSIEdTldHV1XblyBb+l57MrsVjc1NQkFAoH6/Dpp5+GhIRUVFQkJCSoXlRra+u6des8PDw4HI6tre3cuXMvX76M36KzSzGJRJKYmOjm5sbhcEaOHBkVFXXjxg0NtisvL8/Hx4fP5wsEgoCAgPz8fOq7165dQwjZ2tquX7/e1dWVw+GMGTMmMTGxra1Ng3X5+PgghH788UcNPjvsCD1CCB0/flyfawQAGKBFixYtWrRIs88uWLAAIfT48WOFlgULFly9erWzs/Onn37i8XiTJ0+mfkooFPL5/KlTp+I+IpHolVde4XA4RUVFZB8+nz99+nTqp/z8/PDoJxV9sFmzZr300kslJSWabRSd38a8vDyE0I4dOxTaRSKRQCDA/5ZIJK6urgihw4cP4xZyABepoaHB3d3dwcGhoKCgo6OjqqoqKiqKxWIdPHiQ7KN2l9bX148ZM8bBweGHH36QyWQ3b94MDAw0Nze/evXqkDZ8+vTp8fHx5eXlnZ2dlZWV8fHxCKGEhASFSBwdHWNjY2trax89enTo0CE+n+/p6dne3q68QBcXF1NT08FW19HRgRAKCAhQGxgeej2kbdESJGMAgL4NRzLGz7qQy0cISSQSsgWfUF6/fp1sqaioQAgJhUKyRZtkHBgYaGtrO9RURKLz27hr1y6E0L59+xTaqcmYIIiSkhI2m83n83/99VdioGT8zjvvIISOHj1KtsjlcmdnZx6P19jYiFvU7tK3334bIXTkyBGyQ0NDA5fL9fPzo7/VA3r11VcRQqWlpfhlaGgoQsjd3b2np4fss337doRQSkqK8sdVJ2OCIFgs1rhx49SGof9kDJepAQDPg8mTJ5P/xmeH9fX11A58Ph9fpcS8vb2dnZ3FYnFDQ4P2ay8qKmpra5s6dar2ixoMvjLPZrNVd5syZUp6enpXV1d0dPTjx4+VO5w+fRohNH/+fLKFy+UGBwc/fvxY4fqtil165swZExMT6jNmjo6OXl5e5eXlDx48GOqmUeGsX1BQgF/y+XyE0OzZs6k3BcLDw5GmV5vNzMwG3C2Mg2QMAHgeCAQC8t8cDgchpPAElI2NjcJH7O3tEULNzc3DH50OmJubI4R6enrU9kxMTIyJibl586bCE1AIoSdPnnR0dJibmys8AoTHKjc2NlIbB9uleCH9/f0CgYA6WQe+v1tTU6PZBmJOTk6IclDc3NwQQnZ2dtQ++MBJJBINlt/b20tztJeeQTIGALwQWltbiWfnbMK/+PiXHSFkYmLy9OlTaof29naFhbBYrOGMURWcpfBdT7VycnLGjx//zTff4DvNJC6XKxAI5HK5TCajtuO5MhwdHeksnMvl2tjYmJmZUS8dk2bNGe0zDAAAIABJREFUmkV3kwaCT77Jg4KH1ClcvcAHjvqwE01SqZQgCLwnDQ0kYwDAC0Eul+PJqrBffvmlvr5eKBSSP81OTk4PHz4kOzQ2Nv7+++8KC7GwsCAT9vjx4w8cODDMUf/Xyy+/jBCieRHY0tLyu+++4/P5+/fvV3grMjISIUR9vOfJkycXL17k8Xj4Bi0dUVFRvb295HB0bOfOnaNHj+7t7aW5kJycHD8/P2oLQRB4Ug58IRohNG/ePBcXl/Pnz1Of18IXsRcuXEhzRSR8fPGeNDSQjAEALwSBQLBp06aSkpKurq6ysrK4uDgOh5OZmUl2CAkJqa+v37t3b2dnZ21t7dq1a8nzM9LEiROrq6vv379fUlJSV1cXEBCA24OCguzs7EpLS4cvfqFQaG9vLxaLafb38vLKzs5Wbk9LS3N3d09KSiosLJTJZNXV1UuXLm1oaMjMzKR/rpmWlubh4bFs2bJz5851dHS0tbVlZ2dv27YtPT2dvLkbFxfHYrHu3r2rYjnXrl1bs2bNnTt35HJ5VVUVHlmdkJDg7++PO3C53JycnNbW1iVLltTU1LS3t+fl5aWlpfn7+ycmJtKMloQfvgoJCRnqB/VBn6PFEIymBgBoOpoajzwixcbGlpSUUFs++eQT4tkL0fPnz8efFQqFLi4ut2/fDg0NtbKy4vF4gYGBxcXF1OW3t7cvX77cycmJx+PNmDFDJBKR520ff/wx7lNZWRkQEMDn811dXakDmwMCAoZ7NDVBEJs2bTIzM3v48CF+qXDTdMCRzKtWrVIYTU0QREtLS1JSkru7O5vNFggEoaGhFy9exG/R36X4YeWxY8ey2eyRI0eGhIT89NNP1LUEBQVZWlr29vYOtjlyufzkyZORkZEeHh74+vnMmTPz8/OVe169ejU0NFQgEHA4nAkTJmzdurW7u5vagRzwRUV9WAuLjo52cXF5+vTpYCGR9D+amkUoFb4YPiwW6/jx44sXL9bbGgEABig6OhohdPLkSb2t0cfHp6WlRcuBvsOH5m9jR0eHl5dXWFhYVlaWfgLTWHt7u7Ozc2xs7MGDB5mO5T/EYrGvr29+fv6SJUvUdj5x4kRMTIw+8yNcpjYCx44dw4MV8XDKYaU8yZFm6MSsz+0iUedj0ttKtWdpaUkdtmpiYmJraysUClevXl1eXs50dEBPBAJBQUHBqVOn9u3bx3QsqhAEkZiYaG1t/dlnnzEdy3/U1dVFRUUlJyfTycSMgGRsBJYsWUIQRHBwsB7WtXDhQuKPR/5VUFvJjk7M+twu0oYNG4g/poAwIp2dndevX0cILViwgCCInp6eysrKbdu2VVZWTpo06a9//Wt3dzfTMQJ98PX1LSsrO3funFQqZTqWQTU1NdXV1V28eJHm8Gw9yM7OTk1NTU1NZTqQQT23yZjZSmfPPYLpSnYvOFNTUwcHhwULFly6dOmjjz7Kzc1988039XlJzYjgayFisfjhw4csFmvz5s1MR6QtNze3wsJCa2trpgMZlKOjY3FxsZeXF9OB/NfOnTsN9pwYe26TMRhWuJLd2bNnmQ4EoL/97W/+/v7ff//9sWPHmI7FEOFrISQ8kyIAhgaSMQDGjcVi4YmWlJ8oBQAYC4NLxr29vcePH3/99dcdHR15PJ63t3dmZiZ5OVT7SmcqaodhKuqC0S8uRq6Fy+WOGjVq9uzZubm51AlR1YZRWVm5cOFCgUDA5/MDAgKKi4uV9xXNUKuqqhYvXmxnZ4dftrS00DwWjY2NA27jYIO86MSsw+1SfQiGRMW3rr29nTpyCp9X9fb2ki14Kl36YWt8OFTAfw6lpaXkXIna78MnT55s2bJlwoQJFhYWL730Unh4OK4sS3bQVQU9AABChvecMX5cbMeOHW1tbRKJ5O9//7uJiYnChSaNi6uorR1Gpy6Y2uJieC2Ojo4FBQVSqbSxsREPKczIyKAZRk1NjY2NjYuLy4ULF2QyWUVFRUhIiJubG5fLJddCP9TAwMDLly93dXWVlpaamppSS9kMRmEbL168aG1trVCTTqF4Dp2YdbtdqkvmqYafOiVfqv3WhYaGmpiY3Llzh7qQqVOnklVrtDwcdArwUQdwKSD/n1dfX6+rfbh8+XKBQHDhwoXu7u7GxsYNGzYghC5fvkx/e1XQpmrTc4nObyPQJyihSBQUFMycOZPaEhcXx2azOzo6yBaNk7Ha2mF06oKpLS6G16KwpXPmzCGTsdow8FOYp06dIjs8fPiQy+VSkxb9UM+ePUsMkfI2Ll26FD1bk04hGdOJWbfbpbpknmrKyVj1tw7Xh1m9ejXZobi4mDp7gJaHg04BPhXJmBxKjZOxTvahu7v7tGnTqGvx9PQkk7GWFfQgGSuAZGxo9J+M/3v91kCEhYUpPDAjFAoPHz5869Yt7cuTDVY7LC8v78cff3zrrbdU1wWjPpk6YHExfKkcr2Xu3LnUVZ87d45+GOfPn0cIUeeJdXZ29vT0rK6uJlvoh4rrg2qAuo0uLi7UbVRGJ2bdbpeKQzBUar91ISEh3t7eubm527ZtwwVkPv/884SEBLKenZaHo6ioSIOwSXgafTabjTdfJ/twzpw5X3311Xvvvbds2bLJkyebmppWVVWRnemvYjClpaX4P2cAy8jI0OcsKEA1/c8PY3D3jDs6OrZs2eLt7W1ra4tva3344YcIIe0fo1RbO2xIdcFUFxdTXsuQwpDJZObm5paWltQO1GlyhxQqLgiqAeo2mpiYIKWadNR46MSs2+1SWzKPPjrfuqSkpO7ubjxIqrq6+tKlS++9954GYWt8OFTAt96nTp3KZrN1tQ/37dv3j3/8o66uLjg42Nraes6cOeRslMNaQQ+AF5PBnRmHh4f//PPPmZmZb7755ogRI1gs1p49ez744AOC8gylZpXO8NynHR0dMpmMmgjJ2mG4LlhnZ+fjx4+pY76GZLC1DCkMKysrmUzW2dlJzVttbW3UhWgfqg7RjNlgt4vOty42NnbTpk179+796KOPdu/e/fbbb9va2jIbNtbf34/nY1qzZo0Og2GxWPHx8fHx8T09PUVFRenp6VFRUbt37163bp1OVjFlyhQ4ESSxWKwPPvgApgo2HHg6TH2u0bDOjPv6+q5cueLo6JiYmDhy5EicUKmDkDGNK52prR2mk7pgeC0Kz+D6+vp+8MEH1A4qwsCXuPFFXaylpYV6kVBXoeoQnZgNc7tofuu4XO7q1aubm5t379595MiRtWvXMhs2KTk5+f/9v/8XGRlJXvXVSTA2NjaVlZUIITab/frrr+Mx2OSX1tC+fgAYPX3eoEY0BikEBQUhhHbt2iWRSLq7uy9dujR69GiEELUeCH6q8ssvv5TJZHfu3Fm8eLGLi4vCAK45c+YIBILff//96tWrZmZmt2/fJp4dxiyVSslhzAcOHMCfampq8vDwGDt27NmzZ9vb21tbW7OysiwsLKhhKwxcIgji448/Rghdv34dv8RrcXJyKiwslEql9+/fX7VqlYODw71796gdVIRx586dl156iRx1fOvWrdDQUHt7e+pAJ81CpUntNir3oRPz8G2XcniqKQzgovOtIwhCIpHweDwWi6U8ikrLwzHU0dR9fX1NTU1nzpzBkS9btoxaxEYn+1AgEAQGBorFYrlc3tTUtHXrVoTQ9u3b6a9CBRjApYDObyPQJxhNTUgkkpUrV7q6urLZbAcHh3feeWfjxo34/w3kQE1tKp2pqB2GqagLRr+4GHUtTk5OS5Ysqa6upq5FbRhVVVULFy60trbGD5wUFhaSczi/++67Qw2V/reKzjYqV7KjH7Nut0vFIRjM559/rrwEOt86bMWKFQihf/7zn8pL1uZwqC3Ap3CnmcViCQQCb2/vVatWlZeXaxPMYPvwxo0bK1eu/POf/4yfM54yZcrBgwf7+/vprEItSMYKECRjAwMlFAEwaN9+++2+ffvKysqYDsS46b+EooGD30ZDAyUUATBoWVlZ69atYzoKAJ5x7969iIgIqVTa0tJCDm739fVVqIJKfZfFYk2aNImpgFWYMWMGS0lSUpJCtxs3bsyfP9/GxsbKymr27NkKwxc2btyIT22NCCRjANTIycmJjIzs7OzMysp69OgRnL4Ag3Ljxo1JkyaFhIRYW1uPGDGCIAiRSITbFXIYfrekpASPsDHeCzz//ve/p02bZmVl9euvv969e3fs2LEzZ868cOEC2WHFihXJyckpKSkMBjlUkIxfOMr/6yThQTrGbjg28MyZM7a2tl999dWxY8cM4VkyMCTDXVCVwYKtUqk0PDz8jTfewMNaSVwu187OLjs7++jRo4wEpg2RSKRwP3XPnj3ku/39/e+++66Njc23337r5OQ0YsSIr776ysPDY/ny5U+ePMF9PDw8Tp8+nZqaeuLECYY2YsggGb9wVIwgeD6Ssc43cPny5QRB9PT0iMXiiRMn6jpeADS3a9euxsbGLVu2KLSbm5sfOXLExMRk5cqV1BnungP/+te/bt26tWjRIh6Ph1tMTU3ffPPN+/fvFxYWkt2EQuGiRYvWr19vLM/aQTIGAACjRBBETk6Ov7+/s7Oz8ruhoaGbN2+WyWTR0dEKN4+N2qVLlxBCCje88cuLFy9SGyMjIx88eECd0cGQQTIGABguFcVGtSmoittZLNaoUaNEIlFwcLCVlZWFhcWsWbPIoUDaF2wdbmKxuKmpSSgUDtbh008/DQkJqaioSEhIUL0oFfuZft1SXVXVzMvL8/Hx4fP5AoEgICAgPz+f+i6ei0Zh/nM8eb7CNQAfHx+EEK7yYgS0eS5qqBA8SwcAoP2csdpio4QWNdwIghAKhXw+f+rUqbiOpEgkeuWVVzgcTlFRkU6WT2cuF0yz38a8vDyE0I4dOxTaRSKRQCDA/5ZIJLgEyOHDh3ELOYCLRGc/q625qWVVTdL06dPj4+PLy8s7OzsrKyvj4+MRQgkJCWSH119/HSFUWlpK/RSeEX3ixInUxo6ODoRQQEDAkALA9P+cMZwZAwAMVHJy8t27d/fs2RMWFmZtbe3p6Zmfn+/k5JSYmIjnctdeV1fX/v37p06dyufzJ02adPjw4adPnyrMdaoxco4UnSxNGa7WRa32oWzEiBEnTpxgs9krV67E55TK6O/n5cuX4301e/bs+fPni0SilpYWciH37t374osv5s2bZ2lp6eXldezYMYIg1J6UKyguLv7HP/4xceJEPp8/fvz4f/zjH6+++uqXX37573//W8Wn8E5mPVuSwNramsVi4b1k+CAZAwAM1GDFRh8/fqyra498Ph9fzMS8vb2dnZ3FYrFOfsGLiora2tq0r/06GHwnmKzjOZgpU6akp6d3dXVFR0crT7qOhrKfB6y5iV+qrqo51E2jwpW2CwoK8EsbGxuEUFdXF7UPfonfojIzMxtwkw0QJGMAgCFSW2xUJ2tR/vnGNT2bm5t1svxhZW5ujhDq6elR2zMxMTEmJubmzZsKT0ChIe5n1aVjh6mqppOTE6IckQkTJiClesO4dJCnp6fCZ3t7e8lB1wYOkjEAwBDhYqNyuVwmk1HbyWKj+KVmBVVJra2tCpeR8Y8+WWZby+UPK5yl8J1RtXJycsaPH//NN9/gO80kmvtZNVxV08zMrKenR/lu6KxZs+hu0kDwyTd5RPDSysvLqX3wS3Kie0wqlRIEgfeS4YNkDAAwUGqLjSItCqpicrkcz1eF/fLLL/X19UKhkPwF13L5w+rll19GSueIg7G0tPzuu+/4fP7+/fsV3qKzn9XSSVXNnJwcsuoPRhAEnrgjPDwctwQGBv7lL385deoU+bxWX1/fsWPHXF1dqVfa0R+ny3gvGT5IxgAAA5WWlubu7p6UlFRYWCiTyaqrq5cuXdrQ0JCZmYkvoiL0/9u797AorvMP4GeB5eLCLgoiF/FGRFtiV7wEUSmKClFBhYB4gUSN1sdaUKNtxGqSR0WqJVUatSKYalS8kUefQNSYx8vToovlEteq4W4NgiCXwi7oCsj8/jjt/KaLLMvusrPA9/MXO3t25p1ZmJeZOee8JDAwsLKy8uDBg01NTaWlpRs2bGAvoVgTJkwoKioqLy+XyWRlZWV+fn7sWxKJZNu2bTKZrLm5OTc3NyoqytLSMikpiW2gz/oDAgIcHByys7MNf2gIIYRIpVInJye5XK5ley8vr+Tk5I7LtTnOXUpISPDw8Fi1atWVK1caGxvr6+uTk5N37tyZmJjIDveKiooSCASPHz/WsJ78/Pz169eXlJSoVKrCwkLaszomJsbHx4c2MDMzO3bsWH19/cqVK6uqqurq6tavX19cXJySkkLv27PowKrAwEAtd4FnRuu3zWBoEwAwDNOdEopdFhvVp6AqLWv96NGjoKAgOzs7Gxsbf3//rKwsQ62/y8qYLJ3Pjdu2bbOwsKioqKAva2pquKd3tQKg1Lp169SGNjEaj7P2dUu7rKoZEBBga2vb1tbW2e6oVKoLFy6EhoZ6eHjQ++czZsxIS0vr2DI/P3/u3LlisdjW1jYgIEDtW6MiIiLc3NxaWlo625wGKKEIAH2fiZRQHD9+fG1trZ59fQ1C53NjY2Ojl5dXcHDwkSNHeiIwA2poaHB1dV2+fHlKSooRNieXy729vdPS0pYsWaLDx1FCEQAAtCWRSDIyMtLT0w8dOsR3LJowDBMbGysWi3ft2mWEzZWVlYWFhcXFxemWiXmBZAwA0It5e3vn5uZeuXJFoVDwHUunqqury8rKrl+/rmX3bD0lJyfHx8fHx8cbYVuGgmQMAP0OnVNaLpdXVFQIBILt27fzHZFeRowYkZmZKRaL+Q6kU87OzllZWV5eXsbZ3N69e3vRNTGFyqwA0O9s2bJly5YtfEcB8P9wZQwAAMAzJGMAAACeIRkDAADwDMkYAACAZ0jGAAAAPDP2DFxG2xYAAIA+jJkfjTq0ic72CQAGtH//fkLIpk2b+A4EAHRn1CtjADA4OqExLTMHAL0UnhkDAADwDMkYAACAZ0jGAAAAPEMyBgAA4BmSMQAAAM+QjAEAAHiGZAwAAMAzJGMAAACeIRkDAADwDMkYAACAZ0jGAAAAPEMyBgAA4BmSMQAAAM+QjAEAAHiGZAwAAMAzJGMAAACeIRkDAADwDMkYAACAZ0jGAAAAPEMyBgAA4BmSMQAAAM+QjAEAAHiGZAwAAMAzJGMAAACeIRkDAADwDMkYAACAZ0jGAAAAPEMyBgAA4BmSMQAAAM+QjAEAAHiGZAwAAMAzJGMAAACeWfAdAAB0z927d+VyOfuyrKyMEHL06FF2iVQq9fHx4SEyANCVgGEYvmMAgG7IzMwMCQkxNzc3MzMjhNA/YYFAQAhpb29//fp1RkZGcHAwz1ECQHcgGQP0Mq2trY6OjgqF4o3visXimpoaS0tLI0cFAPrAM2OAXkYoFC5duvSN6VbDWwBgypCMAXqfpUuXtrS0dFze2tq6bNky48cDAHrCbWqA3qe9vd3V1bW6ulpt+eDBg6uqquizZADoRfBHC9D7mJmZRUdHq92OtrS0XLFiBTIxQG+Ev1uAXqnjneqWlpalS5fyFQ8A6AO3qQF6q9GjR5eUlLAvR40aVVpaymM8AKAzXBkD9FZRUVFCoZD+bGlp+cEHH/AbDwDoDFfGAL1VSUnJ6NGj2ZeFhYWenp48xgMAOsOVMUBv9dZbb0mlUoFAIBAIpFIpMjFA74VkDNCLvf/+++bm5ubm5u+//z7fsQCA7nCbGqAXq6ysdHd3ZximvLzczc2N73AAQEd9JBlHRETwHQIAP27dukUImTFjBs9xAPDkwoULfIdgAH3kNnV6evrTp0/5jgKAB8OGDRs+fHhPrDk7Ozs7O7sn1txL4Txjap4+fZqens53FIbRR66MBQLBuXPnFi9ezHcgAMZWX19PCBk0aJDB10xvOPWNyw6DwHnG1Jw/fz4yMrJvZDELvgMAAL30RBoGACPrI7epAQAAei8kYwAAAJ4hGQMAAPAMyRgAwBiePHmyYMEChUJRW1sr+C9vb2+VSsVtxn1XIBBMmjSJr4A1mD59uqCDjRs3qjW7d+/e/Pnz7e3t7ezsZs+effv2be67W7duPXfunBGjNmlIxgBgYE1NTaNHjw4ODuY7EBNy7969SZMmBQYGisViR0dHhmFycnLocrUcRt+VyWQODg4Mw+Tm5vIUsr7u3r07depUOzu7H3/88fHjx6NGjZoxY8a1a9fYBmvWrImLi9uxYwePQZoOJGMAMDCGYdrb29vb2/kKwNbWdvr06XxtvSOFQhESEvLee+/95je/4S63srJycHBITk4+c+YMX7HpLCcnh/lfBw4cYN9tb2//8MMP7e3t//rXv7q4uDg6Ov7lL3/x8PBYvXr1q1evaBsPD4+LFy/Gx8efP3+ep50wIUjGAGBgdnZ2paWlly9f5jsQU7Fv376qqqpPPvlEbbm1tfXp06fNzMzWrl1bVFTES2w95G9/+9vDhw/Dw8NtbGzoEnNz86VLl5aXl2dmZrLNpFJpeHj45s2b29raeIrUVCAZAwD0IIZhUlNTfXx8XF1dO74bFBS0fft2pVIZERGh9vC4V7tx4wYhRO2BN315/fp17sLQ0NCnT59+++23xgzPBCEZA4AhXbp0ie3RQ7MLd8m//vWvyMhIe3t7BweH4ODg0tJS+qnExETaYOjQoTk5ObNmzbKzsxswYMDMmTPZXj+7d++mbdhb0FevXqVLHB0duetpbm6+ffs2fcvCguepjeRyeXV1tVQq7azBp59+GhgYeP/+/ZiYGM2rqqur++ijjzw8PCwtLQcOHDh37tybN2/St7Q5yFRNTU1sbOyIESMsLS0HDx4cFhZ27949Hfbr5MmT48ePF4lEEonEz88vLS2N+25BQQEhZOjQodyFtJaJ2j2A8ePHE0K+++47HWLoU5g+gRBy7tw5vqMA6FPCw8PDw8N1++zChQsJIS9fvlRbsnDhwjt37jQ1NX3//fc2NjaTJ0/mfkoqlYpEIl9fX9omJyfnF7/4haWl5a1bt9g2IpFo2rRp3E9NnDiR9nXS0IaaOXPmoEGDZDKZbjul23nm5MmThJA9e/aoLc/JyZFIJPTnmpoad3d3QsipU6foErYDF+vZs2cjR44cMmRIRkZGY2NjYWFhWFiYQCBISUlh23R5kCsrK4cPHz5kyJBvv/1WqVQ+ePDA39/f2tr6zp073dqpadOmRUdH5+XlNTU1FRQUREdHE0JiYmLYBnPmzCGEZGdncz9VXFxMCJkwYQJ3YWNjIyHEz8+vWwFQtDO2Dh80QbgyBgDjWb16ta+vr0gkmj179vz583Nycmpra7kNmpubDx8+TNtMmjTp1KlTLS0tGzZsMMjW29vb6YnPIGvT0rNnzwghEolEQxtHR8fz588LhcK1a9fSa8qO4uLiHj9+fODAgeDgYLFY7OnpmZaW5uLiEhsbW11dzW2p4SDHxcU9efLkT3/607x582xtbb28vM6ePcswTJcX5WqysrK++uqrCRMmiESiMWPGfPXVV++8884XX3xx9+5dDZ+iR14gEHAXisVigUBAj1J/hmQMAMYzefJk9md6LVhZWcltIBKJ6H1Laty4ca6urnK53CAn61u3btXX1/v6+uq/Ku3Re/VCoVBzsylTpiQmJjY3N0dERLx8+bJjg4sXLxJC5s+fzy6xsrKaNWvWy5cv1e7xajjIly5dMjMz4446c3Z29vLyysvL07MgVXh4OCEkIyODvrS3tyeENDc3c9vQl/QtLgsLizfucr+CZAwAxsO9QLS0tCSEqI2A6nimdnJyIoQ8f/6856PrEdbW1oSQ1tbWLlvGxsZGRkY+ePBAbQQUIeTVq1eNjY3W1tZ2dnbc5UOGDCGEVFVVcRd2dpDpStrb2yUSCXeyjvz8fEIIvYesMxcXF8L5msaOHUsIUUvwFRUVhBBPT0+1z7a1tbGdrvstJGMAMCF1dXVqt5Hp+Z2mZEKImZlZS0sLt0FDQ4PaStRuhPKLZin6ZLRLqampY8aM+fLLL+mTZpaVlZVEIlGpVEqlkruc3qB2dnbWZuVWVlb29vYWFhatra0dn1nOnDlT2116E3rxzX5NdG15eXncNvTlrFmzuAsVCgXDMPQo9WdIxgBgQlQqFZ2aivrnP/9ZWVkplUrZk7WLiwu9wKKqqqp++ukntZUMGDCATdhjxow5evRoD0etydtvv006XCN2xtbW9uuvvxaJRIcPH1Z7KzQ0lBDCHQL06tWr69ev29jYBAUFaRlMWFhYW1ub2rSUe/fuHTZsmPYjfVNTUydOnMhdwjAMnbgjJCSELvH39//5z3+enp7Ojtd6/fr12bNn3d3duXfayX8vl+lR6s+QjAHAhEgkkm3btslksubm5tzc3KioKEtLy6SkJLZBYGBgZWXlwYMHm5qaSktLN2zYwF6NsSZMmFBUVFReXi6TycrKyvz8/OjygIAABweH7Oxs4+0PIVKp1MnJSS6Xa9ney8srOTm54/KEhISRI0du3LgxMzNTqVQWFRUtW7bs2bNnSUlJ9Ga1NhISEjw8PFatWnXlypXGxsb6+vrk5OSdO3cmJiayY8CioqIEAsHjx481rCc/P3/9+vUlJSUqlaqwsJD2rI6JifHx8aENzMzMjh07Vl9fv3Llyqqqqrq6uvXr1xcXF6ekpND79iw6sCowMFDLXeizjNZvu0cRDG0CMDTdhjbRfkas5cuXy2Qy7pLf//73zP/eiJ4/fz79rFQqdXNze/ToUVBQkJ2dnY2Njb+/f1ZWFnf9DQ0Nq1evdnFxsbGxmT59ek5ODnuV9vHHH9M2BQUFfn5+IpHI3d390KFD7Gf9/PwGDhzY3WE8LJ3PM9u2bbOwsKioqKAva2pquLs/ceLEjh9Zt26d2tAmhmFqa2s3btw4cuRIoVAokUiCgoKuX79O39L+INPByqNGjRIKhYMHDw4MDPz++++5WwkICLC1tW1ra+tsd1Qq1YXMb7c/AAAP70lEQVQLF0JDQz08POj98xkzZqSlpXVsmZ+fP3fuXLFYbGtrGxAQoPZVUhEREW5ubi0tLZ1tToO+NLRJwBi3l38PEQgE586dW7x4Md+BAPQdERERhJALFy4YbYvjx4+vra3Vs1tvz9H5PNPY2Ojl5RUcHHzkyJGeCMyAGhoaXF1dly9fnpKSYoTNyeVyb2/vtLS0JUuW6PDx8+fPR0ZG9o0shtvUAAA9SyKRZGRkpKenHzp0iO9YNGEYJjY2ViwW79q1ywibKysrCwsLi4uL0y0T9zFIxv3a2bNn6dgGtac4oBtbW1vuiBEzM7OBAwdKpdJf//rXat1Kob/x9vbOzc29cuWKQqHgO5ZOVVdXl5WVXb9+Xcvu2XpKTk6Oj4+Pj483wrZMH5Jxv7ZkyRKGYdRGGoDOmpqafvjhB0LIwoULGYZpbW0tKCjYuXNnQUHBpEmTVq5c+eLFC75jNFF0Tmm5XF5RUSEQCLZv3853RIY3YsSIzMxMsVjMdyCdcnZ2zsrK8vLyMs7m9u7di2tiFpJxt5laqdR+qKe/AkOt39zcfMiQIQsXLrxx48bvfve748ePL126tG883zK4LVu2cDuz7N69m++IAIwKyRjAGP7whz/4+Ph88803Z8+e5TsWADA5SMYAxiAQCOgchx0ncwAA6EfJuK2t7dy5c3PmzHF2draxsRk3blxSUhI7L67+pVI1lBqlNJQR1b4WKbsVKyuroUOHzp49+/jx49w51rsMo6CgYNGiRRKJRCQS+fn5ZWVldTxWWoZaWFi4ePFiBwcH+lKt/M4baQhPn6+gV1TDpdvNzs5mpynW/1fi1atXn3zyydixYwcMGDBo0KCQkJBvvvnm9evXbANDFa8FgJ5l/KHNPYFoMRiflhPZs2dPfX19TU3Nn//8ZzMzM7UnVTqXSu2y1Kg2ZUS7rEVKt+Ls7JyRkaFQKKqqqugIhP3792sZRnFxsb29vZub27Vr15RK5f379wMDA0eMGGFlZcVuRftQ/f39b9682dzcnJ2dbW5uXlNTo/kr0KYgqz7Vak2hGi63A5ca9n+myspKxkC/EqtXr5ZIJNeuXXvx4kVVVdWWLVsIITdv3qTv6lm8Vp96xn2SNucZMKa+NOlHX9kN7ZLxjBkzuEuioqKEQmFjYyO7ROcz9YoVKwghZ86cYZeoVCpXV1cbG5uqqiqGYT744ANCyOnTp9kGz549s7Ky4k6+Q8+8GRkZ7BJalYxNcnQranv67rvvssm4yzDoNA7p6elsg4qKCisrK24y1j7Uy5cvM93RZXiM3smYEPLDDz+wS+7fv08IkUqlGj6r/fr9/f27nL9JQzJmu1LTZGyQX4mRI0dOnTqVuxVPT082GWuzCQ2QjNUgGZsaJGOTo9sfyR//+EdCCPfcqvOZmtYso+VHWNHR0YSQEydO0AZmZmbcxM8wzIQJEwgh5eXl9CU987JpiWGYTZs2EULkcrmGrXQrDFp/TalUchuMGzeOm4y1D7W2trazSHQLjzHElbHaQldXVzb/6bl+bWhIxvT2slAopDP/GeRXYt26dYSQNWvWyGSyjvMXarMJDWjiBzBxXf9Z9gYGex5m+hobGz///POLFy8+ffqUW3NN/6GfXZYapQ3I/9YZZRUXFw8dOpR9qbkWacetdCsMpVJpbW1ta2vLbeDk5FRUVMRdiZahikSiN0aiW3jar0qDN1bDraysfP78Oe812ujjeV9fX6FQaJBfCULIoUOHfH19T5w4QQeL+/n5rV27lpb36dYmOjNlyhSa/oEQEhkZuXHjRl9fX74Dgf+QyWQHDhzgOwrD6EfJOCQk5O9//3tSUtLSpUsdHR0FAsGBAwc2bdrEcMZ96lYqlU6V3tjYqFQquZmGLTVKy4g2NTW9fPlS5w5BnW2lW2HY2dkplcqmpiZuPq6vr+euRP9QuxW/WkFWPavV0mq43AYmUg23vb2dToW4fv16YrjjLBAIoqOjo6OjW1tbb926lZiYGBYW9vnnn3/00UcG2cTQoUMx5TsrMjLS19cXB8Sk9Jlk3F96U79+/fr27dvOzs6xsbGDBw+mZ1tuJ2RK51KpXZYaNUgZUbqVy5cvcxd6e3uz1y5dhjF37lxCyNWrV9kGtbW1hYWF3BUaJFQN8WsuyKpntVqTrYYbFxf3j3/8IzQ0lD62JwY6zvb29gUFBYQQoVA4Z84c2gebPcI991UCgIHxfZ/cMIgWz4wDAgIIIfv27aupqXnx4sWNGzeGDRtGCOGWD6MjQb/44gulUllSUrJ48WI3Nze1B4rvvvuuRCL56aef7ty5Y2Fh8ejRI+Z/+wkrFAq2n/DRo0fpp6qrqz08PEaNGnX58uWGhoa6urojR44MGDCAGzZ9QPjy5Ut2yccff0w4PZLoVlxcXDIzMxUKRXl5+bp164YMGfLkyRNuAw1hlJSUDBo0iO1N/fDhw6CgICcnJ+4zY91C1UaX4enzFTAMI5VKJRLJrFmzNPSm1mf93e1N/fr16+rq6kuXLtHfvVWrVr148ULP46z2KyGRSPz9/eVyuUqlqq6u/uyzzwghu3fv1n4TGqADlxptzjNgTOjAZXK0+SOpqalZu3atu7u7UCgcMmTIihUrtm7dSv8jYTuX6lMqVUOpUUpDGVHta5Fyt+Li4rJkyZKioiLuVroMo7CwcNGiRWKxmA6SyczMZOem/vDDD7sbanf/EroMT5+vgPdquGoP0QUCgUQiGTdu3Lp16/Ly8jq21/9X4t69e2vXrv3Zz35GxxlPmTIlJSWlvb1dm010CclYDZKxqelLyRj1jKHvMPFquL2O8esZmzicZ0wN6hkDAIAmT548WbBggUKhqK2tZSdT8/b2VqlU3GbcdwUCwaRJk/gKWIPp06cLOti4caNas9bW1v3790+cONHOzs7JyWnu3Ll0iLxas8uXL3t6er6xU+HWrVvpxW4/hGQMAGBg9+7dmzRpUmBgoFgsdnR0ZBiG9iu8d++eWg6j78pkMtpxITc3l6eQ9dXc3BwQEHD8+PH9+/c/f/48NzfX1tZ2wYIFDx8+ZNuUlpYuWLAgLi6OjqHoaM2aNXFxcTt27DBW1CYEyRgMpuP/zizasajn9IdquH1eb6mM2SWFQhESEvLee+/R3oIsKysrBweH5OTkM2fOGCEMw8rJyVF7xqk2pui3v/3t/fv3r1279stf/tLGxmbYsGHHjx+3srLittmxY8fUqVPz8vI6myzBw8Pj4sWL8fHx58+f78GdMUn9aJwx9DQen9xs2bKFTssMwLt9+/ZVVVV98sknasutra1Pnz49b968tWvXTpw40dPTk5fwekJ1dfXRo0d/9atf0Tl8KJFIpHZP/tixYzY2NppXJZVKw8PDN2/eHBYWZtipDkwcrowBAAyGYZjU1FQfHx86D6uaoKCg7du3K5XKiIgItUTVq9FaYV3eeOgyE1OhoaFPnz7lTkjQHyAZA4C++nNlTDVyuby6uprWLHmjTz/9NDAw8P79+zExMZpXpeGoal901VA1NE+ePDl+/HiRSCSRSPz8/NLS0rjv5ufnE0IGDhy4efNmd3d3S0vL4cOHx8bGcmf309748eMJId99950On+3FjDqQqscQjP8DMDQtxxn3h8qYlDbnmZMnTxJC9uzZo7Y8JydHIpHQn2tqatzd3Qkhp06dokvYDlwsbY5qlxU29ayhyZo2bVp0dHReXl5TU1NBQQEt7hITE6MWibOz8/Lly0tLS//973+fOHFCJBJ5eno2NDR0XKGbm5u5uXlnm6Nzqvv5+XUZWF8aZ9xXdgPJGMDQtEzG/aEyJqXNeWbfvn2EEO5cMRQ3GTMMI5PJhEKhSCT68ccfmTclY22OapcVNvWsoanBO++8QwjJzs6mL+l0tiNHjmxtbWXb7N69mxCyY8eOjh/XnIwZhhEIBG+99VaXYfSlZIzb1ACgl4sXLxJC5s+fzy6xsrKaNWvWy5cvDXWnUSQS0VuX1Lhx41xdXeVy+bNnz/Rf+a1bt+rr6w1Vi4k+CRYKhZqbTZkyJTExsbm5OSIiouMk+aQ7R3Xy5Mnsz/SCu7Kykr68dOmSmZlZcHAw28DZ2dnLyysvL0/PuXFo1s/IyKAv6dxzs2fP5t7/DwkJIbrebbawsHjjYenDkIwBQHc8VsYk/y3JZVKsra0JIa2trV22jI2NjYyMfPDggdoIKNLNo6q56Gp7e7tEIuGOM6TPd4uLi3XbQYpWXmGP/4gRIwghDg4O3Db0O6qpqdFh/W1tbVr29uozkIwBQHe0MqZKpVIqldzlPVEZk7vERCpjdkSzFH3q2aXU1NQxY8Z8+eWX9EkzS8ujqhmtoWlhYcG9dcyaOXOmtrv0JvTimz3+tPec2o0K+h1xBztpSaFQMAzDewFyI0MyBgC99OfKmB29/fbbhBAtbwLb2tp+/fXXIpHo8OHDam9pc1S7ZJAamqmpqWwxFYphGDopB70RTQiZN2+em5vb1atXueO16E3sRYsWabkhFv0q6ZHsP5CMAUAvCQkJI0eO3LhxY2ZmplKpLCoqWrZs2bNnz5KSktirosDAwMrKyoMHDzY1NZWWlm7YsIG9qGJNmDChqKiovLxcJpOVlZX5+fmxb0kkkm3btslksubm5tzc3KioKEtLy6SkJLaBPusPCAhwcHDIzs42yNGQSqVOTk5yuVzL9l5eXsnJyR2Xa3NUu5SQkODh4bFq1aorV640NjbW19cnJyfv3LkzMTGRfbgbFRUlEAgeP36sYT35+fnr168vKSlRqVSFhYW0Z3VMTIyPjw9tYGVllZqaWldXt2TJkuLi4oaGhpMnTyYkJPj4+MTGxmoZLYsOvgoMDOzuB3s3I3cY6yEEvakBDE37Eop9uzImS8vzzLZt2ywsLCoqKuhLtYemb+zJvG7dOrXe1IzGo6p90dUua2gGBATY2tq2tbV1tjsqlerChQuhoaEeHh70/vmMGTPS0tI6trxz505QUJBEIrG0tBw7duxnn33GLeDNMAzb4YuLO1iLioiIcHNza2lp6SwkVl/qTY0SigDwZiZSQtF0KmNqeZ5pbGz08vIKDg4+cuSIcQLTWUNDg6ur6/Lly1NSUviO5T/kcrm3t3daWtqSJUu6bIwSigAA8GYSiSQjIyM9Pf3QoUN8x6IJwzCxsbFisXjXrl18x/IfZWVlYWFhcXFx2mTiPgbJGADAwLy9vXNzc69cuaJQKPiOpVPV1dVlZWXXr1/Xsnu2ESQnJ8fHx8fHx/MdCA+QjAHARPXqypgjRozIzMwUi8V8B9IpZ2fnrKwsLy8vvgP5f3v37u2H18RUPypQBQC9CypjQv+BK2MAAACeIRkDAADwDMkYAACAZ0jGAAAAPOs7HbjUpqQBAD3ReTboLMRA4TxjUvrS19F3ZuDiOwQAAOBBH8lifWM3AAAAei88MwYAAOAZkjEAAADPkIwBAAB4hmQMAADAs/8D04pg9DhkqvcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ide_AE,\\\n",
    "latent_encoder_score_Ide_AE=Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                                 p_encoding_dim=50,\\\n",
    "                                                 p_learning_rate= 1E-2,\\\n",
    "                                                 p_l1_lambda=l1_lambda)\n",
    "\n",
    "file_name=\"./log/AgnoSS.png\"\n",
    "plot_model(Ide_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 4135 samples, validate on 460 samples\n",
      "Epoch 1/1000\n",
      "4135/4135 [==============================] - 1s 183us/step - loss: 132.8542 - val_loss: 30.1016\n",
      "Epoch 2/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 5.5135 - val_loss: 0.0221\n",
      "Epoch 3/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0220 - val_loss: 0.0223\n",
      "Epoch 4/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 5/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 6/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0220 - val_loss: 0.0222\n",
      "Epoch 7/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0220 - val_loss: 0.0222\n",
      "Epoch 8/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 9/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0219 - val_loss: 0.0231\n",
      "Epoch 10/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0220 - val_loss: 0.0226\n",
      "Epoch 11/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0220 - val_loss: 0.0223\n",
      "Epoch 12/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 13/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0227\n",
      "Epoch 14/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 15/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 16/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0220 - val_loss: 0.0221\n",
      "Epoch 17/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 18/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 19/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 20/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 21/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 22/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 23/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 24/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 25/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 26/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 27/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 28/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 29/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 30/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 31/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 32/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 33/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 34/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 35/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 36/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 37/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 38/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 39/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 40/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 41/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 42/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 43/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 44/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 45/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 46/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 47/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 48/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 49/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 50/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 51/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 52/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 53/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 54/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 55/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 56/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 57/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 58/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 59/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 60/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 61/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 62/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 63/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 64/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 65/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 66/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0226\n",
      "Epoch 67/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 68/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 69/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 70/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 71/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 72/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 73/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 74/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 75/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 76/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 77/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 78/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 79/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 80/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 81/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 82/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 83/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 84/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 85/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 86/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 87/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 88/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 89/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 90/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 91/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 92/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 93/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 94/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 95/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 96/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 97/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 98/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 99/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 100/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/Ide_AE_weights.0100.hdf5\n",
      "Epoch 101/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 102/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 103/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 104/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 105/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 106/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 107/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 108/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 109/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 110/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 111/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 112/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 113/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 114/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 115/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 116/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 117/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 118/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 119/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 120/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 121/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 122/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 123/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 124/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 125/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 126/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 127/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 128/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 129/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 130/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 131/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 132/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 133/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 134/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 135/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 136/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 137/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 138/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 139/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 140/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 141/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 142/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 143/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 144/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 145/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 146/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 147/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 148/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 149/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0228\n",
      "Epoch 150/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 151/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 152/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 153/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 154/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 155/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 156/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 157/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 158/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 159/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 160/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 161/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 162/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 163/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 164/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 165/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 166/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 167/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 168/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 169/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 170/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 171/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 172/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 173/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 174/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 175/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 176/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 177/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 178/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 179/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 180/1000\n",
      "4135/4135 [==============================] - 0s 98us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 181/1000\n",
      "4135/4135 [==============================] - 0s 100us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 182/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 183/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 184/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 185/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 186/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 187/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 188/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 189/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 190/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 191/1000\n",
      "4135/4135 [==============================] - 1s 139us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 192/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 193/1000\n",
      "4135/4135 [==============================] - 1s 136us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 194/1000\n",
      "4135/4135 [==============================] - 1s 145us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 195/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 196/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 197/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 198/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 199/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 200/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/Ide_AE_weights.0200.hdf5\n",
      "Epoch 201/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 202/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 203/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 204/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 205/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 206/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 207/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 208/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 209/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 210/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 211/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 212/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 213/1000\n",
      "4135/4135 [==============================] - 1s 136us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 214/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 215/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 216/1000\n",
      "4135/4135 [==============================] - 0s 103us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 217/1000\n",
      "4135/4135 [==============================] - 0s 98us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 218/1000\n",
      "4135/4135 [==============================] - 0s 98us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 219/1000\n",
      "4135/4135 [==============================] - 0s 98us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 220/1000\n",
      "4135/4135 [==============================] - 0s 100us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 221/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 222/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 223/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 224/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 225/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 226/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 227/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 228/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 229/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 230/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 231/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 232/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 233/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 234/1000\n",
      "4135/4135 [==============================] - 1s 137us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 235/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 236/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 237/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 238/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 239/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 240/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 241/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 242/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 243/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 244/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 245/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 246/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 247/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 248/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0226\n",
      "Epoch 249/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 250/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 251/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 252/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 253/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 254/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 255/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 256/1000\n",
      "4135/4135 [==============================] - 1s 133us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 257/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 258/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 259/1000\n",
      "4135/4135 [==============================] - 1s 135us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 260/1000\n",
      "4135/4135 [==============================] - 1s 137us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 261/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 262/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 263/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 264/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 265/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 266/1000\n",
      "4135/4135 [==============================] - 1s 136us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 267/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 268/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 269/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 270/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 271/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 272/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 273/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 274/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 275/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 276/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 277/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 278/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 279/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0219 - val_loss: 0.0227\n",
      "Epoch 280/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 281/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 282/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 283/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 284/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 285/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 286/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 287/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 288/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 289/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 290/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 291/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 292/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 293/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 294/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 295/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 296/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 297/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 298/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 299/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 300/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00300: saving model to ./log_weights/Ide_AE_weights.0300.hdf5\n",
      "Epoch 301/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 302/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 303/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 304/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 305/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 306/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 307/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 308/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 309/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 310/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 311/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 312/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 313/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 314/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 315/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 316/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 317/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 318/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 319/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 320/1000\n",
      "4135/4135 [==============================] - 1s 133us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 321/1000\n",
      "4135/4135 [==============================] - 1s 144us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 322/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 323/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 324/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 325/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 326/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 327/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 328/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 329/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 330/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 331/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 332/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 333/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 334/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 335/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 336/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 337/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 338/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 339/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 340/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 341/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 342/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 343/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 344/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 345/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 346/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 347/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 348/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 349/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 350/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 351/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 352/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 353/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 354/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 355/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 356/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 357/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 358/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 359/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 360/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 361/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 362/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 363/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 364/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 365/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 366/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 367/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 368/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 369/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 370/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 371/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 372/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 373/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 374/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 375/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 376/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 377/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 378/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 379/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 380/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 381/1000\n",
      "4135/4135 [==============================] - 0s 101us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 382/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 383/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 384/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 385/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 386/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 387/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 388/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 389/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 390/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 391/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 392/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 393/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 394/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 395/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 396/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 397/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 398/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 399/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 400/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00400: saving model to ./log_weights/Ide_AE_weights.0400.hdf5\n",
      "Epoch 401/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 402/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 403/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 404/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 405/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0234\n",
      "Epoch 406/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 407/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 408/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 409/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 410/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 411/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 412/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 413/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 414/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 415/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 416/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 417/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 418/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 419/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 420/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 421/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 422/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 423/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 424/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 425/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 426/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 427/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 428/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 429/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 430/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 431/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 432/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 433/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 434/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 435/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 436/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 437/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 438/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 439/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 440/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 441/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 442/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 443/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 444/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 445/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 446/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 447/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 448/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 449/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0226\n",
      "Epoch 450/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 451/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 452/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 453/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 454/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 455/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 456/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 457/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 458/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 459/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 460/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 461/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 462/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 463/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 464/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 465/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 466/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 467/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 468/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 469/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 470/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 471/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 472/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 473/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 474/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 475/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 476/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 477/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 478/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 479/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 480/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 481/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 482/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 483/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0219 - val_loss: 0.0225\n",
      "Epoch 484/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 485/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 486/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 487/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 488/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 489/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 490/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 491/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 492/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 493/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 494/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 495/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 496/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 497/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 498/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 499/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 500/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00500: saving model to ./log_weights/Ide_AE_weights.0500.hdf5\n",
      "Epoch 501/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 502/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 503/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 504/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 505/1000\n",
      "4135/4135 [==============================] - 1s 133us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 506/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 507/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 508/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 509/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 510/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 511/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 512/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 513/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 514/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 515/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 516/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 517/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 518/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 519/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 520/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 521/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 522/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 523/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 524/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 525/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 526/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 527/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0226\n",
      "Epoch 528/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 529/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 530/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 531/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 532/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 533/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 534/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 535/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 536/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 537/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 538/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 539/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 540/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 541/1000\n",
      "4135/4135 [==============================] - 0s 101us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 542/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 543/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 544/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 545/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 546/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 547/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 548/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 549/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 550/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 551/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 552/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 553/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 554/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 555/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 556/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 557/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 558/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 559/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 560/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 561/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 562/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 563/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 564/1000\n",
      "4135/4135 [==============================] - 0s 103us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 565/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 566/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 567/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 568/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 569/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 570/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0230\n",
      "Epoch 571/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 572/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 573/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 574/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 575/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 576/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 577/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 578/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 579/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 580/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 581/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 582/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 583/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 584/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 585/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 586/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 587/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 588/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 589/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 590/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 591/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 592/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 593/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 594/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 595/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 596/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 597/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 598/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 599/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 600/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00600: saving model to ./log_weights/Ide_AE_weights.0600.hdf5\n",
      "Epoch 601/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 602/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 603/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 604/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 605/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 606/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 607/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 608/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 609/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 610/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 611/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 612/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 613/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 614/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 615/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 616/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 617/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 618/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 619/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 620/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 621/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 622/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 623/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 624/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 625/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 626/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 627/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 628/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 629/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 630/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 631/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 632/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 633/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 634/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 635/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 636/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 637/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 638/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 639/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 640/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 641/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 642/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 643/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 644/1000\n",
      "4135/4135 [==============================] - 0s 104us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 645/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 646/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 647/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 648/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 649/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 650/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 651/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 652/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 653/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 654/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 655/1000\n",
      "4135/4135 [==============================] - 0s 102us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 656/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 657/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 658/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 659/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 660/1000\n",
      "4135/4135 [==============================] - 1s 133us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 661/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 662/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 663/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 664/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 665/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 666/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 667/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 668/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 669/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 670/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 671/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 672/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 673/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 674/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 675/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 676/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 677/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 678/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 679/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 680/1000\n",
      "4135/4135 [==============================] - 1s 135us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 681/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 682/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 683/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 684/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 685/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 686/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 687/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 688/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 689/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 690/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 691/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 692/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 693/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 694/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 695/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 696/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 697/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 698/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 699/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 700/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "\n",
      "Epoch 00700: saving model to ./log_weights/Ide_AE_weights.0700.hdf5\n",
      "Epoch 701/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 702/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 703/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 704/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 705/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 706/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 707/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 708/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 709/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 710/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 711/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 712/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 713/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 714/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 715/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 716/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 717/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 718/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 719/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 720/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 721/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 722/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 723/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0228\n",
      "Epoch 724/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 725/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 726/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 727/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 728/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 729/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 730/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 731/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 732/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 733/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 734/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 735/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 736/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 737/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 738/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 739/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 740/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 741/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 742/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 743/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 744/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 745/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 746/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 747/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 748/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 749/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 750/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 751/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 752/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 753/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 754/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 755/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 756/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 757/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 758/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 759/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 760/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 761/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 762/1000\n",
      "4135/4135 [==============================] - 0s 106us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 763/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 764/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 765/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 766/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 767/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 768/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 769/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 770/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 771/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 772/1000\n",
      "4135/4135 [==============================] - 1s 139us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 773/1000\n",
      "4135/4135 [==============================] - 1s 138us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 774/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 775/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 776/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 777/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 778/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 779/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 780/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 781/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 782/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 783/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 784/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 785/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 786/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 787/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 788/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 789/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 790/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 791/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 792/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 793/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 794/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 795/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 796/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 797/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 798/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 799/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 800/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00800: saving model to ./log_weights/Ide_AE_weights.0800.hdf5\n",
      "Epoch 801/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 802/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 803/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 804/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 805/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 806/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 807/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 808/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 809/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 810/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 811/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 812/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 813/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 814/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 815/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 816/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 817/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 818/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 819/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 820/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 821/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 822/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 823/1000\n",
      "4135/4135 [==============================] - 1s 137us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 824/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 825/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 826/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 827/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 828/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 829/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0219 - val_loss: 0.0226\n",
      "Epoch 830/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 831/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 832/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 833/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 834/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 835/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 836/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 837/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 838/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 839/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 840/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 841/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 842/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 843/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 844/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 845/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 846/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 847/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 848/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 849/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 850/1000\n",
      "4135/4135 [==============================] - 1s 127us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 851/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 852/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 853/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 854/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 855/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 856/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 857/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 858/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 859/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 860/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 861/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 862/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 863/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 864/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 865/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 866/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 867/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 868/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 869/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 870/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 871/1000\n",
      "4135/4135 [==============================] - 0s 102us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 872/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0219\n",
      "Epoch 873/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 874/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 875/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 876/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 877/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 878/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 879/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 880/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 881/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 882/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 883/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 884/1000\n",
      "4135/4135 [==============================] - 0s 93us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 885/1000\n",
      "4135/4135 [==============================] - 0s 98us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 886/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 887/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 888/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 889/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 890/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 891/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 892/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 893/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 894/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 895/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 896/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 897/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 898/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0223\n",
      "Epoch 899/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 900/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "\n",
      "Epoch 00900: saving model to ./log_weights/Ide_AE_weights.0900.hdf5\n",
      "Epoch 901/1000\n",
      "4135/4135 [==============================] - 0s 115us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 902/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 903/1000\n",
      "4135/4135 [==============================] - 0s 105us/step - loss: 0.0218 - val_loss: 0.0227\n",
      "Epoch 904/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 905/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0219 - val_loss: 0.0232\n",
      "Epoch 906/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 907/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0219 - val_loss: 0.0220\n",
      "Epoch 908/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 909/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 910/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 911/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0225\n",
      "Epoch 912/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 913/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 914/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 915/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 916/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 917/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 918/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 919/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 920/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 921/1000\n",
      "4135/4135 [==============================] - 0s 117us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 922/1000\n",
      "4135/4135 [==============================] - 0s 118us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 923/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 924/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 925/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 926/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 927/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 928/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 929/1000\n",
      "4135/4135 [==============================] - 1s 134us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 930/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 931/1000\n",
      "4135/4135 [==============================] - 0s 103us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 932/1000\n",
      "4135/4135 [==============================] - 0s 102us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 933/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 934/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 935/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 936/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 937/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 938/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 939/1000\n",
      "4135/4135 [==============================] - 0s 112us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 940/1000\n",
      "4135/4135 [==============================] - 0s 114us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 941/1000\n",
      "4135/4135 [==============================] - 0s 113us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 942/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 943/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 944/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 945/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 946/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 947/1000\n",
      "4135/4135 [==============================] - 0s 109us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 948/1000\n",
      "4135/4135 [==============================] - 0s 116us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 949/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 950/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 951/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 952/1000\n",
      "4135/4135 [==============================] - 1s 125us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 953/1000\n",
      "4135/4135 [==============================] - 1s 133us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 954/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 955/1000\n",
      "4135/4135 [==============================] - 0s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 956/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 957/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 958/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 959/1000\n",
      "4135/4135 [==============================] - 1s 124us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 960/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 961/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 962/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 963/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0219 - val_loss: 0.0226\n",
      "Epoch 964/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 965/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 966/1000\n",
      "4135/4135 [==============================] - 1s 129us/step - loss: 0.0219 - val_loss: 0.0224\n",
      "Epoch 967/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 968/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 969/1000\n",
      "4135/4135 [==============================] - 1s 130us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 970/1000\n",
      "4135/4135 [==============================] - 1s 126us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 971/1000\n",
      "4135/4135 [==============================] - 1s 123us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 972/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 973/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 974/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 975/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 976/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 977/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 978/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 979/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 980/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 981/1000\n",
      "4135/4135 [==============================] - 1s 131us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 982/1000\n",
      "4135/4135 [==============================] - 1s 132us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 983/1000\n",
      "4135/4135 [==============================] - 1s 128us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 984/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 985/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 986/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0224\n",
      "Epoch 987/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 988/1000\n",
      "4135/4135 [==============================] - 0s 119us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 989/1000\n",
      "4135/4135 [==============================] - 0s 120us/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 990/1000\n",
      "4135/4135 [==============================] - 1s 122us/step - loss: 0.0218 - val_loss: 0.0223\n",
      "Epoch 991/1000\n",
      "4135/4135 [==============================] - 1s 121us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 992/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 993/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 994/1000\n",
      "4135/4135 [==============================] - 0s 111us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 995/1000\n",
      "4135/4135 [==============================] - 0s 107us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 996/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 997/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "Epoch 998/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0219 - val_loss: 0.0221\n",
      "Epoch 999/1000\n",
      "4135/4135 [==============================] - 0s 108us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 1000/1000\n",
      "4135/4135 [==============================] - 0s 110us/step - loss: 0.0218 - val_loss: 0.0221\n",
      "\n",
      "Epoch 01000: saving model to ./log_weights/Ide_AE_weights.1000.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/Ide_AE_weights.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(Ide_AE.layers[1].get_weights()))\n",
    "\n",
    "Ide_AE_history = Ide_AE.fit(x_train, x_train,\\\n",
    "                            epochs=epochs_number,\\\n",
    "                            batch_size=batch_size_value,\\\n",
    "                            shuffle=True,\\\n",
    "                            validation_data=(x_validate,x_validate),\\\n",
    "                            callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = Ide_AE_history.history['loss']\n",
    "val_loss = Ide_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.021583883441267906\n"
     ]
    }
   ],
   "source": [
    "p_data=Ide_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "key_number=50\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_number=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_features=F.top_k_keepWeights_1(Ide_AE.get_layer(index=1).get_weights()[0],key_number)\n",
    "\n",
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./Defined/Functions.py:196: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(p_train_feature, p_train_label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9251523063533508\n",
      "Testing accuracy： 0.9251523063533508\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4595, 4)\n",
      "(1149, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./Defined/Functions.py:196: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(p_train_feature, p_train_label)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.5613577023498695\n",
      "Testing accuracy： 0.5613577023498695\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)\n",
    "print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4595, 4)\n",
      "(1149, 4)\n",
      "0.010118838995898757\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
