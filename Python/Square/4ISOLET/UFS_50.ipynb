{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frame=np.array(pd.read_csv('./Dataset/isolet1+2+3+4.data',header=None))\n",
    "test_data_frame=np.array(pd.read_csv('./Dataset/isolet5.data',header=None))\n",
    "\n",
    "train_data_arr=(train_data_frame[:,0:617]).copy()\n",
    "train_label_arr=((train_data_frame[:,617]).copy()-1)\n",
    "test_data_arr=(test_data_frame[:,0:617]).copy()\n",
    "test_label_arr=((test_data_frame[:,617]).copy()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6238, 617)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1559, 617)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7797, 617)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.r_[train_data_arr,test_data_arr].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=MinMaxScaler(feature_range=(0,1)).fit_transform(np.r_[train_data_arr,test_data_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7797, 617)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train_x=Data[:len(train_data_arr)]\n",
    "C_test_x=Data[len(train_data_arr):]\n",
    "C_train_y=train_label_arr#to_categorical(train_label_arr)\n",
    "C_test_y=test_label_arr#to_categorical(test_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (5614, 617)\n",
      "Shape of x_validate: (624, 617)\n",
      "Shape of x_test: (1559, 617)\n",
      "Shape of y_train: (5614,)\n",
      "Shape of y_validate: (624,)\n",
      "Shape of y_test: (1559,)\n",
      "Shape of C_train_x: (6238, 617)\n",
      "Shape of C_train_y: (6238,)\n",
      "Shape of C_test_x: (1559, 617)\n",
      "Shape of C_test_y: (1559,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "x_test=C_test_x\n",
    "y_test_onehot=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train_onehot.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate_onehot.shape))\n",
    "print('Shape of y_test: ' + str(y_test_onehot.shape))\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0.999999, maxval=0.9999999, seed=seed),\n",
    "                                      trainable=True)\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=key_feture_number):\n",
    "        kernel=K.pow(self.kernel,2)       \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                p_encoding_dim=key_feture_number,\\\n",
    "                p_learning_rate= 1E-3):\n",
    "    input_img = Input(shape=(p_data_feature,), name='input_img')\n",
    "\n",
    "    encoded = Dense(p_encoding_dim, activation='linear',kernel_initializer=initializers.glorot_uniform(seed))(input_img)\n",
    "    bottleneck=encoded\n",
    "    decoded = Dense(p_data_feature, activation='linear',kernel_initializer=initializers.glorot_uniform(seed))(encoded)\n",
    "\n",
    "    latent_encoder = Model(input_img, bottleneck)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error', optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    #print('Latent Encoder Structure-------------------------------------')\n",
    "    #latent_encoder.summary()\n",
    "    return autoencoder,latent_encoder\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=key_feture_number,\\\n",
    "                         p_learning_rate= 1E-3):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Fractal_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                        p_feture_number=key_feture_number,\\\n",
    "                        p_encoding_dim=key_feture_number,\\\n",
    "                        p_learning_rate=1E-3,\\\n",
    "                        p_loss_weight_1=1,\\\n",
    "                        p_loss_weight_2=2):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "    feature_selection_choose=feature_selection(input_img,selection=True,k=p_feture_number)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    encoded_choose=encoded(feature_selection_choose)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    bottleneck_choose=encoded_choose\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "    decoded_choose =decoded(bottleneck_choose)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    latent_encoder_choose = Model(input_img, bottleneck_choose)\n",
    "    feature_selection_output=Model(input_img,feature_selection_choose)\n",
    "    autoencoder = Model(input_img, [decoded_score,decoded_choose])\n",
    "    \n",
    "    autoencoder.compile(loss=['mean_squared_error','mean_squared_error'],\\\n",
    "                        loss_weights=[p_loss_weight_1, p_loss_weight_2],\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,feature_selection_output,latent_encoder_score,latent_encoder_choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Structure and paramter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=200\n",
    "batch_size_value=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.1 Fractal Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-11-478f6b761227>:22: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "autoencoder_input (InputLayer)  (None, 617)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_selection (Feature_Sele (None, 617)          617         autoencoder_input[0][0]          \n",
      "                                                                 autoencoder_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "autoencoder_hidden_layer (Dense (None, 50)           30900       feature_selection[0][0]          \n",
      "                                                                 feature_selection[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "autoencoder_output (Dense)      (None, 617)          31467       autoencoder_hidden_layer[0][0]   \n",
      "                                                                 autoencoder_hidden_layer[1][0]   \n",
      "==================================================================================================\n",
      "Total params: 62,984\n",
      "Trainable params: 62,984\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss_weight_1=0.0078125\n",
    "F_AE,\\\n",
    "feature_selection_output,\\\n",
    "latent_encoder_score_F_AE,\\\n",
    "latent_encoder_choose_F_AE=Fractal_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                               p_feture_number=key_feture_number,\\\n",
    "                                               p_encoding_dim=key_feture_number,\\\n",
    "                                               p_learning_rate= 1E-3,\\\n",
    "                                               p_loss_weight_1=loss_weight_1,\\\n",
    "                                               p_loss_weight_2=1)\n",
    "\n",
    "#file_name=\"./log/F_AE_\"+str(key_feture_number)+\".png\"\n",
    "#plot_model(F_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 5614 samples, validate on 624 samples\n",
      "Epoch 1/200\n",
      "5614/5614 [==============================] - 1s 258us/step - loss: 0.0985 - autoencoder_output_loss: 0.0976 - val_loss: 0.0458 - val_autoencoder_output_loss: 0.0454\n",
      "Epoch 2/200\n",
      "5614/5614 [==============================] - 1s 129us/step - loss: 0.0396 - autoencoder_output_loss: 0.0392 - val_loss: 0.0367 - val_autoencoder_output_loss: 0.0364\n",
      "Epoch 3/200\n",
      "5614/5614 [==============================] - 1s 139us/step - loss: 0.0345 - autoencoder_output_loss: 0.0343 - val_loss: 0.0337 - val_autoencoder_output_loss: 0.0334\n",
      "Epoch 4/200\n",
      "5614/5614 [==============================] - 1s 151us/step - loss: 0.0316 - autoencoder_output_loss: 0.0314 - val_loss: 0.0303 - val_autoencoder_output_loss: 0.0300\n",
      "Epoch 5/200\n",
      "5614/5614 [==============================] - 1s 180us/step - loss: 0.0287 - autoencoder_output_loss: 0.0285 - val_loss: 0.0277 - val_autoencoder_output_loss: 0.0275\n",
      "Epoch 6/200\n",
      "5614/5614 [==============================] - 1s 153us/step - loss: 0.0267 - autoencoder_output_loss: 0.0265 - val_loss: 0.0261 - val_autoencoder_output_loss: 0.0259\n",
      "Epoch 7/200\n",
      "5614/5614 [==============================] - 1s 161us/step - loss: 0.0253 - autoencoder_output_loss: 0.0252 - val_loss: 0.0249 - val_autoencoder_output_loss: 0.0247\n",
      "Epoch 8/200\n",
      "5614/5614 [==============================] - 1s 127us/step - loss: 0.0243 - autoencoder_output_loss: 0.0241 - val_loss: 0.0240 - val_autoencoder_output_loss: 0.0238\n",
      "Epoch 9/200\n",
      "5614/5614 [==============================] - 1s 201us/step - loss: 0.0235 - autoencoder_output_loss: 0.0233 - val_loss: 0.0233 - val_autoencoder_output_loss: 0.0231\n",
      "Epoch 10/200\n",
      "5614/5614 [==============================] - 1s 138us/step - loss: 0.0228 - autoencoder_output_loss: 0.0227 - val_loss: 0.0228 - val_autoencoder_output_loss: 0.0226\n",
      "Epoch 11/200\n",
      "5614/5614 [==============================] - 1s 203us/step - loss: 0.0223 - autoencoder_output_loss: 0.0221 - val_loss: 0.0223 - val_autoencoder_output_loss: 0.0222\n",
      "Epoch 12/200\n",
      "5614/5614 [==============================] - 1s 151us/step - loss: 0.0219 - autoencoder_output_loss: 0.0217 - val_loss: 0.0219 - val_autoencoder_output_loss: 0.0218\n",
      "Epoch 13/200\n",
      "5614/5614 [==============================] - 1s 141us/step - loss: 0.0215 - autoencoder_output_loss: 0.0213 - val_loss: 0.0216 - val_autoencoder_output_loss: 0.0214\n",
      "Epoch 14/200\n",
      "5614/5614 [==============================] - 1s 237us/step - loss: 0.0211 - autoencoder_output_loss: 0.0210 - val_loss: 0.0213 - val_autoencoder_output_loss: 0.0211\n",
      "Epoch 15/200\n",
      "5614/5614 [==============================] - 1s 185us/step - loss: 0.0209 - autoencoder_output_loss: 0.0207 - val_loss: 0.0211 - val_autoencoder_output_loss: 0.0209\n",
      "Epoch 16/200\n",
      "5614/5614 [==============================] - 1s 144us/step - loss: 0.0206 - autoencoder_output_loss: 0.0205 - val_loss: 0.0208 - val_autoencoder_output_loss: 0.0206\n",
      "Epoch 17/200\n",
      "5614/5614 [==============================] - 1s 205us/step - loss: 0.0204 - autoencoder_output_loss: 0.0202 - val_loss: 0.0205 - val_autoencoder_output_loss: 0.0204\n",
      "Epoch 18/200\n",
      "5614/5614 [==============================] - 1s 191us/step - loss: 0.0203 - autoencoder_output_loss: 0.0202 - val_loss: 0.0211 - val_autoencoder_output_loss: 0.0209\n",
      "Epoch 19/200\n",
      "5614/5614 [==============================] - 1s 139us/step - loss: 0.0205 - autoencoder_output_loss: 0.0203 - val_loss: 0.0208 - val_autoencoder_output_loss: 0.0207\n",
      "Epoch 20/200\n",
      "5614/5614 [==============================] - 1s 197us/step - loss: 0.0201 - autoencoder_output_loss: 0.0200 - val_loss: 0.0205 - val_autoencoder_output_loss: 0.0204\n",
      "Epoch 21/200\n",
      "5614/5614 [==============================] - 1s 142us/step - loss: 0.0200 - autoencoder_output_loss: 0.0198 - val_loss: 0.0201 - val_autoencoder_output_loss: 0.0199\n",
      "Epoch 22/200\n",
      "5614/5614 [==============================] - 1s 140us/step - loss: 0.0198 - autoencoder_output_loss: 0.0197 - val_loss: 0.0202 - val_autoencoder_output_loss: 0.0200\n",
      "Epoch 23/200\n",
      "5614/5614 [==============================] - 1s 128us/step - loss: 0.0196 - autoencoder_output_loss: 0.0194 - val_loss: 0.0197 - val_autoencoder_output_loss: 0.0195\n",
      "Epoch 24/200\n",
      "5614/5614 [==============================] - 1s 131us/step - loss: 0.0193 - autoencoder_output_loss: 0.0192 - val_loss: 0.0196 - val_autoencoder_output_loss: 0.0195\n",
      "Epoch 25/200\n",
      "5614/5614 [==============================] - 1s 158us/step - loss: 0.0192 - autoencoder_output_loss: 0.0191 - val_loss: 0.0195 - val_autoencoder_output_loss: 0.0194\n",
      "Epoch 26/200\n",
      "5614/5614 [==============================] - 1s 157us/step - loss: 0.0191 - autoencoder_output_loss: 0.0190 - val_loss: 0.0194 - val_autoencoder_output_loss: 0.0193\n",
      "Epoch 27/200\n",
      "5614/5614 [==============================] - 1s 198us/step - loss: 0.0190 - autoencoder_output_loss: 0.0189 - val_loss: 0.0194 - val_autoencoder_output_loss: 0.0192\n",
      "Epoch 28/200\n",
      "5614/5614 [==============================] - 1s 188us/step - loss: 0.0189 - autoencoder_output_loss: 0.0188 - val_loss: 0.0192 - val_autoencoder_output_loss: 0.0191\n",
      "Epoch 29/200\n",
      "5614/5614 [==============================] - 1s 167us/step - loss: 0.0189 - autoencoder_output_loss: 0.0187 - val_loss: 0.0193 - val_autoencoder_output_loss: 0.0192\n",
      "Epoch 30/200\n",
      "5614/5614 [==============================] - 1s 126us/step - loss: 0.0188 - autoencoder_output_loss: 0.0187 - val_loss: 0.0191 - val_autoencoder_output_loss: 0.0190\n",
      "Epoch 31/200\n",
      "5614/5614 [==============================] - 1s 130us/step - loss: 0.0187 - autoencoder_output_loss: 0.0186 - val_loss: 0.0190 - val_autoencoder_output_loss: 0.0189\n",
      "Epoch 32/200\n",
      "5614/5614 [==============================] - 1s 148us/step - loss: 0.0186 - autoencoder_output_loss: 0.0185 - val_loss: 0.0190 - val_autoencoder_output_loss: 0.0188\n",
      "Epoch 33/200\n",
      "5614/5614 [==============================] - 1s 198us/step - loss: 0.0186 - autoencoder_output_loss: 0.0185 - val_loss: 0.0189 - val_autoencoder_output_loss: 0.0187\n",
      "Epoch 34/200\n",
      "5614/5614 [==============================] - 1s 202us/step - loss: 0.0185 - autoencoder_output_loss: 0.0184 - val_loss: 0.0188 - val_autoencoder_output_loss: 0.0187\n",
      "Epoch 35/200\n",
      "5614/5614 [==============================] - 1s 199us/step - loss: 0.0184 - autoencoder_output_loss: 0.0183 - val_loss: 0.0187 - val_autoencoder_output_loss: 0.0186\n",
      "Epoch 36/200\n",
      "5614/5614 [==============================] - 1s 163us/step - loss: 0.0184 - autoencoder_output_loss: 0.0183 - val_loss: 0.0188 - val_autoencoder_output_loss: 0.0187\n",
      "Epoch 37/200\n",
      "5614/5614 [==============================] - 1s 194us/step - loss: 0.0184 - autoencoder_output_loss: 0.0182 - val_loss: 0.0186 - val_autoencoder_output_loss: 0.0185\n",
      "Epoch 38/200\n",
      "5614/5614 [==============================] - 1s 187us/step - loss: 0.0183 - autoencoder_output_loss: 0.0182 - val_loss: 0.0186 - val_autoencoder_output_loss: 0.0185\n",
      "Epoch 39/200\n",
      "5614/5614 [==============================] - 1s 214us/step - loss: 0.0183 - autoencoder_output_loss: 0.0181 - val_loss: 0.0186 - val_autoencoder_output_loss: 0.0185\n",
      "Epoch 40/200\n",
      "5614/5614 [==============================] - 1s 223us/step - loss: 0.0182 - autoencoder_output_loss: 0.0181 - val_loss: 0.0185 - val_autoencoder_output_loss: 0.0184\n",
      "Epoch 41/200\n",
      "5614/5614 [==============================] - 2s 287us/step - loss: 0.0182 - autoencoder_output_loss: 0.0181 - val_loss: 0.0185 - val_autoencoder_output_loss: 0.0184\n",
      "Epoch 42/200\n",
      "5614/5614 [==============================] - 2s 306us/step - loss: 0.0181 - autoencoder_output_loss: 0.0180 - val_loss: 0.0185 - val_autoencoder_output_loss: 0.0183\n",
      "Epoch 43/200\n",
      "5614/5614 [==============================] - 1s 155us/step - loss: 0.0181 - autoencoder_output_loss: 0.0180 - val_loss: 0.0184 - val_autoencoder_output_loss: 0.0183\n",
      "Epoch 44/200\n",
      "5614/5614 [==============================] - 1s 181us/step - loss: 0.0181 - autoencoder_output_loss: 0.0180 - val_loss: 0.0184 - val_autoencoder_output_loss: 0.0183\n",
      "Epoch 45/200\n",
      "5614/5614 [==============================] - 1s 194us/step - loss: 0.0180 - autoencoder_output_loss: 0.0179 - val_loss: 0.0184 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 46/200\n",
      "5614/5614 [==============================] - 1s 215us/step - loss: 0.0180 - autoencoder_output_loss: 0.0179 - val_loss: 0.0184 - val_autoencoder_output_loss: 0.0183\n",
      "Epoch 47/200\n",
      "5614/5614 [==============================] - 1s 193us/step - loss: 0.0180 - autoencoder_output_loss: 0.0179 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 48/200\n",
      "5614/5614 [==============================] - 1s 187us/step - loss: 0.0180 - autoencoder_output_loss: 0.0179 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 49/200\n",
      "5614/5614 [==============================] - 1s 222us/step - loss: 0.0179 - autoencoder_output_loss: 0.0178 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 50/200\n",
      "5614/5614 [==============================] - 1s 194us/step - loss: 0.0179 - autoencoder_output_loss: 0.0178 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 51/200\n",
      "5614/5614 [==============================] - 1s 158us/step - loss: 0.0179 - autoencoder_output_loss: 0.0178 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 52/200\n",
      "5614/5614 [==============================] - 1s 149us/step - loss: 0.0179 - autoencoder_output_loss: 0.0178 - val_loss: 0.0182 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 53/200\n",
      "5614/5614 [==============================] - 1s 133us/step - loss: 0.0179 - autoencoder_output_loss: 0.0178 - val_loss: 0.0182 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 54/200\n",
      "5614/5614 [==============================] - 1s 134us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0182 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 55/200\n",
      "5614/5614 [==============================] - 1s 152us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 56/200\n",
      "5614/5614 [==============================] - 1s 141us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0182 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 57/200\n",
      "5614/5614 [==============================] - 1s 131us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0182 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 58/200\n",
      "5614/5614 [==============================] - 1s 152us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0182 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 59/200\n",
      "5614/5614 [==============================] - 1s 191us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0181 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 60/200\n",
      "5614/5614 [==============================] - 1s 165us/step - loss: 0.0180 - autoencoder_output_loss: 0.0179 - val_loss: 0.0181 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 61/200\n",
      "5614/5614 [==============================] - 1s 136us/step - loss: 0.0177 - autoencoder_output_loss: 0.0176 - val_loss: 0.0181 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 62/200\n",
      "5614/5614 [==============================] - 1s 153us/step - loss: 0.0177 - autoencoder_output_loss: 0.0176 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 63/200\n",
      "5614/5614 [==============================] - 1s 143us/step - loss: 0.0177 - autoencoder_output_loss: 0.0176 - val_loss: 0.0181 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 64/200\n",
      "5614/5614 [==============================] - 1s 195us/step - loss: 0.0177 - autoencoder_output_loss: 0.0176 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 65/200\n",
      "5614/5614 [==============================] - 1s 146us/step - loss: 0.0177 - autoencoder_output_loss: 0.0175 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 66/200\n",
      "5614/5614 [==============================] - 1s 224us/step - loss: 0.0177 - autoencoder_output_loss: 0.0176 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 67/200\n",
      "5614/5614 [==============================] - 1s 160us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 68/200\n",
      "5614/5614 [==============================] - 1s 133us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0181 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 69/200\n",
      "5614/5614 [==============================] - 1s 126us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 70/200\n",
      "5614/5614 [==============================] - 1s 125us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 71/200\n",
      "5614/5614 [==============================] - 1s 124us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 72/200\n",
      "5614/5614 [==============================] - 1s 125us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 73/200\n",
      "5614/5614 [==============================] - 1s 150us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 74/200\n",
      "5614/5614 [==============================] - 1s 187us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 75/200\n",
      "5614/5614 [==============================] - 1s 173us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 76/200\n",
      "5614/5614 [==============================] - 1s 164us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 77/200\n",
      "5614/5614 [==============================] - 1s 131us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 78/200\n",
      "5614/5614 [==============================] - 1s 133us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 79/200\n",
      "5614/5614 [==============================] - 1s 168us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 80/200\n",
      "5614/5614 [==============================] - 1s 130us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 81/200\n",
      "5614/5614 [==============================] - 1s 133us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 82/200\n",
      "5614/5614 [==============================] - 1s 134us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 83/200\n",
      "5614/5614 [==============================] - 1s 173us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 84/200\n",
      "5614/5614 [==============================] - 1s 123us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 85/200\n",
      "5614/5614 [==============================] - 1s 141us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 86/200\n",
      "5614/5614 [==============================] - 1s 135us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 87/200\n",
      "5614/5614 [==============================] - 1s 148us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 88/200\n",
      "5614/5614 [==============================] - 1s 208us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 89/200\n",
      "5614/5614 [==============================] - 1s 137us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 90/200\n",
      "5614/5614 [==============================] - 1s 142us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 91/200\n",
      "5614/5614 [==============================] - 1s 177us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 92/200\n",
      "5614/5614 [==============================] - 1s 129us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 93/200\n",
      "5614/5614 [==============================] - 1s 131us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 94/200\n",
      "5614/5614 [==============================] - 1s 168us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 95/200\n",
      "5614/5614 [==============================] - 1s 174us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0178\n",
      "Epoch 96/200\n",
      "5614/5614 [==============================] - 1s 129us/step - loss: 0.0178 - autoencoder_output_loss: 0.0177 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 97/200\n",
      "5614/5614 [==============================] - 1s 153us/step - loss: 0.0181 - autoencoder_output_loss: 0.0180 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0182\n",
      "Epoch 98/200\n",
      "5614/5614 [==============================] - 1s 191us/step - loss: 0.0182 - autoencoder_output_loss: 0.0181 - val_loss: 0.0181 - val_autoencoder_output_loss: 0.0180\n",
      "Epoch 99/200\n",
      "5614/5614 [==============================] - 1s 141us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 100/200\n",
      "5614/5614 [==============================] - 1s 151us/step - loss: 0.0173 - autoencoder_output_loss: 0.0172 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/F_AE_50_weights_0.0078125.0100.hdf5\n",
      "Epoch 101/200\n",
      "5614/5614 [==============================] - 1s 147us/step - loss: 0.0173 - autoencoder_output_loss: 0.0172 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 102/200\n",
      "5614/5614 [==============================] - 1s 202us/step - loss: 0.0173 - autoencoder_output_loss: 0.0172 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 103/200\n",
      "5614/5614 [==============================] - 1s 147us/step - loss: 0.0173 - autoencoder_output_loss: 0.0172 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 104/200\n",
      "5614/5614 [==============================] - 1s 128us/step - loss: 0.0173 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 105/200\n",
      "5614/5614 [==============================] - 1s 126us/step - loss: 0.0173 - autoencoder_output_loss: 0.0172 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 106/200\n",
      "5614/5614 [==============================] - 1s 152us/step - loss: 0.0173 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 107/200\n",
      "5614/5614 [==============================] - 1s 152us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 108/200\n",
      "5614/5614 [==============================] - 1s 186us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 109/200\n",
      "5614/5614 [==============================] - 1s 208us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 110/200\n",
      "5614/5614 [==============================] - 1s 167us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 111/200\n",
      "5614/5614 [==============================] - 1s 194us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 112/200\n",
      "5614/5614 [==============================] - 1s 206us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 113/200\n",
      "5614/5614 [==============================] - 1s 221us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 114/200\n",
      "5614/5614 [==============================] - 2s 282us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 115/200\n",
      "5614/5614 [==============================] - 1s 231us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 116/200\n",
      "5614/5614 [==============================] - 1s 221us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 117/200\n",
      "5614/5614 [==============================] - 1s 173us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 118/200\n",
      "5614/5614 [==============================] - 1s 199us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 119/200\n",
      "5614/5614 [==============================] - 1s 189us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 120/200\n",
      "5614/5614 [==============================] - 1s 220us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 121/200\n",
      "5614/5614 [==============================] - 1s 160us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 122/200\n",
      "5614/5614 [==============================] - 1s 160us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 123/200\n",
      "5614/5614 [==============================] - 1s 207us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 124/200\n",
      "5614/5614 [==============================] - 1s 148us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 125/200\n",
      "5614/5614 [==============================] - 1s 154us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 126/200\n",
      "5614/5614 [==============================] - 1s 184us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 127/200\n",
      "5614/5614 [==============================] - 1s 171us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0176 - val_autoencoder_output_loss: 0.0175\n",
      "Epoch 128/200\n",
      "5614/5614 [==============================] - 1s 195us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 129/200\n",
      "5614/5614 [==============================] - 1s 162us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 130/200\n",
      "5614/5614 [==============================] - 1s 182us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 131/200\n",
      "5614/5614 [==============================] - 1s 201us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 132/200\n",
      "5614/5614 [==============================] - 1s 157us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 133/200\n",
      "5614/5614 [==============================] - 1s 151us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 134/200\n",
      "5614/5614 [==============================] - 1s 186us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 135/200\n",
      "5614/5614 [==============================] - 1s 151us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 136/200\n",
      "5614/5614 [==============================] - 1s 155us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 137/200\n",
      "5614/5614 [==============================] - 1s 170us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 138/200\n",
      "5614/5614 [==============================] - 1s 167us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 139/200\n",
      "5614/5614 [==============================] - 1s 202us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 140/200\n",
      "5614/5614 [==============================] - 1s 217us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 141/200\n",
      "5614/5614 [==============================] - 1s 159us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 142/200\n",
      "5614/5614 [==============================] - 1s 181us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 143/200\n",
      "5614/5614 [==============================] - 1s 183us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 144/200\n",
      "5614/5614 [==============================] - 1s 167us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 145/200\n",
      "5614/5614 [==============================] - 1s 186us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 146/200\n",
      "5614/5614 [==============================] - 1s 148us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 147/200\n",
      "5614/5614 [==============================] - 1s 220us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 148/200\n",
      "5614/5614 [==============================] - 1s 229us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 149/200\n",
      "5614/5614 [==============================] - 1s 191us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 150/200\n",
      "5614/5614 [==============================] - 1s 174us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 151/200\n",
      "5614/5614 [==============================] - 1s 228us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 152/200\n",
      "5614/5614 [==============================] - 1s 187us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 153/200\n",
      "5614/5614 [==============================] - 1s 167us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 154/200\n",
      "5614/5614 [==============================] - 1s 173us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 155/200\n",
      "5614/5614 [==============================] - 1s 164us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 156/200\n",
      "5614/5614 [==============================] - 1s 190us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 157/200\n",
      "5614/5614 [==============================] - 1s 241us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 158/200\n",
      "5614/5614 [==============================] - 1s 205us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 159/200\n",
      "5614/5614 [==============================] - 1s 174us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 160/200\n",
      "5614/5614 [==============================] - 1s 201us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 161/200\n",
      "5614/5614 [==============================] - 1s 193us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 162/200\n",
      "5614/5614 [==============================] - 1s 249us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 163/200\n",
      "5614/5614 [==============================] - 1s 207us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 164/200\n",
      "5614/5614 [==============================] - 1s 227us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 165/200\n",
      "5614/5614 [==============================] - 1s 154us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 166/200\n",
      "5614/5614 [==============================] - 1s 192us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 167/200\n",
      "5614/5614 [==============================] - 1s 246us/step - loss: 0.0172 - autoencoder_output_loss: 0.0171 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 168/200\n",
      "5614/5614 [==============================] - 1s 202us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 169/200\n",
      "5614/5614 [==============================] - 1s 225us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 170/200\n",
      "5614/5614 [==============================] - 1s 256us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 171/200\n",
      "5614/5614 [==============================] - 1s 232us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 172/200\n",
      "5614/5614 [==============================] - 1s 215us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 173/200\n",
      "5614/5614 [==============================] - 1s 222us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 174/200\n",
      "5614/5614 [==============================] - 1s 245us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 175/200\n",
      "5614/5614 [==============================] - 1s 239us/step - loss: 0.0171 - autoencoder_output_loss: 0.0170 - val_loss: 0.0180 - val_autoencoder_output_loss: 0.0179\n",
      "Epoch 176/200\n",
      "5614/5614 [==============================] - 1s 199us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0179 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 177/200\n",
      "5614/5614 [==============================] - 1s 235us/step - loss: 0.0176 - autoencoder_output_loss: 0.0175 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 178/200\n",
      "5614/5614 [==============================] - 1s 223us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 179/200\n",
      "5614/5614 [==============================] - 1s 185us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 180/200\n",
      "5614/5614 [==============================] - 1s 188us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 181/200\n",
      "5614/5614 [==============================] - 1s 217us/step - loss: 0.0175 - autoencoder_output_loss: 0.0174 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 182/200\n",
      "5614/5614 [==============================] - 1s 257us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 183/200\n",
      "5614/5614 [==============================] - 1s 198us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 184/200\n",
      "5614/5614 [==============================] - 1s 199us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 185/200\n",
      "5614/5614 [==============================] - 2s 286us/step - loss: 0.0175 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 186/200\n",
      "5614/5614 [==============================] - 1s 266us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 187/200\n",
      "5614/5614 [==============================] - 2s 278us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 188/200\n",
      "5614/5614 [==============================] - 2s 274us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 189/200\n",
      "5614/5614 [==============================] - 2s 268us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 190/200\n",
      "5614/5614 [==============================] - 1s 265us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 191/200\n",
      "5614/5614 [==============================] - 1s 260us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 192/200\n",
      "5614/5614 [==============================] - 1s 259us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 193/200\n",
      "5614/5614 [==============================] - 1s 265us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 194/200\n",
      "5614/5614 [==============================] - 1s 266us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 195/200\n",
      "5614/5614 [==============================] - 1s 261us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 196/200\n",
      "5614/5614 [==============================] - 2s 293us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 197/200\n",
      "5614/5614 [==============================] - 2s 278us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0177\n",
      "Epoch 198/200\n",
      "5614/5614 [==============================] - 1s 236us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0178 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 199/200\n",
      "5614/5614 [==============================] - 2s 272us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "Epoch 200/200\n",
      "5614/5614 [==============================] - 2s 268us/step - loss: 0.0174 - autoencoder_output_loss: 0.0173 - val_loss: 0.0177 - val_autoencoder_output_loss: 0.0176\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/F_AE_50_weights_0.0078125.0200.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/F_AE_'+str(key_feture_number)+'_weights_'+str(loss_weight_1)+'.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(F_AE.layers[1].get_weights()))\n",
    "\n",
    "F_AE_history = F_AE.fit(x_train, [x_train,x_train],\\\n",
    "                        epochs=epochs_number,\\\n",
    "                        batch_size=batch_size_value,\\\n",
    "                        shuffle=True,\\\n",
    "                        validation_data=(x_validate, [x_validate,x_validate]),\\\n",
    "                        callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = F_AE_history.history['loss']\n",
    "val_loss = F_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs[250:], loss[250:], 'bo', label='Training Loss')\n",
    "plt.plot(epochs[250:], val_loss[250:], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.013655707391002414\n",
      "MSE for feature selection layer 0.017671113262337328\n"
     ]
    }
   ],
   "source": [
    "p_data=F_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)[0]-x_test,2))/numbers)\n",
    "print(\"MSE for feature selection layer\",np.sum(np.power(np.array(p_data)[1]-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.2 Feature selection layer output\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "FS_layer_output=feature_selection_output.predict(x_test)\n",
    "print(np.sum(FS_layer_output[0]>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.3 Key features show\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617\n"
     ]
    }
   ],
   "source": [
    "key_features=F.top_k_keepWeights_1(F_AE.get_layer(index=1).get_weights()[0],key_feture_number)\n",
    "print(np.sum(F_AE.get_layer(index=1).get_weights()[0]>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9448364336112893\n",
      "Testing accuracy： 0.9448364336112893\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4.1.1. On Identity Selection layer\n",
    "---\n",
    "\n",
    "a) with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feature>0:  44\n",
      "(6238, 617)\n",
      "test_feature>0:  44\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7716484926234766\n",
      "Testing accuracy： 0.7716484926234766\n"
     ]
    }
   ],
   "source": [
    "train_feature=feature_selection_output.predict(C_train_x)\n",
    "print(\"train_feature>0: \",np.sum(train_feature[0]>0))\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "test_feature=feature_selection_output.predict(C_test_x)\n",
    "print(\"test_feature>0: \",np.sum(test_feature[0]>0))\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "b) Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 617)\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7716484926234766\n",
      "Testing accuracy： 0.7716484926234766\n"
     ]
    }
   ],
   "source": [
    "train_feature=feature_selection_output.predict(C_train_x)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature=feature_selection_output.predict(C_test_x)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "train_feature_sparse=sparse.coo_matrix(train_feature)\n",
    "test_feature_sparse=sparse.coo_matrix(test_feature)\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature_sparse,train_label,test_feature_sparse,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "c) Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7613855035279025\n",
      "Testing accuracy： 0.7613855035279025\n"
     ]
    }
   ],
   "source": [
    "train_feature_=feature_selection_output.predict(C_train_x)\n",
    "train_feature=F.compress_zero(train_feature_,key_feture_number)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=feature_selection_output.predict(C_test_x)\n",
    "test_feature=F.compress_zero(test_feature_,key_feture_number)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "d) Compression with structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8236048749198204\n",
      "Testing accuracy： 0.8236048749198204\n"
     ]
    }
   ],
   "source": [
    "train_feature_=feature_selection_output.predict(C_train_x)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=feature_selection_output.predict(C_test_x)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4.1.2. On Original Selection\n",
    "---\n",
    "\n",
    "a) with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feature>0:  44\n",
      "(6238, 617)\n",
      "test_feature>0:  44\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7716484926234766\n",
      "Testing accuracy： 0.7716484926234766\n"
     ]
    }
   ],
   "source": [
    "train_feature=np.multiply(C_train_x, key_features)\n",
    "print(\"train_feature>0: \",np.sum(train_feature[0]>0))\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature=np.multiply(C_test_x, key_features)\n",
    "print(\"test_feature>0: \",np.sum(test_feature[0]>0))\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "b) Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 617)\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7716484926234766\n",
      "Testing accuracy： 0.7716484926234766\n"
     ]
    }
   ],
   "source": [
    "train_feature=np.multiply(C_train_x, key_features)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature=np.multiply(C_test_x, key_features)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "train_feature_sparse=sparse.coo_matrix(train_feature)\n",
    "test_feature_sparse=sparse.coo_matrix(test_feature)\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature_sparse,train_label,test_feature_sparse,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "c) Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7491982039769083\n",
      "Testing accuracy： 0.7491982039769083\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero(train_feature_,key_feture_number)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero(test_feature_,key_feture_number)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "d) Compression with structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8236048749198204\n",
      "Testing accuracy： 0.8236048749198204\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4.1.3. Latent space\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8640153944836434\n",
      "Testing accuracy： 0.8640153944836434\n"
     ]
    }
   ],
   "source": [
    "train_feature=latent_encoder_score_F_AE.predict(C_train_x)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "test_feature=latent_encoder_score_F_AE.predict(C_test_x)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7703656189865298\n",
      "Testing accuracy： 0.7703656189865298\n"
     ]
    }
   ],
   "source": [
    "train_feature=latent_encoder_choose_F_AE.predict(C_train_x)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "test_feature=latent_encoder_choose_F_AE.predict(C_test_x)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6 Feature group compare\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_Weights=F.top_k_keep(F_AE.get_layer(index=1).get_weights()[0],key_feture_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_position_group=F.k_index_argsort_1d(Selected_Weights,key_feture_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_group)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_group)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.6382296343810134\n",
      "Testing accuracy： 0.6382296343810134\n"
     ]
    }
   ],
   "source": [
    "p_seed=seed\n",
    "F.ETree(train_feature[:,0:25],train_label,test_feature[:,0:25],test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7754971135343168\n",
      "Testing accuracy： 0.7754971135343168\n"
     ]
    }
   ],
   "source": [
    "p_seed=seed\n",
    "F.ETree(train_feature[:,25:],train_label,test_feature[:,25:],test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7299550994227069\n",
      "Testing accuracy： 0.7299550994227069\n"
     ]
    }
   ],
   "source": [
    "p_seed=seed\n",
    "F.ETree(train_feature[:,0:30],train_label,test_feature[:,0:30],test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7087876844130853\n",
      "Testing accuracy： 0.7087876844130853\n"
     ]
    }
   ],
   "source": [
    "p_seed=seed\n",
    "F.ETree(train_feature[:,30:],train_label,test_feature[:,30:],test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 50)\n",
      "(1559, 50)\n",
      "0.01756144064464018\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
