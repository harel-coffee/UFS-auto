{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "\n",
    "seed=0\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "rn.seed(seed)\n",
    "#session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "session_conf =tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "#tf.set_random_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "#sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "\n",
    "K.set_session(sess)\n",
    "#----------------------------Reproducible----------------------------------------------------------------------------------------\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten, Activation, Dropout, Layer\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers,initializers,constraints,regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import LambdaCallback,ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import h5py\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "#Import ourslef defined methods\n",
    "import sys\n",
    "sys.path.append(r\"./Defined\")\n",
    "import Functions as F\n",
    "\n",
    "# The following code should be added before the keras model\n",
    "#np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frame=np.array(pd.read_csv('./Dataset/isolet1+2+3+4.data',header=None))\n",
    "test_data_frame=np.array(pd.read_csv('./Dataset/isolet5.data',header=None))\n",
    "\n",
    "train_data_arr=(train_data_frame[:,0:617]).copy()\n",
    "train_label_arr=((train_data_frame[:,617]).copy()-1)\n",
    "test_data_arr=(test_data_frame[:,0:617]).copy()\n",
    "test_label_arr=((test_data_frame[:,617]).copy()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6238, 617)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1559, 617)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7797, 617)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.r_[train_data_arr,test_data_arr].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=MinMaxScaler(feature_range=(0,1)).fit_transform(np.r_[train_data_arr,test_data_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7797, 617)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_train_x=Data[:len(train_data_arr)]\n",
    "C_test_x=Data[len(train_data_arr):]\n",
    "C_train_y=train_label_arr#to_categorical(train_label_arr)\n",
    "C_test_y=test_label_arr#to_categorical(test_label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (5614, 617)\n",
      "Shape of x_validate: (624, 617)\n",
      "Shape of x_test: (1559, 617)\n",
      "Shape of y_train: (5614,)\n",
      "Shape of y_validate: (624,)\n",
      "Shape of y_test: (1559,)\n",
      "Shape of C_train_x: (6238, 617)\n",
      "Shape of C_train_y: (6238,)\n",
      "Shape of C_test_x: (1559, 617)\n",
      "Shape of C_test_y: (1559,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_validate,y_train_onehot,y_validate_onehot= train_test_split(C_train_x,C_train_y,test_size=0.1,random_state=seed)\n",
    "x_test=C_test_x\n",
    "y_test_onehot=C_test_y\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape)) \n",
    "print('Shape of x_validate: ' + str(x_validate.shape)) \n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "print('Shape of y_train: ' + str(y_train_onehot.shape))\n",
    "print('Shape of y_validate: ' + str(y_validate_onehot.shape))\n",
    "print('Shape of y_test: ' + str(y_test_onehot.shape))\n",
    "\n",
    "print('Shape of C_train_x: ' + str(C_train_x.shape)) \n",
    "print('Shape of C_train_y: ' + str(C_train_y.shape)) \n",
    "print('Shape of C_test_x: ' + str(C_test_x.shape)) \n",
    "print('Shape of C_test_y: ' + str(C_test_y.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feture_number=85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "class Feature_Select_Layer(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super(Feature_Select_Layer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',  \n",
    "                                      shape=(input_shape[1],),\n",
    "                                      initializer=initializers.RandomUniform(minval=0.999999, maxval=0.9999999, seed=seed),\n",
    "                                      trainable=True)\n",
    "        super(Feature_Select_Layer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, selection=False,k=key_feture_number):\n",
    "        kernel=K.pow(self.kernel,2)       \n",
    "        if selection:\n",
    "            kernel_=K.transpose(kernel)\n",
    "            kth_largest = tf.math.top_k(kernel_, k=k)[0][-1]\n",
    "            kernel = tf.where(condition=K.less(kernel,kth_largest),x=K.zeros_like(kernel),y=kernel)        \n",
    "        return K.dot(x, tf.linalg.tensor_diag(kernel))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                p_encoding_dim=key_feture_number,\\\n",
    "                p_learning_rate= 1E-3):\n",
    "    input_img = Input(shape=(p_data_feature,), name='input_img')\n",
    "\n",
    "    encoded = Dense(p_encoding_dim, activation='linear',kernel_initializer=initializers.glorot_uniform(seed))(input_img)\n",
    "    bottleneck=encoded\n",
    "    decoded = Dense(p_data_feature, activation='linear',kernel_initializer=initializers.glorot_uniform(seed))(encoded)\n",
    "\n",
    "    latent_encoder = Model(input_img, bottleneck)\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error', optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    #print('Latent Encoder Structure-------------------------------------')\n",
    "    #latent_encoder.summary()\n",
    "    return autoencoder,latent_encoder\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Identity_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                         p_encoding_dim=key_feture_number,\\\n",
    "                         p_learning_rate= 1E-3):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    autoencoder = Model(input_img, decoded_score)\n",
    "    \n",
    "    autoencoder.compile(loss='mean_squared_error',\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,latent_encoder_score\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------\n",
    "def Fractal_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                        p_feture_number=key_feture_number,\\\n",
    "                        p_encoding_dim=key_feture_number,\\\n",
    "                        p_learning_rate=1E-3,\\\n",
    "                        p_loss_weight_1=1,\\\n",
    "                        p_loss_weight_2=2):\n",
    "    \n",
    "    input_img = Input(shape=(p_data_feature,), name='autoencoder_input')\n",
    "\n",
    "    feature_selection = Feature_Select_Layer(output_dim=p_data_feature,\\\n",
    "                                             input_shape=(p_data_feature,),\\\n",
    "                                             name='feature_selection')\n",
    "\n",
    "    feature_selection_score=feature_selection(input_img)\n",
    "    feature_selection_choose=feature_selection(input_img,selection=True,k=p_feture_number)\n",
    "\n",
    "    encoded = Dense(p_encoding_dim,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_hidden_layer')\n",
    "    \n",
    "    encoded_score=encoded(feature_selection_score)\n",
    "    encoded_choose=encoded(feature_selection_choose)\n",
    "    \n",
    "    bottleneck_score=encoded_score\n",
    "    bottleneck_choose=encoded_choose\n",
    "    \n",
    "    decoded = Dense(p_data_feature,\\\n",
    "                    activation='linear',\\\n",
    "                    kernel_initializer=initializers.glorot_uniform(seed),\\\n",
    "                    name='autoencoder_output')\n",
    "    \n",
    "    decoded_score =decoded(bottleneck_score)\n",
    "    decoded_choose =decoded(bottleneck_choose)\n",
    "\n",
    "    latent_encoder_score = Model(input_img, bottleneck_score)\n",
    "    latent_encoder_choose = Model(input_img, bottleneck_choose)\n",
    "    feature_selection_output=Model(input_img,feature_selection_choose)\n",
    "    autoencoder = Model(input_img, [decoded_score,decoded_choose])\n",
    "    \n",
    "    autoencoder.compile(loss=['mean_squared_error','mean_squared_error'],\\\n",
    "                        loss_weights=[p_loss_weight_1, p_loss_weight_2],\\\n",
    "                        optimizer=optimizers.Adam(lr=p_learning_rate))\n",
    "    \n",
    "    print('Autoencoder Structure-------------------------------------')\n",
    "    autoencoder.summary()\n",
    "    return autoencoder,feature_selection_output,latent_encoder_score,latent_encoder_choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Structure and paramter testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_number=200\n",
    "batch_size_value=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.1 Fractal Autoencoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-11-478f6b761227>:22: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Autoencoder Structure-------------------------------------\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "autoencoder_input (InputLayer)  (None, 617)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "feature_selection (Feature_Sele (None, 617)          617         autoencoder_input[0][0]          \n",
      "                                                                 autoencoder_input[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "autoencoder_hidden_layer (Dense (None, 85)           52530       feature_selection[0][0]          \n",
      "                                                                 feature_selection[1][0]          \n",
      "__________________________________________________________________________________________________\n",
      "autoencoder_output (Dense)      (None, 617)          53062       autoencoder_hidden_layer[0][0]   \n",
      "                                                                 autoencoder_hidden_layer[1][0]   \n",
      "==================================================================================================\n",
      "Total params: 106,209\n",
      "Trainable params: 106,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loss_weight_1=0.0078125\n",
    "F_AE,\\\n",
    "feature_selection_output,\\\n",
    "latent_encoder_score_F_AE,\\\n",
    "latent_encoder_choose_F_AE=Fractal_Autoencoder(p_data_feature=x_train.shape[1],\\\n",
    "                                               p_feture_number=key_feture_number,\\\n",
    "                                               p_encoding_dim=key_feture_number,\\\n",
    "                                               p_learning_rate= 1E-3,\\\n",
    "                                               p_loss_weight_1=loss_weight_1,\\\n",
    "                                               p_loss_weight_2=1)\n",
    "\n",
    "#file_name=\"./log/F_AE_\"+str(key_feture_number)+\".png\"\n",
    "#plot_model(F_AE, to_file=file_name,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 5614 samples, validate on 624 samples\n",
      "Epoch 1/200\n",
      "5614/5614 [==============================] - 3s 622us/step - loss: 0.0833 - autoencoder_output_loss: 0.0824 - val_loss: 0.0435 - val_autoencoder_output_loss: 0.0431\n",
      "Epoch 2/200\n",
      "5614/5614 [==============================] - 2s 440us/step - loss: 0.0382 - autoencoder_output_loss: 0.0379 - val_loss: 0.0360 - val_autoencoder_output_loss: 0.0357\n",
      "Epoch 3/200\n",
      "5614/5614 [==============================] - 2s 403us/step - loss: 0.0324 - autoencoder_output_loss: 0.0321 - val_loss: 0.0310 - val_autoencoder_output_loss: 0.0307\n",
      "Epoch 4/200\n",
      "5614/5614 [==============================] - 2s 414us/step - loss: 0.0288 - autoencoder_output_loss: 0.0285 - val_loss: 0.0278 - val_autoencoder_output_loss: 0.0276\n",
      "Epoch 5/200\n",
      "5614/5614 [==============================] - 2s 416us/step - loss: 0.0260 - autoencoder_output_loss: 0.0258 - val_loss: 0.0252 - val_autoencoder_output_loss: 0.0250\n",
      "Epoch 6/200\n",
      "5614/5614 [==============================] - 2s 442us/step - loss: 0.0243 - autoencoder_output_loss: 0.0240 - val_loss: 0.0238 - val_autoencoder_output_loss: 0.0236\n",
      "Epoch 7/200\n",
      "5614/5614 [==============================] - 2s 412us/step - loss: 0.0226 - autoencoder_output_loss: 0.0224 - val_loss: 0.0222 - val_autoencoder_output_loss: 0.0220\n",
      "Epoch 8/200\n",
      "5614/5614 [==============================] - 2s 440us/step - loss: 0.0213 - autoencoder_output_loss: 0.0211 - val_loss: 0.0210 - val_autoencoder_output_loss: 0.0208\n",
      "Epoch 9/200\n",
      "5614/5614 [==============================] - 3s 464us/step - loss: 0.0202 - autoencoder_output_loss: 0.0200 - val_loss: 0.0199 - val_autoencoder_output_loss: 0.0197\n",
      "Epoch 10/200\n",
      "5614/5614 [==============================] - 2s 438us/step - loss: 0.0195 - autoencoder_output_loss: 0.0193 - val_loss: 0.0192 - val_autoencoder_output_loss: 0.0190\n",
      "Epoch 11/200\n",
      "5614/5614 [==============================] - 3s 461us/step - loss: 0.0187 - autoencoder_output_loss: 0.0186 - val_loss: 0.0185 - val_autoencoder_output_loss: 0.0184\n",
      "Epoch 12/200\n",
      "5614/5614 [==============================] - 2s 442us/step - loss: 0.0181 - autoencoder_output_loss: 0.0180 - val_loss: 0.0183 - val_autoencoder_output_loss: 0.0181\n",
      "Epoch 13/200\n",
      "5614/5614 [==============================] - 2s 441us/step - loss: 0.0177 - autoencoder_output_loss: 0.0175 - val_loss: 0.0175 - val_autoencoder_output_loss: 0.0174\n",
      "Epoch 14/200\n",
      "5614/5614 [==============================] - 2s 404us/step - loss: 0.0173 - autoencoder_output_loss: 0.0171 - val_loss: 0.0174 - val_autoencoder_output_loss: 0.0173\n",
      "Epoch 15/200\n",
      "5614/5614 [==============================] - 3s 470us/step - loss: 0.0169 - autoencoder_output_loss: 0.0167 - val_loss: 0.0167 - val_autoencoder_output_loss: 0.0166\n",
      "Epoch 16/200\n",
      "5614/5614 [==============================] - 3s 464us/step - loss: 0.0165 - autoencoder_output_loss: 0.0164 - val_loss: 0.0166 - val_autoencoder_output_loss: 0.0165\n",
      "Epoch 17/200\n",
      "5614/5614 [==============================] - 3s 460us/step - loss: 0.0162 - autoencoder_output_loss: 0.0161 - val_loss: 0.0162 - val_autoencoder_output_loss: 0.0160\n",
      "Epoch 18/200\n",
      "5614/5614 [==============================] - 3s 449us/step - loss: 0.0160 - autoencoder_output_loss: 0.0158 - val_loss: 0.0160 - val_autoencoder_output_loss: 0.0159\n",
      "Epoch 19/200\n",
      "5614/5614 [==============================] - 2s 443us/step - loss: 0.0158 - autoencoder_output_loss: 0.0157 - val_loss: 0.0160 - val_autoencoder_output_loss: 0.0159\n",
      "Epoch 20/200\n",
      "5614/5614 [==============================] - 3s 457us/step - loss: 0.0156 - autoencoder_output_loss: 0.0155 - val_loss: 0.0155 - val_autoencoder_output_loss: 0.0154\n",
      "Epoch 21/200\n",
      "5614/5614 [==============================] - 3s 462us/step - loss: 0.0153 - autoencoder_output_loss: 0.0152 - val_loss: 0.0153 - val_autoencoder_output_loss: 0.0152\n",
      "Epoch 22/200\n",
      "5614/5614 [==============================] - 3s 452us/step - loss: 0.0151 - autoencoder_output_loss: 0.0150 - val_loss: 0.0152 - val_autoencoder_output_loss: 0.0151\n",
      "Epoch 23/200\n",
      "5614/5614 [==============================] - 2s 441us/step - loss: 0.0148 - autoencoder_output_loss: 0.0147 - val_loss: 0.0149 - val_autoencoder_output_loss: 0.0148\n",
      "Epoch 24/200\n",
      "5614/5614 [==============================] - 3s 481us/step - loss: 0.0146 - autoencoder_output_loss: 0.0145 - val_loss: 0.0147 - val_autoencoder_output_loss: 0.0146\n",
      "Epoch 25/200\n",
      "5614/5614 [==============================] - 2s 437us/step - loss: 0.0144 - autoencoder_output_loss: 0.0143 - val_loss: 0.0146 - val_autoencoder_output_loss: 0.0144\n",
      "Epoch 26/200\n",
      "5614/5614 [==============================] - 3s 455us/step - loss: 0.0143 - autoencoder_output_loss: 0.0142 - val_loss: 0.0144 - val_autoencoder_output_loss: 0.0143\n",
      "Epoch 27/200\n",
      "5614/5614 [==============================] - 2s 443us/step - loss: 0.0142 - autoencoder_output_loss: 0.0141 - val_loss: 0.0142 - val_autoencoder_output_loss: 0.0141\n",
      "Epoch 28/200\n",
      "5614/5614 [==============================] - 2s 431us/step - loss: 0.0139 - autoencoder_output_loss: 0.0138 - val_loss: 0.0140 - val_autoencoder_output_loss: 0.0139\n",
      "Epoch 29/200\n",
      "5614/5614 [==============================] - 3s 466us/step - loss: 0.0138 - autoencoder_output_loss: 0.0137 - val_loss: 0.0139 - val_autoencoder_output_loss: 0.0138\n",
      "Epoch 30/200\n",
      "5614/5614 [==============================] - 3s 457us/step - loss: 0.0137 - autoencoder_output_loss: 0.0136 - val_loss: 0.0138 - val_autoencoder_output_loss: 0.0137\n",
      "Epoch 31/200\n",
      "5614/5614 [==============================] - 3s 460us/step - loss: 0.0136 - autoencoder_output_loss: 0.0135 - val_loss: 0.0138 - val_autoencoder_output_loss: 0.0137\n",
      "Epoch 32/200\n",
      "5614/5614 [==============================] - 3s 484us/step - loss: 0.0136 - autoencoder_output_loss: 0.0135 - val_loss: 0.0137 - val_autoencoder_output_loss: 0.0136\n",
      "Epoch 33/200\n",
      "5614/5614 [==============================] - 2s 417us/step - loss: 0.0135 - autoencoder_output_loss: 0.0134 - val_loss: 0.0136 - val_autoencoder_output_loss: 0.0135\n",
      "Epoch 34/200\n",
      "5614/5614 [==============================] - 2s 444us/step - loss: 0.0135 - autoencoder_output_loss: 0.0134 - val_loss: 0.0136 - val_autoencoder_output_loss: 0.0135\n",
      "Epoch 35/200\n",
      "5614/5614 [==============================] - 3s 453us/step - loss: 0.0134 - autoencoder_output_loss: 0.0133 - val_loss: 0.0135 - val_autoencoder_output_loss: 0.0134\n",
      "Epoch 36/200\n",
      "5614/5614 [==============================] - 2s 433us/step - loss: 0.0133 - autoencoder_output_loss: 0.0132 - val_loss: 0.0134 - val_autoencoder_output_loss: 0.0134\n",
      "Epoch 37/200\n",
      "5614/5614 [==============================] - 2s 441us/step - loss: 0.0133 - autoencoder_output_loss: 0.0132 - val_loss: 0.0134 - val_autoencoder_output_loss: 0.0133\n",
      "Epoch 38/200\n",
      "5614/5614 [==============================] - 2s 431us/step - loss: 0.0133 - autoencoder_output_loss: 0.0132 - val_loss: 0.0133 - val_autoencoder_output_loss: 0.0132\n",
      "Epoch 39/200\n",
      "5614/5614 [==============================] - 2s 420us/step - loss: 0.0132 - autoencoder_output_loss: 0.0131 - val_loss: 0.0133 - val_autoencoder_output_loss: 0.0132\n",
      "Epoch 40/200\n",
      "5614/5614 [==============================] - 2s 422us/step - loss: 0.0132 - autoencoder_output_loss: 0.0131 - val_loss: 0.0133 - val_autoencoder_output_loss: 0.0132\n",
      "Epoch 41/200\n",
      "5614/5614 [==============================] - 2s 415us/step - loss: 0.0131 - autoencoder_output_loss: 0.0130 - val_loss: 0.0133 - val_autoencoder_output_loss: 0.0132\n",
      "Epoch 42/200\n",
      "5614/5614 [==============================] - 2s 361us/step - loss: 0.0131 - autoencoder_output_loss: 0.0130 - val_loss: 0.0132 - val_autoencoder_output_loss: 0.0132\n",
      "Epoch 43/200\n",
      "5614/5614 [==============================] - 2s 436us/step - loss: 0.0131 - autoencoder_output_loss: 0.0130 - val_loss: 0.0131 - val_autoencoder_output_loss: 0.0131\n",
      "Epoch 44/200\n",
      "5614/5614 [==============================] - 2s 420us/step - loss: 0.0130 - autoencoder_output_loss: 0.0129 - val_loss: 0.0132 - val_autoencoder_output_loss: 0.0131\n",
      "Epoch 45/200\n",
      "5614/5614 [==============================] - 2s 403us/step - loss: 0.0130 - autoencoder_output_loss: 0.0129 - val_loss: 0.0131 - val_autoencoder_output_loss: 0.0131\n",
      "Epoch 46/200\n",
      "5614/5614 [==============================] - 2s 390us/step - loss: 0.0130 - autoencoder_output_loss: 0.0129 - val_loss: 0.0131 - val_autoencoder_output_loss: 0.0130\n",
      "Epoch 47/200\n",
      "5614/5614 [==============================] - 3s 460us/step - loss: 0.0129 - autoencoder_output_loss: 0.0128 - val_loss: 0.0132 - val_autoencoder_output_loss: 0.0131\n",
      "Epoch 48/200\n",
      "5614/5614 [==============================] - 3s 482us/step - loss: 0.0129 - autoencoder_output_loss: 0.0128 - val_loss: 0.0130 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 49/200\n",
      "5614/5614 [==============================] - 3s 450us/step - loss: 0.0129 - autoencoder_output_loss: 0.0128 - val_loss: 0.0133 - val_autoencoder_output_loss: 0.0132\n",
      "Epoch 50/200\n",
      "5614/5614 [==============================] - 2s 437us/step - loss: 0.0129 - autoencoder_output_loss: 0.0128 - val_loss: 0.0130 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 51/200\n",
      "5614/5614 [==============================] - 2s 419us/step - loss: 0.0129 - autoencoder_output_loss: 0.0128 - val_loss: 0.0130 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 52/200\n",
      "5614/5614 [==============================] - 3s 468us/step - loss: 0.0128 - autoencoder_output_loss: 0.0127 - val_loss: 0.0130 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 53/200\n",
      "5614/5614 [==============================] - 3s 467us/step - loss: 0.0128 - autoencoder_output_loss: 0.0127 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 54/200\n",
      "5614/5614 [==============================] - 3s 481us/step - loss: 0.0128 - autoencoder_output_loss: 0.0127 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 55/200\n",
      "5614/5614 [==============================] - 3s 463us/step - loss: 0.0128 - autoencoder_output_loss: 0.0127 - val_loss: 0.0130 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 56/200\n",
      "5614/5614 [==============================] - 2s 436us/step - loss: 0.0128 - autoencoder_output_loss: 0.0127 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 57/200\n",
      "5614/5614 [==============================] - 2s 419us/step - loss: 0.0128 - autoencoder_output_loss: 0.0127 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 58/200\n",
      "5614/5614 [==============================] - 3s 446us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 59/200\n",
      "5614/5614 [==============================] - 2s 435us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 60/200\n",
      "5614/5614 [==============================] - 3s 459us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 61/200\n",
      "5614/5614 [==============================] - 3s 450us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 62/200\n",
      "5614/5614 [==============================] - 2s 429us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 63/200\n",
      "5614/5614 [==============================] - 3s 456us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 64/200\n",
      "5614/5614 [==============================] - 2s 436us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 65/200\n",
      "5614/5614 [==============================] - 2s 432us/step - loss: 0.0126 - autoencoder_output_loss: 0.0126 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 66/200\n",
      "5614/5614 [==============================] - 2s 434us/step - loss: 0.0126 - autoencoder_output_loss: 0.0126 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0128\n",
      "Epoch 67/200\n",
      "5614/5614 [==============================] - 3s 479us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 68/200\n",
      "5614/5614 [==============================] - 2s 331us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 69/200\n",
      "5614/5614 [==============================] - 2s 312us/step - loss: 0.0126 - autoencoder_output_loss: 0.0126 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 70/200\n",
      "5614/5614 [==============================] - 2s 363us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 71/200\n",
      "5614/5614 [==============================] - 3s 451us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 72/200\n",
      "5614/5614 [==============================] - 2s 422us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 73/200\n",
      "5614/5614 [==============================] - 2s 383us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 74/200\n",
      "5614/5614 [==============================] - 2s 401us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 75/200\n",
      "5614/5614 [==============================] - 2s 375us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 76/200\n",
      "5614/5614 [==============================] - 2s 385us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 77/200\n",
      "5614/5614 [==============================] - 2s 396us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 78/200\n",
      "5614/5614 [==============================] - 2s 379us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 79/200\n",
      "5614/5614 [==============================] - 2s 356us/step - loss: 0.0125 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 80/200\n",
      "5614/5614 [==============================] - 2s 373us/step - loss: 0.0125 - autoencoder_output_loss: 0.0125 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 81/200\n",
      "5614/5614 [==============================] - 2s 370us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 82/200\n",
      "5614/5614 [==============================] - 2s 409us/step - loss: 0.0125 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 83/200\n",
      "5614/5614 [==============================] - 2s 356us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 84/200\n",
      "5614/5614 [==============================] - 2s 389us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 85/200\n",
      "5614/5614 [==============================] - 2s 397us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 86/200\n",
      "5614/5614 [==============================] - 2s 359us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 87/200\n",
      "5614/5614 [==============================] - 2s 335us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 88/200\n",
      "5614/5614 [==============================] - 2s 363us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 89/200\n",
      "5614/5614 [==============================] - 2s 421us/step - loss: 0.0125 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 90/200\n",
      "5614/5614 [==============================] - 3s 455us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 91/200\n",
      "5614/5614 [==============================] - 2s 429us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 92/200\n",
      "5614/5614 [==============================] - 2s 342us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 93/200\n",
      "5614/5614 [==============================] - 2s 384us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 94/200\n",
      "5614/5614 [==============================] - 2s 412us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 95/200\n",
      "5614/5614 [==============================] - 2s 382us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 96/200\n",
      "5614/5614 [==============================] - 2s 395us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 97/200\n",
      "5614/5614 [==============================] - 2s 300us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 98/200\n",
      "5614/5614 [==============================] - 2s 315us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 99/200\n",
      "5614/5614 [==============================] - 2s 338us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 100/200\n",
      "5614/5614 [==============================] - 2s 311us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "\n",
      "Epoch 00100: saving model to ./log_weights/F_AE_85_weights_0.0078125.0100.hdf5\n",
      "Epoch 101/200\n",
      "5614/5614 [==============================] - 2s 340us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 102/200\n",
      "5614/5614 [==============================] - 2s 347us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 103/200\n",
      "5614/5614 [==============================] - 2s 387us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 104/200\n",
      "5614/5614 [==============================] - 2s 385us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 105/200\n",
      "5614/5614 [==============================] - 2s 368us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 106/200\n",
      "5614/5614 [==============================] - 2s 350us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 107/200\n",
      "5614/5614 [==============================] - 2s 379us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0128 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 108/200\n",
      "5614/5614 [==============================] - 2s 395us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 109/200\n",
      "5614/5614 [==============================] - 2s 368us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 110/200\n",
      "5614/5614 [==============================] - 2s 368us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 111/200\n",
      "5614/5614 [==============================] - 2s 390us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 112/200\n",
      "5614/5614 [==============================] - 2s 402us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 113/200\n",
      "5614/5614 [==============================] - 2s 365us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0127\n",
      "Epoch 114/200\n",
      "5614/5614 [==============================] - 2s 361us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 115/200\n",
      "5614/5614 [==============================] - 2s 356us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 116/200\n",
      "5614/5614 [==============================] - 2s 374us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 117/200\n",
      "5614/5614 [==============================] - 2s 362us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 118/200\n",
      "5614/5614 [==============================] - 2s 349us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 119/200\n",
      "5614/5614 [==============================] - 2s 342us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 120/200\n",
      "5614/5614 [==============================] - 2s 352us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 121/200\n",
      "5614/5614 [==============================] - 2s 352us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 122/200\n",
      "5614/5614 [==============================] - 2s 342us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 123/200\n",
      "5614/5614 [==============================] - 2s 363us/step - loss: 0.0127 - autoencoder_output_loss: 0.0126 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 124/200\n",
      "5614/5614 [==============================] - 2s 360us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 125/200\n",
      "5614/5614 [==============================] - 2s 367us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 126/200\n",
      "5614/5614 [==============================] - 2s 347us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 127/200\n",
      "5614/5614 [==============================] - 2s 364us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 128/200\n",
      "5614/5614 [==============================] - 2s 369us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 129/200\n",
      "5614/5614 [==============================] - 2s 337us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 130/200\n",
      "5614/5614 [==============================] - 2s 302us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 131/200\n",
      "5614/5614 [==============================] - 2s 299us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 132/200\n",
      "5614/5614 [==============================] - 2s 317us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 133/200\n",
      "5614/5614 [==============================] - 2s 299us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 134/200\n",
      "5614/5614 [==============================] - 2s 305us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 135/200\n",
      "5614/5614 [==============================] - 2s 319us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 136/200\n",
      "5614/5614 [==============================] - 2s 299us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 137/200\n",
      "5614/5614 [==============================] - 2s 300us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 138/200\n",
      "5614/5614 [==============================] - 2s 310us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 139/200\n",
      "5614/5614 [==============================] - 2s 303us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 140/200\n",
      "5614/5614 [==============================] - 2s 297us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 141/200\n",
      "5614/5614 [==============================] - 2s 325us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 142/200\n",
      "5614/5614 [==============================] - 2s 294us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 143/200\n",
      "5614/5614 [==============================] - 2s 291us/step - loss: 0.0125 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 144/200\n",
      "5614/5614 [==============================] - 2s 301us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 145/200\n",
      "5614/5614 [==============================] - 2s 308us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 146/200\n",
      "5614/5614 [==============================] - 2s 324us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 147/200\n",
      "5614/5614 [==============================] - 2s 305us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 148/200\n",
      "5614/5614 [==============================] - 2s 300us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 149/200\n",
      "5614/5614 [==============================] - 2s 285us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 150/200\n",
      "5614/5614 [==============================] - 2s 291us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 151/200\n",
      "5614/5614 [==============================] - 2s 294us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 152/200\n",
      "5614/5614 [==============================] - 2s 295us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 153/200\n",
      "5614/5614 [==============================] - 2s 301us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 154/200\n",
      "5614/5614 [==============================] - 2s 302us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 155/200\n",
      "5614/5614 [==============================] - 2s 301us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 156/200\n",
      "5614/5614 [==============================] - 2s 291us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 157/200\n",
      "5614/5614 [==============================] - 2s 298us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 158/200\n",
      "5614/5614 [==============================] - 2s 297us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 159/200\n",
      "5614/5614 [==============================] - 2s 307us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 160/200\n",
      "5614/5614 [==============================] - 2s 301us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 161/200\n",
      "5614/5614 [==============================] - 2s 306us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 162/200\n",
      "5614/5614 [==============================] - 2s 296us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 163/200\n",
      "5614/5614 [==============================] - 2s 326us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 164/200\n",
      "5614/5614 [==============================] - 1s 213us/step - loss: 0.0126 - autoencoder_output_loss: 0.0126 - val_loss: 0.0130 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 165/200\n",
      "5614/5614 [==============================] - 2s 274us/step - loss: 0.0126 - autoencoder_output_loss: 0.0125 - val_loss: 0.0129 - val_autoencoder_output_loss: 0.0129\n",
      "Epoch 166/200\n",
      "5614/5614 [==============================] - 2s 282us/step - loss: 0.0124 - autoencoder_output_loss: 0.0124 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 167/200\n",
      "5614/5614 [==============================] - 2s 288us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0125 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 168/200\n",
      "5614/5614 [==============================] - 2s 299us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 169/200\n",
      "5614/5614 [==============================] - 2s 300us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 170/200\n",
      "5614/5614 [==============================] - 2s 269us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 171/200\n",
      "5614/5614 [==============================] - 2s 269us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 172/200\n",
      "5614/5614 [==============================] - 1s 253us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 173/200\n",
      "5614/5614 [==============================] - 1s 204us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 174/200\n",
      "5614/5614 [==============================] - 1s 265us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 175/200\n",
      "5614/5614 [==============================] - 1s 246us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 176/200\n",
      "5614/5614 [==============================] - 1s 230us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 177/200\n",
      "5614/5614 [==============================] - 1s 264us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 178/200\n",
      "5614/5614 [==============================] - 2s 283us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 179/200\n",
      "5614/5614 [==============================] - 1s 229us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 180/200\n",
      "5614/5614 [==============================] - 1s 241us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 181/200\n",
      "5614/5614 [==============================] - 1s 231us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0125 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 182/200\n",
      "5614/5614 [==============================] - 1s 208us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 183/200\n",
      "5614/5614 [==============================] - 1s 210us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 184/200\n",
      "5614/5614 [==============================] - 1s 260us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 185/200\n",
      "5614/5614 [==============================] - 1s 255us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 186/200\n",
      "5614/5614 [==============================] - 1s 190us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 187/200\n",
      "5614/5614 [==============================] - 1s 195us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 188/200\n",
      "5614/5614 [==============================] - 1s 230us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 189/200\n",
      "5614/5614 [==============================] - 1s 184us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0125 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 190/200\n",
      "5614/5614 [==============================] - 1s 228us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 191/200\n",
      "5614/5614 [==============================] - 1s 199us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 192/200\n",
      "5614/5614 [==============================] - 1s 197us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 193/200\n",
      "5614/5614 [==============================] - 1s 205us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 194/200\n",
      "5614/5614 [==============================] - 1s 209us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 195/200\n",
      "5614/5614 [==============================] - 1s 229us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 196/200\n",
      "5614/5614 [==============================] - 1s 194us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 197/200\n",
      "5614/5614 [==============================] - 1s 181us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0125 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 198/200\n",
      "5614/5614 [==============================] - 1s 228us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "Epoch 199/200\n",
      "5614/5614 [==============================] - 1s 217us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0127 - val_autoencoder_output_loss: 0.0126\n",
      "Epoch 200/200\n",
      "5614/5614 [==============================] - 1s 231us/step - loss: 0.0124 - autoencoder_output_loss: 0.0123 - val_loss: 0.0126 - val_autoencoder_output_loss: 0.0125\n",
      "\n",
      "Epoch 00200: saving model to ./log_weights/F_AE_85_weights_0.0078125.0200.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint=ModelCheckpoint('./log_weights/F_AE_'+str(key_feture_number)+'_weights_'+str(loss_weight_1)+'.{epoch:04d}.hdf5',period=100,save_weights_only=True,verbose=1)\n",
    "#print_weights = LambdaCallback(on_epoch_end=lambda batch, logs: print(F_AE.layers[1].get_weights()))\n",
    "\n",
    "F_AE_history = F_AE.fit(x_train, [x_train,x_train],\\\n",
    "                        epochs=epochs_number,\\\n",
    "                        batch_size=batch_size_value,\\\n",
    "                        shuffle=True,\\\n",
    "                        validation_data=(x_validate, [x_validate,x_validate]),\\\n",
    "                        callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = F_AE_history.history['loss']\n",
    "val_loss = F_AE_history.history['val_loss']\n",
    "\n",
    "epochs = range(epochs_number)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs[250:], loss[250:], 'bo', label='Training Loss')\n",
    "plt.plot(epochs[250:], val_loss[250:], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for one-to-one map layer 0.010505052408552991\n",
      "MSE for feature selection layer 0.012824035136022777\n"
     ]
    }
   ],
   "source": [
    "p_data=F_AE.predict(x_test)\n",
    "numbers=x_test.shape[0]*x_test.shape[1]\n",
    "\n",
    "print(\"MSE for one-to-one map layer\",np.sum(np.power(np.array(p_data)[0]-x_test,2))/numbers)\n",
    "print(\"MSE for feature selection layer\",np.sum(np.power(np.array(p_data)[1]-x_test,2))/numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.2 Feature selection layer output\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "FS_layer_output=feature_selection_output.predict(x_test)\n",
    "print(np.sum(FS_layer_output[0]>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.3 Key features show\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617\n"
     ]
    }
   ],
   "source": [
    "key_features=F.top_k_keepWeights_1(F_AE.get_layer(index=1).get_weights()[0],key_feture_number)\n",
    "print(np.sum(F_AE.get_layer(index=1).get_weights()[0]>0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.9448364336112893\n",
      "Testing accuracy： 0.9448364336112893\n"
     ]
    }
   ],
   "source": [
    "train_feature=C_train_x\n",
    "train_label=C_train_y\n",
    "test_feature=C_test_x\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_position_list=np.where(key_features>0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4.1.1. On Identity Selection layer\n",
    "---\n",
    "\n",
    "a) with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feature>0:  67\n",
      "(6238, 617)\n",
      "test_feature>0:  70\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8665811417575369\n",
      "Testing accuracy： 0.8665811417575369\n"
     ]
    }
   ],
   "source": [
    "train_feature=feature_selection_output.predict(C_train_x)\n",
    "print(\"train_feature>0: \",np.sum(train_feature[0]>0))\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "test_feature=feature_selection_output.predict(C_test_x)\n",
    "print(\"test_feature>0: \",np.sum(test_feature[0]>0))\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "b) Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 617)\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8665811417575369\n",
      "Testing accuracy： 0.8665811417575369\n"
     ]
    }
   ],
   "source": [
    "train_feature=feature_selection_output.predict(C_train_x)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature=feature_selection_output.predict(C_test_x)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "train_feature_sparse=sparse.coo_matrix(train_feature)\n",
    "test_feature_sparse=sparse.coo_matrix(test_feature)\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature_sparse,train_label,test_feature_sparse,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "c) Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7479153303399615\n",
      "Testing accuracy： 0.7479153303399615\n"
     ]
    }
   ],
   "source": [
    "train_feature_=feature_selection_output.predict(C_train_x)\n",
    "train_feature=F.compress_zero(train_feature_,key_feture_number)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=feature_selection_output.predict(C_test_x)\n",
    "test_feature=F.compress_zero(test_feature_,key_feture_number)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "d) Compression with structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8832584990378448\n",
      "Testing accuracy： 0.8832584990378448\n"
     ]
    }
   ],
   "source": [
    "train_feature_=feature_selection_output.predict(C_train_x)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=feature_selection_output.predict(C_test_x)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4.1.2. On Original Selection\n",
    "---\n",
    "\n",
    "a) with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_feature>0:  67\n",
      "(6238, 617)\n",
      "test_feature>0:  70\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8665811417575369\n",
      "Testing accuracy： 0.8665811417575369\n"
     ]
    }
   ],
   "source": [
    "train_feature=np.multiply(C_train_x, key_features)\n",
    "print(\"train_feature>0: \",np.sum(train_feature[0]>0))\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature=np.multiply(C_test_x, key_features)\n",
    "print(\"test_feature>0: \",np.sum(test_feature[0]>0))\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "b) Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 617)\n",
      "(1559, 617)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8665811417575369\n",
      "Testing accuracy： 0.8665811417575369\n"
     ]
    }
   ],
   "source": [
    "train_feature=np.multiply(C_train_x, key_features)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature=np.multiply(C_test_x, key_features)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "train_feature_sparse=sparse.coo_matrix(train_feature)\n",
    "test_feature_sparse=sparse.coo_matrix(test_feature)\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature_sparse,train_label,test_feature_sparse,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "c) Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.7613855035279025\n",
      "Testing accuracy： 0.7613855035279025\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero(train_feature_,key_feture_number)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero(test_feature_,key_feture_number)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "d) Compression with structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8832584990378448\n",
      "Testing accuracy： 0.8832584990378448\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "train_feature=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "test_feature=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 4.1.3. Latent space\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8774855676715844\n",
      "Testing accuracy： 0.8774855676715844\n"
     ]
    }
   ],
   "source": [
    "train_feature=latent_encoder_score_F_AE.predict(C_train_x)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "test_feature=latent_encoder_score_F_AE.predict(C_test_x)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "Training accuracy： 1.0\n",
      "Training accuracy： 1.0\n",
      "Testing accuracy： 0.8338678640153945\n",
      "Testing accuracy： 0.8338678640153945\n"
     ]
    }
   ],
   "source": [
    "train_feature=latent_encoder_choose_F_AE.predict(C_train_x)\n",
    "print(train_feature.shape)\n",
    "train_label=C_train_y\n",
    "test_feature=latent_encoder_choose_F_AE.predict(C_test_x)\n",
    "print(test_feature.shape)\n",
    "test_label=C_test_y\n",
    "p_seed=seed\n",
    "F.ETree(train_feature,train_label,test_feature,test_label,p_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mse_check(train, test):\n",
    "    LR = LinearRegression(n_jobs = -1)\n",
    "    LR.fit(train[0], train[1])\n",
    "    MSELR = ((LR.predict(test[0]) - test[1]) ** 2).mean()\n",
    "    return MSELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6238, 85)\n",
      "(1559, 85)\n",
      "0.012661939958993758\n"
     ]
    }
   ],
   "source": [
    "train_feature_=np.multiply(C_train_x, key_features)\n",
    "C_train_selected_x=F.compress_zero_withkeystructure(train_feature_,selected_position_list)\n",
    "print(C_train_selected_x.shape)\n",
    "\n",
    "test_feature_=np.multiply(C_test_x, key_features)\n",
    "C_test_selected_x=F.compress_zero_withkeystructure(test_feature_,selected_position_list)\n",
    "print(C_test_selected_x.shape)\n",
    "\n",
    "\n",
    "train_feature_tuple=(C_train_selected_x,C_train_x)\n",
    "test_feature_tuple=(C_test_selected_x,C_test_x)\n",
    "\n",
    "reconstruction_loss=mse_check(train_feature_tuple, test_feature_tuple)\n",
    "print(reconstruction_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
